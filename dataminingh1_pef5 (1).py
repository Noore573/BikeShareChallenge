# -*- coding: utf-8 -*-
"""DataminingH1_pef5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gJBKvOPD8gmsZixkUq_zAQyyxFoaUvqj

# **Loading libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install gdown
# %pip install tqdm scikit-learn
# %pip install geopandas
# %pip install geohash2
# %pip install folium
# %pip install python-geohash
# %pip install statsmodels
# %pip install -U kaleido==0.2.1
# %pip install prophet -q
import pandas as pd

from sklearn.decomposition import PCA



from prophet import Prophet
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt

from google.colab import files
import pandas as pd
import plotly.express as px
import gdown
import os
import plotly.graph_objects as go
import numpy as np
from scipy.stats import gaussian_kde
from google.colab import drive
from math import radians, sin, cos, sqrt, atan2
import geopandas as gpd
from shapely.geometry import Point
from sklearn.neighbors import BallTree
from tqdm import tqdm
import geohash2
from sklearn.cluster import KMeans
import json
import folium
from folium.plugins import MarkerCluster
from scipy.stats import chi2_contingency
import geohash as gh
import kaleido
from shapely.geometry import Polygon
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore
from statsmodels.tsa.seasonal import STL
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

drive.mount('/content/drive')

"""# **Loading the data**

---
downloading the dataset
---
"""

folder_id = '1O3w5OKnS__hzlL8kTSfGCUc_iX8XNjEN'
output_dir = 'Homework'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
print(f"Attempting to download content from folder ID: {folder_id} into {output_dir}")
try:
    gdown.download_folder(id=folder_id, output=output_dir, quiet=False, use_cookies=False)
    print(f"\nSuccessfully downloaded content to: /content/{output_dir}")
    print("You can now find the downloaded content in the 'downloaded_external_folder' directory in your Colab files browser.")
except Exception as e:
    print(f"\nAn error occurred during download: {e}")
    print("Please ensure the Google Drive folder is publicly accessible or shared with 'Anyone with the link can view'.")

stations_info=pd.read_csv("Homework/data/Capital_Bikeshare_Locations.csv")
#
# Load tabular data
weather_df = pd.read_csv("Homework/data/Washington,DC,USA 2024-01-01 to 2024-12-31.csv")
trips_df = pd.read_parquet('Homework/data/daily-rent.parquet')

# Load spatial parking zones
parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')

stations_df = pd.read_csv("Homework/data/Capital_Bikeshare_Locations.csv")
# Load spatial parking zones
parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')
Shuttle_Bus_Stops=pd.read_csv("Homework/data/Shuttle_Bus_Stops.csv")
Metro_Bus_Stops =pd.read_csv("Homework/data/Metro_Bus_Stops.csv")
#Loading Residential and Visitor Parking Zones
Residential_Visitor_Parking_Zones  = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')

# Ensure STATION_ID is string type for merging later
stations_df['STATION_ID'] = stations_df['STATION_ID'].astype(str)

import gdown

file_id = "1q7TZ3dGBpDZ819SYo3o5cUS5N1bYB-0T"  # Replace with your actual file ID
output_file = "downloaded_file.csv"  # You can change the output file name

gdown.download(id=file_id, output=output_file, quiet=False)
trips_df = pd.read_csv(output_file)
print(f"File downloaded to {output_file}")
trips_df.head()

"""# **Preprocessing , Cleaning & inspecting the data**

There is a problem with missing start/id , almost 20% of the data are nulls so we must find a way to fill these up

**Spatial Join**


---


using lang and lati we can match it to the nearest station and then assign this id
"""

trips_df = trips_df.dropna(subset=['end_lat', 'end_lng'])
trips_df_cleaned=trips_df.drop_duplicates()
trips_df_cleaned.isna().sum()

trips_df = trips_df.dropna(subset=['end_lat', 'end_lng']).copy() # Added .copy() to avoid SettingWithCopyWarning

# Ensure station ID columns are string type for consistency
trips_df['start_station_id'] = trips_df['start_station_id'].astype(str)
trips_df['end_station_id'] = trips_df['end_station_id'].astype(str)


# EPSG:4326 = lat/lon
trips_gdf = gpd.GeoDataFrame(
    trips_df,
    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),
    crs='EPSG:4326'
)

stations_gdf = gpd.GeoDataFrame(
    stations_df,
    geometry=gpd.points_from_xy(stations_df['LONGITUDE'], stations_df['LATITUDE']),
    crs='EPSG:4326'
)
# Find nearest station to each ride
trips_with_nearest_station = gpd.sjoin_nearest(
    trips_gdf, stations_gdf[['STATION_ID', 'geometry']],
    how="left", distance_col="distance"
)

# Now we fill missing station_id with nearest one
trips_df['start_station_id'] = trips_df['start_station_id'].fillna(
    trips_with_nearest_station['STATION_ID']
)
# Creating a mapping from STATION_ID to STATION_NAME
id_to_name = stations_df.set_index('STATION_ID')['NAME'].to_dict()

# Fill in missing start_station_name using start_station_id
trips_df['start_station_name'] = trips_df['start_station_name'].fillna(
    trips_df['start_station_id'].map(id_to_name)
)
trips_df_cleaned=trips_df.drop_duplicates().copy() # Added .copy()
trips_df_cleaned.isna().sum()

"""Repeating the process to end id and name"""

trips_gdf_end = gpd.GeoDataFrame(
    trips_df,
    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),
    crs='EPSG:4326'
)

trips_with_nearest_end_station = gpd.sjoin_nearest(
    trips_gdf_end, stations_gdf[['STATION_ID', 'geometry']],
    how="left", distance_col="end_distance"
)

trips_df['end_station_id'] = trips_df['end_station_id'].fillna(
    trips_with_nearest_end_station['STATION_ID']
)
trips_df['end_station_name'] = trips_df['end_station_name'].fillna(
    trips_df['end_station_id'].map(id_to_name)
)
trips_df=trips_df.drop_duplicates().copy() # Added .copy()
trips_df.isna().sum()

"""we will continue inspecting the rest of the data"""

stations_df=stations_df.drop_duplicates()
stations_df.isna().sum()
# we dont need to drop null values here

weather_df=weather_df.drop_duplicates()
weather_df.isna().sum()

parking_zones_gdf.head(2)

parking_zones_gdf=parking_zones_gdf.drop_duplicates()
parking_zones_gdf.isna().sum()

parking_zones_gdf = parking_zones_gdf.drop(columns=['CREATOR', 'CREATED','EDITOR','EDITED'])
# these columns has null values yes...but the are irrelvant to our project so we can just throw them out and "ŸÜÿ±Ÿäÿ≠ ÿ±ÿßÿ≥ŸÜÿß" ü§∑‚Äç‚ôÇÔ∏è
parking_zones_gdf.isna().sum()

"""---
**The outside WDC problem :**
"""

# The bounding box method
DC_LAT_MIN = 38.7916
DC_LAT_MAX = 38.9955
DC_LNG_MIN = -77.1198
DC_LNG_MAX = -76.9094
# Filtering  Points Outside the Bounding Box
out_of_bounds_start = ~(
    (trips_df['start_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &
    (trips_df['start_lng'].between(DC_LNG_MIN, DC_LNG_MAX))
)

out_of_bounds_end = ~(
    (trips_df['end_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &
    (trips_df['end_lng'].between(DC_LNG_MIN, DC_LNG_MAX))
)

# Combine both to detect any trip with at least one bad coordinate
outlier_mask = out_of_bounds_start | out_of_bounds_end
outliers = trips_df[outlier_mask]

# Inspect the Outliers
print(f"Number of outlier trips: {len(outliers)}")
outliers[['ride_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng']].head()
# Total number of trips in the dataset
total_trips = len(trips_df)

# Number of outliers detected
num_outliers = len(outliers)

# Calculate the percentage of outliers
percentage_outliers = (num_outliers / total_trips) * 100

print(f"Percentage of outlier trips: {percentage_outliers:.2f}%")

# we will drop them
trips_df = trips_df[~outlier_mask].reset_index(drop=True)

"""**The duplicated id problem**
____
"""

# checking ride_id
print("Duplicate ride_ids:", trips_df['ride_id'].duplicated().sum())
print("Missing ride_ids:", trips_df['ride_id'].isna().sum())

"""*there an issue with dublicated ride_id so we will only keep the first occurrence*"""

# Keep first occurrence or drop based on your context:
trips_df = trips_df.drop_duplicates(subset='ride_id', keep='first')

print("Null times:", trips_df[['started_at', 'ended_at']].isna().sum())
print("Negative durations:", (trips_df['ended_at'] < trips_df['started_at']).sum())

print("Missing start station:", trips_df['start_station_id'].isna().sum())
print("Missing end station:", trips_df['end_station_id'].isna().sum())

zero_coords = trips_df[(trips_df['start_lat'] == 0) | (trips_df['start_lng'] == 0)]
print("Zero coordinates:", len(zero_coords))

# checkign if rideable_type and member_casual has weird values
print("Ride types:", trips_df['rideable_type'].unique())
print("Member types:", trips_df['member_casual'].unique())

weather_df.head(1)

# first we make sure all the dates are in the same format (by checking the length)
datetime_lengths = weather_df["datetime"].astype(str).apply(len)
print(datetime_lengths.value_counts())
weather_df["date"] = pd.to_datetime(weather_df["datetime"])
print(weather_df["date"].dtype)

trips_df["start_time"] = pd.to_datetime(trips_df["started_at"])
trips_df["end_time"] = pd.to_datetime(trips_df["ended_at"])
# ensuring that CRS is EPSG:4326
if parking_zones_gdf.crs != "EPSG:4326":
    parking_zones_gdf = parking_zones_gdf.to_crs("EPSG:4326")
# Spatial Join to Map Stations to Parking Zones
# Spatial join: add zone info to each station
stations_with_zone = gpd.sjoin(
    stations_gdf,
    parking_zones_gdf[["NAME", "geometry"]],
    how="left",
    predicate="within"
)
# Rename column for clarity
stations_with_zone = stations_with_zone.rename(columns={"zone_name": "residential_zone"})
# Joining Weather Data
# Extract date from start_time for weather join
trips_df["date"] = trips_df["start_time"].dt.date
weather_df["date"] = weather_df["date"].dt.date

# Join weather by date
trips_df = trips_df.merge(weather_df, on="date", how="left")

trips_df[['start_station_id', 'end_station_id', 'start_station_name', 'end_station_name']].isnull().sum()

trips_df.head(2)

"""---

# **Feature engineering**

---

---
B1
---
"""

# From started_at
trips_df['start_year'] = trips_df['started_at'].dt.year
trips_df['start_month'] = trips_df['started_at'].dt.month
trips_df['start_day_num'] = trips_df['started_at'].dt.day
trips_df['start_day_name'] = trips_df['started_at'].dt.day_name()

# From ended_at
trips_df['end_year'] = trips_df['ended_at'].dt.year
trips_df['end_month'] = trips_df['ended_at'].dt.month
trips_df['end_day_num'] = trips_df['ended_at'].dt.day
trips_df['end_day_name'] = trips_df['ended_at'].dt.day_name()

"""
---
B2
---
"""

trips_df['trip_duration_minutes'] = (trips_df['end_time'] - trips_df['start_time']).dt.total_seconds() / 60
trips_df['trip_duration_minutes']=trips_df['trip_duration_minutes'].round(2)
trips_df['trip_duration_minutes'].head(2)

"""**The trip_duration_minutes problem**
___
"""

trips_df['trip_duration_minutes'].describe()

"""*we can clearly see that there is a problem with the tripd_durations, the min is a negative value and that is not right*"""

# Show trips with negative or 0 duration
invalid_durations = trips_df[trips_df['trip_duration_minutes'] <= 0]
print(f"Invalid rows: {len(invalid_durations)}")
invalid_durations[['ride_id', 'started_at', 'ended_at', 'trip_duration_minutes']].head()

# Filter only valid trips
trips_df = trips_df[trips_df['trip_duration_minutes'] > 0]
trips_df['trip_duration_minutes'].describe()

"""---
B3
---

"""

trips_df['member_casual'].value_counts()

trips_df['rideable_type'].value_counts()

# Initialize base cost
# Start with 0 cost
trips_df['trip_cost'] = 0.0

# Define fixed costs
trips_df.loc[trips_df['member_casual'] == 'member', 'trip_cost'] = 3.95
trips_df.loc[trips_df['member_casual'] == 'casual', 'trip_cost'] = 1.00

# Add extra cost for duration
# for members :
# Create condition for member rides longer than 45 mins
cond_member_extra = (trips_df['member_casual'] == 'member') & (trips_df['trip_duration_minutes'] > 45)

# Electric bike extra for members
trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'electric_bike'), 'trip_cost'] += \
    (trips_df['trip_duration_minutes'] - 45) * 0.10

# Classic bike extra for members
trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'classic_bike'), 'trip_cost'] += \
    (trips_df['trip_duration_minutes'] - 45) * 0.05
# Electric bike for casuals
cond_casual_electric = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'electric_bike')
trips_df.loc[cond_casual_electric, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.15

# Classic bike for casuals
cond_casual_classic = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'classic_bike')
trips_df.loc[cond_casual_classic, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.05
# Add Central Business District (CBD) fee
# Preparaing your geometry points
# Create GeoDataFrame of start points
trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)
trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)
# #  Load CBD Polygon
CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')
CBD = CBD.to_crs(epsg=4326)  # Ensures it's in WGS 84


# Convert to GeoDataFrames with correct CRS
start_gdf = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs('EPSG:6933')
end_gdf = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs('EPSG:6933')

# Load CBD polygon and project to EPSG:6933
CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')
CBD = CBD.to_crs(epsg=6933)
cbd_polygon = CBD.geometry.unary_union  # Get full boundary
CBD

# Check spatial containment in EPSG:6933
trips_df['start_in_cbd'] = start_gdf['start_point'].apply(lambda point: point.within(cbd_polygon))
trips_df['end_in_cbd'] = end_gdf['end_point'].apply(lambda point: point.within(cbd_polygon))

# Final condition and cost update
trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']
trips_df.loc[trips_df['in_cbd'], 'trip_cost'] += 0.5
trips_df['trip_cost'].head()

"""**The trips cost problem**
___
"""

trips_df['trip_cost'].describe()

"""*we can see a clear issue in the data ,  and super high values (4.3 mil in the max ) and std is very high (4837.62) , so we must identify this outliers and deal with them*"""

# High-cost trips
high_cost = trips_df[trips_df['trip_cost'] > 1000].copy()
print(high_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])

# Negative-cost trips
neg_cost = trips_df[trips_df['trip_cost'] < 0].copy()
# try to only print the len
print(neg_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])

# Total rows
total_rows = len(trips_df)
# Define thresholds
high_cost_threshold = 10000
negative_cost_threshold = 0

# Find outliers
high_cost_outliers = trips_df[trips_df['trip_cost'] > high_cost_threshold]
negative_cost_outliers = trips_df[trips_df['trip_cost'] < negative_cost_threshold]

# Count
num_high_cost = len(high_cost_outliers)
num_negative_cost = len(negative_cost_outliers)
total_outliers = num_high_cost + num_negative_cost

# Percentages
percent_high_cost = (num_high_cost / total_rows) * 100
percent_negative_cost = (num_negative_cost / total_rows) * 100
percent_total_outliers = (total_outliers / total_rows) * 100

print(f"High cost outliers: {num_high_cost} ({percent_high_cost:.2f}%)")
print(f"Negative cost outliers: {num_negative_cost} ({percent_negative_cost:.2f}%)")
print(f"Total outliers: {total_outliers} ({percent_total_outliers:.2f}%)")

"""since they make a very small amount of the data (0.0%) they can be classifed as false data and we can drop them"""

# Drop outliers by reassigning the filtered DataFrame back to df
trips_df = trips_df[(trips_df['trip_cost'] <= high_cost_threshold) & (trips_df['trip_cost'] >= negative_cost_threshold)]
trips_df['trip_cost'].describe()

"""---
B4
---

"""

stations_df['CAPACITY'].describe()

# Basic histogram using Plotly
fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Distribution of Station Capacity')
fig.update_layout(xaxis_title='Capacity', yaxis_title='Count', bargap=0.1)
fig.show(config={'staticPlot': True})

"""*Choosing the right threshold*"""

# Drop NaNs
capacity_data = stations_df['CAPACITY'].dropna()
# Histogram
hist_data = go.Histogram(x=capacity_data, nbinsx=30, name='Histogram', opacity=0.6)
# Density Curve
kde = gaussian_kde(capacity_data)
x_vals = np.linspace(capacity_data.min(), capacity_data.max(), 1000)
kde_data = go.Scatter(x=x_vals, y=kde(x_vals) * len(capacity_data) * (x_vals[1] - x_vals[0]),
                      mode='lines', name='KDE Curve')

# Plot both
fig = go.Figure(data=[hist_data, kde_data])
fig.update_layout(title='Capacity Distribution with KDE',
                  xaxis_title='Capacity', yaxis_title='Count')
# Example thresholds
low_thresh = stations_df['CAPACITY'].quantile(0.30)
high_thresh = stations_df['CAPACITY'].quantile(0.66)
print(low_thresh,high_thresh)
fig.add_vline(x=low_thresh, line_dash="dash", line_color="green", annotation_text="Small/Average")
fig.add_vline(x=high_thresh, line_dash="dash", line_color="red", annotation_text="Average/Large")

fig.show(config={'staticPlot': True})

"""Method 1 : using quantiles
____
"""

# Calculate the thresholds
low_thresh = stations_df['CAPACITY'].quantile(0.33)
high_thresh = stations_df['CAPACITY'].quantile(0.66)

def classify_capacity(cap):
    if cap <= low_thresh:
        return 'Small'
    elif cap <= high_thresh:
        return 'Average'
    else:
        return 'Large'

stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)
stations_df['STATION_SIZE'].value_counts()

"""Method 2 : based on domain knowledge
___
"""

def classify_capacity(cap):
    if cap <= 12: # these threshold are taken from the sources provided in the report
        return 'Small'
    elif cap <= 29:
        return 'Average'
    else:
        return 'Large'

stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)
stations_df['STATION_SIZE'].value_counts()
print(stations_df['STATION_SIZE'].value_counts())

"""combine with trips df

try1 : using the station_id
"""

# Step 2: Create a simplified DataFrame for merging
station_size_map = stations_df[['STATION_ID', 'STATION_SIZE']].copy()
# Step 3: Merge for start_station_size
trips_df = trips_df.merge(
    station_size_map.rename(columns={
        'STATION_ID': 'start_station_id',
        'STATION_SIZE': 'start_station_size'
    }),
    on='start_station_id',
    how='left'
)

# Step 4: Merge for end_station_size
trips_df = trips_df.merge(
    station_size_map.rename(columns={
        'STATION_ID': 'end_station_id',
        'STATION_SIZE': 'end_station_size'
    }),
    on='end_station_id',
    how='left'
)

# Now perform sampling after merging station size information
sample_size = 50000  # Choose a sample size appropriate for your analysis needs and environment
if 'trips_df' in locals() and len(trips_df) > sample_size:
    sampled_df = trips_df.sample(n=sample_size, random_state=42).copy() # Added .copy()
    print(f"\nCreated a sampled DataFrame with {len(sampled_df)} rows.")
    print("Displaying info for the sampled DataFrame:")
    sampled_df.info()
elif 'trips_df' not in locals():
    print("\n'trips_df' DataFrame not found. Please ensure it is loaded before sampling.")
else:
    print(f"\nTrips DataFrame has only {len(trips_df)} rows, less than the requested sample size {sample_size}.")
    print("Using the full DataFrame for analysis.")
    sampled_df = trips_df.copy() # Added .copy()

print("Missing start_station_size:", trips_df['start_station_size'].isna().sum())
print("Missing end_station_size:", trips_df['end_station_size'].isna().sum())

# What kind of start_station_id had no match?
print(trips_df[trips_df['start_station_size'].isna()][['start_station_id']].drop_duplicates().head(10))

# Same for end_station
print(trips_df[trips_df['end_station_size'].isna()][['end_station_id']].drop_duplicates().head(10))

"""try2 : with names"""

stations_df['NAME'] = stations_df['NAME'].str.strip().str.lower()
trips_df['start_station_name'] = trips_df['start_station_name'].str.strip().str.lower()
trips_df['end_station_name'] = trips_df['end_station_name'].str.strip().str.lower()

# Map start station size using name
station_size_name_map = stations_df[['NAME', 'STATION_SIZE']]

trips_df = trips_df.merge(
    station_size_name_map.rename(columns={'NAME': 'start_station_name', 'STATION_SIZE': 'start_station_size_name'}),
    on='start_station_name',
    how='left'
)

# Same for end station
trips_df = trips_df.merge(
    station_size_name_map.rename(columns={'NAME': 'end_station_name', 'STATION_SIZE': 'end_station_size_name'}),
    on='end_station_name',
    how='left'
)

trips_df['end_station_size'].value_counts()

trips_df['start_station_size'] = trips_df['start_station_size_name']
trips_df['end_station_size'] = trips_df['end_station_size_name']

# Then drop the temp columns
trips_df.drop(columns=['start_station_size_name', 'end_station_size_name'], inplace=True)

print(trips_df[['start_station_size', 'end_station_size']].isna().sum())

"""using names was better but we still have some null values"""

fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Station Capacity Distribution')
fig.add_vline(x=12, line_dash="dash", line_color="green", annotation_text="Small/Average")
fig.add_vline(x=29, line_dash="dash", line_color="red", annotation_text="Average/Large")
fig.show(config={'staticPlot': True})

"""---
B5
---


"""

Shuttle_Bus_Stops.isna().sum()

Metro_Bus_Stops['BSTP_LAT'].isna().sum()

"""Approaches


---


| Approach                    | Time Complexity | Vectorized | Fast    |
| --------------------------- | --------------- | ---------- | ------- |
| Brute Force (Your original) | O(N √ó M)        |  No       |  Slow |
| BallTree (New)              | O(N log M)      |  Yes      |  Fast  |

*We first tried the brute force method and it sucked üò§ it felt like watching valverde plays, so we went with the other approach*

Project all your coordinates to EPSG:6933
"""

# Create start and end point geometries
trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)
trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)

# Create GeoDataFrames
gdf_start = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs(epsg=6933)
gdf_end = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs(epsg=6933)

# Add x/y columns
trips_df['start_x'] = gdf_start.geometry.x
trips_df['start_y'] = gdf_start.geometry.y
trips_df['end_x'] = gdf_end.geometry.x
trips_df['end_y'] = gdf_end.geometry.y




# Convert station lat/lng to projected coordinates
def project_coords(coords_list):
    gdf = gpd.GeoDataFrame(geometry=[Point(lon, lat) for lat, lon in coords_list], crs='EPSG:4326')
    gdf = gdf.to_crs(epsg=6933)
    return np.array([(geom.x, geom.y) for geom in gdf.geometry])
# coords
# Metro stop coordinates
metro_coords = Metro_Bus_Stops[['BSTP_LAT', 'BSTP_LON']].dropna().values

# Shuttle stop coordinates
shuttle_coords = Shuttle_Bus_Stops[['LATITUDE', 'LONGITUDE']].dropna().values

# projecting   metro and shuttle station coordinates:
metro_coords_projected = project_coords(metro_coords)
shuttle_coords_projected = project_coords(shuttle_coords)

def euclidean_tree_batch(source_df, stop_coords, x_col, y_col, batch_size=10000):
    tree = BallTree(stop_coords, metric='euclidean')

    distances = []
    n = len(source_df)
    tqdm.pandas(desc=f"Computing distances for {x_col}")

    for i in tqdm(range(0, n, batch_size), desc="Batch processing", unit="batch"):
        batch = source_df.iloc[i:i+batch_size]
        batch_points = batch[[x_col, y_col]].values

        dists, _ = tree.query(batch_points, k=1)
        distances.extend(dists.flatten().tolist())

    return distances

# Start ‚Üí Metro
trips_df['start_nearest_metro_distance'] = euclidean_tree_batch(
    trips_df, metro_coords_projected, 'start_x', 'start_y'
)

# End ‚Üí Metro
trips_df['end_nearest_metro_distance'] = euclidean_tree_batch(
    trips_df, metro_coords_projected, 'end_x', 'end_y'
)

# Start ‚Üí Shuttle
trips_df['start_nearest_shuttle_distance'] = euclidean_tree_batch(
    trips_df, shuttle_coords_projected, 'start_x', 'start_y'
)

# End ‚Üí Shuttle
trips_df['end_nearest_shuttle_distance'] = euclidean_tree_batch(
    trips_df, shuttle_coords_projected, 'end_x', 'end_y'
)

# converting to from meters to km (our choice)
trips_df['start_nearest_metro_distance'] = trips_df['start_nearest_metro_distance'] / 1000
trips_df['end_nearest_metro_distance'] = trips_df['end_nearest_metro_distance'] / 1000
trips_df['start_nearest_shuttle_distance'] = trips_df['start_nearest_shuttle_distance'] / 1000
trips_df['end_nearest_shuttle_distance'] = trips_df['end_nearest_shuttle_distance'] / 1000

trips_df[
    ['start_nearest_metro_distance',
     'end_nearest_metro_distance',
     'start_nearest_shuttle_distance',
     'end_nearest_shuttle_distance']
].describe()

sampled_df = trips_df.sample(n=20000, random_state=50)

cols = ['start_nearest_metro_distance', 'end_nearest_metro_distance',
        'start_nearest_shuttle_distance', 'end_nearest_shuttle_distance']
for col in cols:
    fig = go.Figure(
        data=[go.Histogram(
            x=sampled_df[col],
            nbinsx=100,
            marker=dict(color='skyblue'),
            opacity=0.75
        )]
    )
    fig.update_layout(
        title=col,
        xaxis_title=col,
        yaxis_title='Count (Log Scale)',
        yaxis_type='log',
        bargap=0.1,
        width=800,
        height=400
    )
    fig.show(config={'staticPlot':True})

# start_nearest_metro_distance_thr = 1 # kmeters
# end_nearest_metro_distance_thr = 1   # kmeters
# start_nearest_shuttle_distance_thr = 9 # kmeters
# end_nearest_shuttle_distance_thr = 9 # kmeters

# far_metro_start_trips = trips_df[trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr]
# far_metro_end_trips = trips_df[trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr]
# far_shuttle_start_trips = trips_df[trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr]
# far_shuttle_end_trips = trips_df[trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr]

# # Combine all "far from transit" trips for a general map
# far_from_transit_trips = trips_df[
#     (trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr) |
#     (trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr) |
#     (trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr) |
#     (trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr)
# ].copy()

# print(f"Number of trips far from metro (start): {len(far_metro_start_trips)}")
# print(f"Number of trips far from metro (end): {len(far_metro_end_trips)}")
# print(f"Number of trips far from shuttle (start): {len(far_shuttle_start_trips)}")
# print(f"Number of trips far from shuttle (end): {len(far_shuttle_end_trips)}")
# print(f"Total unique trips identified as 'far from transit': {len(far_from_transit_trips)}")


# # Creating a Folium Map

# # Get the approximate center of Washington D.C. for the map's initial view
# dc_center_lat = trips_df['start_lat'].mean()
# dc_center_lng = trips_df['start_lng'].mean()

# # Create a base map
# m = folium.Map(location=[dc_center_lat, dc_center_lng], zoom_start=12)
# marker_cluster_start = MarkerCluster().add_to(m)
# marker_cluster_end = MarkerCluster().add_to(m)

# # Add start points
# for idx, row in far_from_transit_trips.iterrows():
#     if pd.notnull(row['start_lat']) and pd.notnull(row['start_lng']):
#         folium.CircleMarker(
#             location=[row['start_lat'], row['start_lng']],
#             radius=2,
#             color='red',
#             fill=True,
#             fill_color='red',
#             fill_opacity=0.6,
#             tooltip=f"Start: {row['start_station_name']} (Far from Transit)"
#         ).add_to(marker_cluster_start)
# m
import plotly.express as px

# Define thresholds
start_nearest_metro_distance_thr = 1  # km
end_nearest_metro_distance_thr = 1    # km
start_nearest_shuttle_distance_thr = 9  # km
end_nearest_shuttle_distance_thr = 9    # km

# Filter trips
far_metro_start_trips = trips_df[trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr]
far_metro_end_trips = trips_df[trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr]
far_shuttle_start_trips = trips_df[trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr]
far_shuttle_end_trips = trips_df[trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr]

# Combine for general map
far_from_transit_trips = trips_df[
    (trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr) |
    (trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr) |
    (trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr) |
    (trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr)
].copy()

# Print summary
print(f"Number of trips far from metro (start): {len(far_metro_start_trips)}")
print(f"Number of trips far from metro (end): {len(far_metro_end_trips)}")
print(f"Number of trips far from shuttle (start): {len(far_shuttle_start_trips)}")
print(f"Number of trips far from shuttle (end): {len(far_shuttle_end_trips)}")
print(f"Total unique trips identified as 'far from transit': {len(far_from_transit_trips)}")

# Drop rows with missing lat/lng
plot_df = far_from_transit_trips.dropna(subset=['start_lat', 'start_lng'])

# Plotly Map
fig = px.scatter_mapbox(
    plot_df,
    lat='start_lat',
    lon='start_lng',
    color_discrete_sequence=['red'],
    hover_name='start_station_name',
    hover_data={'start_lat': True, 'start_lng': True},
    zoom=11,
    height=600,
    title='Trips Starting Far from Transit Points'
)

# Set map style and access token (you can also use open-street-map without token)
fig.update_layout(mapbox_style='open-street-map')  # or 'carto-positron', 'carto-darkmatter'
fig.update_layout(margin={'r':0,'t':40,'l':0,'b':0})

fig.show(config={'staticPlot': True})

"""**An argument:**<br>
in here we had two choices , we could have droped these values and that was our first instinct, but after rethinking ... <br>
these distances are still in washington , and they are real data , and the numbers are not that crazy<br>
so upon rethinking we decided to keep it but flag it in a new binary features , so they are available for further analysis if needed
____
"""

new_metro_start_thr = 1  # kmeters
new_metro_end_thr = 1    # kmeters
new_shuttle_start_thr = 9 # kmeters
new_shuttle_end_thr = 9 # kmeters

#  Create the new boolean features
trips_df['is_far_from_metro_start'] = trips_df['start_nearest_metro_distance'] > new_metro_start_thr
trips_df['is_far_from_metro_end'] = trips_df['end_nearest_metro_distance'] > new_metro_end_thr
trips_df['is_far_from_shuttle_start'] = trips_df['start_nearest_shuttle_distance'] > new_shuttle_start_thr
trips_df['is_far_from_shuttle_end'] = trips_df['end_nearest_shuttle_distance'] > new_shuttle_end_thr

trips_df['is_far_from_any_transit'] = (
    trips_df['is_far_from_metro_start'] |
    trips_df['is_far_from_metro_end'] |
    trips_df['is_far_from_shuttle_start'] |
    trips_df['is_far_from_shuttle_end']
)
print("\nCounts for new 'far from' features:")
print(trips_df[['is_far_from_metro_start', 'is_far_from_metro_end',
                'is_far_from_shuttle_start', 'is_far_from_shuttle_end',
                'is_far_from_any_transit']].sum())

"""---
B6
---

"""

print(trips_df['start_point'].iloc[0], type(trips_df['start_point'].iloc[0]))
print(trips_df['end_point'].iloc[0], type(trips_df['end_point'].iloc[0]))
print(type(cbd_polygon))

CBD = CBD.to_crs(epsg=6933)
cbd_polygon = CBD.geometry.iloc[0]

# Rebuilding the point geometries from lat/lng in EPSG:4326
start_gdf = gpd.GeoDataFrame(
    trips_df,
    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),
    crs="EPSG:4326"
)

end_gdf = gpd.GeoDataFrame(
    trips_df,
    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),
    crs="EPSG:4326"
)


# Projecting everything to EPSG:6933
CBD = CBD.to_crs(epsg=6933)
start_gdf = start_gdf.to_crs(epsg=6933)
end_gdf = end_gdf.to_crs(epsg=6933)

# CBD polygon (in same projection)
cbd_polygon = CBD.geometry.unary_union
# Checking containment
trips_df['start_in_cbd'] = start_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))
trips_df['end_in_cbd']   = end_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))

# Final result
trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']
trips_df['in_cbd'].value_counts()

"""---
B7
---

"""

# Compute the CBD centroid (already in EPSG:6933)
cbd_centroid = cbd_polygon.centroid  # geometry in meters (EPSG:6933)

#Recreate end point GeoDataFrame and project to EPSG:6933
end_gdf = gpd.GeoDataFrame(
    trips_df,
    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),
    crs="EPSG:4326"
).to_crs(epsg=6933)

# Compute Euclidean distance in meters
trips_df['distance_to_cbd_m'] = end_gdf.geometry.distance(cbd_centroid)

#Set distance to None where start AND end are in the CBD
mask = trips_df['start_in_cbd'] & trips_df['end_in_cbd']
trips_df.loc[mask, 'distance_to_cbd_m'] = None

trips_df['distance_to_cbd_m'].describe()

"""**Threasholding strategies**
___

kinda of an elbow method
"""

sampled_df = trips_df.sample(n=20000, random_state=50)

# Extract the data
data = sampled_df['distance_to_cbd_m'].dropna()

# Create histogram trace
hist = go.Histogram(
    x=data,
    nbinsx=100,
    name='Histogram',
    marker_color='lightblue',
    opacity=0.75
)

# Create KDE line (manual since Plotly doesn‚Äôt support KDE directly)
kde = gaussian_kde(data)
x_vals = np.linspace(data.min(), data.max(), 1000)
kde_vals = kde(x_vals) * len(data) * (x_vals[1] - x_vals[0])  # scale to match histogram

kde_trace = go.Scatter(
    x=x_vals,
    y=kde_vals,
    mode='lines',
    name='KDE',
    line=dict(color='darkblue')
)

# Vertical reference lines
vline1 = go.Scatter(
    x=[2000, 2000],
    y=[0, max(kde_vals)],
    mode='lines',
    name='2km Threshold',
    line=dict(color='red', dash='dash')
)

vline2 = go.Scatter(
    x=[2764, 2764],
    y=[0, max(kde_vals)],
    mode='lines',
    name='Median',
    line=dict(color='green', dash='dash')
)

# Create the figure
fig = go.Figure(data=[hist, kde_trace, vline1, vline2])

# Update layout
fig.update_layout(
    title='Distance to CBD at End of Trip',
    xaxis_title='distance_to_cbd_m',
    yaxis_title='Count',
    width=800,
    height=500,
    legend=dict(x=0.7, y=0.95)
)

fig.show( config={'staticPlot':True})

"""*i will choose the median beacause looking at the histogram we can see the counts drops*

"""

threshold = 2764
# Apply binary classification
trips_df['close_to_cbd'] = trips_df['distance_to_cbd_m'].apply(
    lambda d: None if pd.isna(d) else d <= threshold
)
trips_df['close_to_cbd'].value_counts()

print(trips_df['close_to_cbd'].isna().sum())

"""---
B8
---

Washington, D.C. is roughly:

~16 km (north-south)

~13 km (east-west)

So, a geohash precision of 5‚Äì8 is appropriate.
"""

def encode_geohashes(df, lat_col, lon_col, precisions):
    for p in precisions:
        col_name = f'geohash_p{p}'
        df[col_name] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lon_col], p), axis=1)
    return df

# Try precisions from 5 to 8
precisions_to_test = [5, 6, 7, 8]
trips_df = encode_geohashes(trips_df, 'start_lat', 'start_lng', precisions_to_test)
for p in precisions_to_test:
    print(f"Precision {p}: {trips_df[f'geohash_p{p}'].nunique()} unique regions")
"""
If the number is too small ‚Üí  over-aggregating.

If it's too big (e.g. thousands) ‚Üí too fine ‚Üí hard to summarize meaningfully.
"""

for p in precisions_to_test:
    counts = trips_df[f'geohash_p{p}'].value_counts()
    print(f"Precision {p} ‚Üí median trips per geohash: {counts.median()}")
"""
This tells us how balanced the spatial bins are.

we ideally want 50-500 trips per cell.
"""

# we will choose 6t
trips_df['geohash_sector'] = trips_df['geohash_p6']
trips_df.drop(columns='geohash_p6', inplace=True)

"""---

B9
---

"""

# Group by Sector and Date
trips_df['date'] = pd.to_datetime(trips_df['date'])

# Count trips per day per sector
daily_counts = trips_df.groupby(['geohash_sector', 'date']).size().reset_index(name='trip_count')

# Now compute average daily trips per geohash sector
avg_daily_trips = daily_counts.groupby('geohash_sector')['trip_count'].mean().reset_index()
avg_daily_trips.rename(columns={'trip_count': 'avg_daily_trips'}, inplace=True)

"""Choose Segmentation Method (for Red / Yellow / Gray)

| Method                         | Description                          | Pros             | Use Case             |
| ------------------------------ | ------------------------------------ | ---------------- | -------------------- |
| **Quantiles** (e.g., tertiles) | Divide into 3 equal-sized groups     | Simple, fair     | Balanced datasets    |
| **KMeans Clustering (k=3)**    | Machine learning-based segmentation  | Optimal grouping | Large datasets       |
"""

# quantiles :
# Assign labels based on quantiles
quantiles = avg_daily_trips['avg_daily_trips'].quantile([1/3, 2/3])
low_thresh = quantiles.iloc[0]
high_thresh = quantiles.iloc[1]

def classify_volume(val):
    if val < low_thresh:
        return 'gray'   # Low volume
    elif val < high_thresh:
        return 'yellow' # Medium volume
    else:
        return 'red'    # High volume

avg_daily_trips['volume_segment'] = avg_daily_trips['avg_daily_trips'].apply(classify_volume)

# Extract the data
data = avg_daily_trips['avg_daily_trips'].dropna()

# Histogram trace
hist = go.Histogram(
    x=data,
    nbinsx=30,
    marker_color='lightblue',
    opacity=0.75,
    name='Avg Daily Trips'
)

# Vertical threshold lines
vline_low = go.Scatter(
    x=[low_thresh, low_thresh],
    y=[0, data.value_counts().max()],
    mode='lines',
    name='Low Threshold',
    line=dict(color='gray', dash='dash')
)

vline_high = go.Scatter(
    x=[high_thresh, high_thresh],
    y=[0, data.value_counts().max()],
    mode='lines',
    name='High Threshold',
    line=dict(color='orange', dash='dash')
)

# Combine into figure
fig = go.Figure(data=[hist, vline_low, vline_high])

# Update layout
fig.update_layout(
    title='Distribution of Avg Daily Trips per Geohash Sector',
    xaxis_title='Avg Daily Trips',
    yaxis_title='Count',
    width=800,
    height=500,
    bargap=0.1
)

fig.show(config={'staticPlot':True})

X = avg_daily_trips[['avg_daily_trips']].values

kmeans = KMeans(n_clusters=3, random_state=42  , n_init=10).fit(X)
avg_daily_trips['kmeans_label'] = kmeans.labels_

# Map to red/yellow/gray using sorted cluster means
label_map = dict(zip(
    np.argsort(kmeans.cluster_centers_.flatten()),
    ['gray', 'yellow', 'red']
))
avg_daily_trips['kmeans_segment'] = avg_daily_trips['kmeans_label'].map(label_map)

avg_daily_trips.head()

trips_df['geohash_sector'].nunique()

# Merge segments into trips_df
trips_df = trips_df.merge(
    avg_daily_trips[['geohash_sector','volume_segment','kmeans_segment']],
    on='geohash_sector',
    how='left'
)

trips_df['kmeans_segment'].value_counts()

trips_df['volume_segment'].value_counts()

"""

---

B10
----"""

trips_df['conditions'].value_counts()

def classify_weather(condition):
    condition = condition.lower()
    if 'rain' in condition or 'snow' in condition:
        return 'rainy'
    elif 'overcast' in condition or 'cloudy' in condition:
        return 'cloudy'
    elif 'clear' in condition:
        return 'sunny'
    else:
        return 'unknown'


trips_df['weather_segment'] = trips_df['conditions'].apply(classify_weather)
trips_df['weather_segment'].value_counts()

"""---

B11
---

quick inspection of the data dates
"""

sorted_ended_at_df = trips_df[['ended_at']].sort_values(by='ended_at')
print("--- Sorted 'ended_at' DataFrame (first 5 rows) ---")
print(sorted_ended_at_df.head())
print("\n")

#  Find the earliest and latest dates
earliest_date = sorted_ended_at_df['ended_at'].min()
latest_date = sorted_ended_at_df['ended_at'].max()

print(f"The earliest date in 'ended_at' is: {earliest_date}")
print(f"The latest date in 'ended_at' is: {latest_date}")

#Sort the DataFrame by 'started_at'
sorted_started_at_df = trips_df[['started_at']].sort_values(by='started_at')
print("--- Sorted 'started_at' DataFrame (first 5 rows) ---")
print(sorted_started_at_df.head())
print("\n")

# Find the earliest and latest dates using 'started_at'
earliest_date_started = sorted_started_at_df['started_at'].min()
latest_date_started = sorted_started_at_df['started_at'].max()

print(f"The earliest date in 'started_at' is: {earliest_date_started}")
print(f"The latest date in 'started_at' is: {latest_date_started}")

trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'], format='mixed', errors='coerce')


# Extract just the date (without time)
trips_df['end_date'] = trips_df['ended_at'].dt.date
daily_income_weather = trips_df.groupby(['end_date', 'weather_segment'])['trip_cost'].sum().reset_index()

# convert

daily_income_weather['end_date'] = pd.to_datetime(daily_income_weather['end_date'])

fig_long = px.line(
    daily_income_weather,
    x='end_date',
    y='trip_cost',
    color='weather_segment',
    title='Daily Total Trip Cost by Weather Condition (Long Format)',
    labels={'end_date': 'Date', 'trip_cost': 'Total Income', 'weather_segment': 'Weather'}
)

fig_long.update_layout(xaxis_title='Date', yaxis_title='Trip Cost', hovermode='x unified')
fig_long.show(config={'staticPlot':True})

# Pivot to wide format
wide_df = daily_income_weather.pivot(index='end_date', columns='weather_segment', values='trip_cost').fillna(0)
wide_df = wide_df.sort_index()

fig_wide = go.Figure()

for condition in wide_df.columns:
    fig_wide.add_trace(go.Scatter(
        x=wide_df.index,
        y=wide_df[condition],
        mode='lines',
        name=condition
    ))

fig_wide.update_layout(
    title='Daily Total Trip Cost by Weather Condition (Wide Format)',
    xaxis_title='Date',
    yaxis_title='Trip Cost',
    hovermode='x unified',
    template='plotly_white',
    legend_title='Weather'
)

fig_wide.show(config={'staticPlot':True})

"""Which one is better for our problem  ?<br>
the Long Format is the most suitable and effective,This is because it allows for direct visual comparison of revenue trends across different weather types over time on a single graph, making it easier to spot patterns and seasonal impacts. The long format is also considered more intuitive for time-series visualization. Conversely, the "wide format" is deemed less clear due to potential visual clutter when many categories are present.

---
B12
---

Feature 1 : rush_hour
<br> Indicates if the ride occurred during typical commuting hours (7‚Äì10 AM or 4‚Äì7 PM).
"""

trips_df['start_time'] = pd.to_datetime(trips_df['start_time'], errors='coerce')

trips_df['rush_hour'] = (
    trips_df['start_time'].dt.hour.between(7, 10) |
    trips_df['start_time'].dt.hour.between(16, 19)
).astype(int)
trips_df['rush_hour'].value_counts()

"""Feature 2 : hour_segment <br>
Categorize ride start times into broader buckets.
"""

def get_hour_segment(hour):
    if 5 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 17:
        return 'Midday'
    elif 17 <= hour < 21:
        return 'Evening'
    else:
        return 'Night'

trips_df['hour_segment'] = trips_df['start_time'].dt.hour.apply(get_hour_segment)
trips_df['hour_segment'].value_counts()

"""Feature 3 : is_weekend<br>
Helps spot usage patterns on weekends vs weekdays.
"""

trips_df['is_weekend'] = trips_df['start_time'].dt.dayofweek >= 5
trips_df['is_weekend'] = trips_df['is_weekend'].astype(int)
trips_df['is_weekend'].value_counts()

"""Feature 4 : is_round_trip<br>
helps spot if a trip started and ended at the same station which could be an intersting pattern
"""

trips_df = trips_df.assign(
    is_round_trip=lambda df: df['start_station_id'] == df['end_station_id']
)

"""Feature 5 : is holiday
to get better understanding of bikesharing during holidays
"""

import holidays

trips_df['date'] = pd.to_datetime(trips_df['date'], errors='coerce')

trips_df['is_holiday'] = trips_df['date'].dt.date.apply(lambda x: x in us_holidays)
trips_df['holiday_name'] = trips_df['date'].dt.date.apply(lambda x: us_holidays.get(x) if x in us_holidays else 'Not Holiday')

trips_df['is_holiday'].value_counts()

"""# Final check"""

trips_df.info()

# prompt: i want to upload my trips_df as a csv file to my drive

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define the path where you want to save the CSV in your Drive
# 'MyDrive' is the root of your Google Drive
output_path = '/content/drive/MyDrive/trips_df_final.csv'

# Save the DataFrame to a CSV file in your Google Drive
trips_df.to_csv(output_path, index=False)

print(f"Successfully saved trips_df to {output_path}")

"""---


# **EDA**


---

---
Sampling the data
---
"""

# Sampled data stats
# sampled_df = trips_df.sample(n=20000, random_state=50)
sampled_df=trips_df.copy()
sampled_df.info()

"""---

# A )


---

## Task 1: Top 5 Starting Stations Analysis

### Objective
Identify the top 5 stations with the highest number of trip departures (starting stations) and create a bar chart showing statistical information for these top 5 stations.

### Requirements
- Identify the top 5 stations with the highest number of trip departures (starting stations)
- Create a bar chart showing statistical information for these top 5 stations
- Display the count of trips starting from each station
"""

start_station_counts = sampled_df['start_station_name'].value_counts()
top_5_start_stations = start_station_counts.head(5)

print("Top 5 Starting Stations:")
print(top_5_start_stations)

fig = px.bar(
    x=top_5_start_stations.index,
    y=top_5_start_stations.values,
    title='Top 5 Starting Stations by Trip Count',
    labels={'x': 'Station Name', 'y': 'Number of Trips'},
    color=top_5_start_stations.values,
    color_continuous_scale=px.colors.sequential.Viridis
)

fig.update_layout(
    xaxis={'categoryorder':'total descending'},
    xaxis_title='Station Name',
    yaxis_title='Number of Trips',
    hovermode='x unified'
)
fig.show(config={'staticPlot': True})

"""### Top 5 Starting Stations by Trip Count

**Key Insight:**

* The station **"Park Rd & Holmead Pl NW"** is by far the **most popular starting point**

**Detailed Breakdown:**

* The remaining top 4 stations:

  * **"14th & Belmont St NW"**
  * **"18th St & Wyoming Ave NW"**
  * **"17th & p st nw"**
  * **"Lamont & Mt Pleasant NW"**

  Each of these falls in the **69k - 58k trip range**, indicating moderate popularity and relatively close usage levels among them.

**Implication:**

* The high volume at **Park Rd & Holmead Pl NW** could reflect factors such as:

  * Proximity to residential or commercial zones
  * Availability of bike lanes or connectivity
  * Strategic location near transit hubs or universities

This station likely plays a **central role in trip generation**, which may warrant priority in terms of maintenance, expansions, or targeted promotions.

---

## Task 2: Trip Distribution by Bike Type and Membership

### Objective
Calculate the distribution of trips by bike type (classic vs electric) and membership type (member vs casual), then create a single bar chart showing these distributions.

### Requirements
- Calculate the distribution of trips by:
  - Bike type (classic vs electric)
  - Membership type (member vs casual)
- Create one bar chart showing these distributions
"""

distribution = sampled_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='count')

fig = px.bar(
    distribution,
    x='rideable_type',
    y='count',
    color='member_casual',
    barmode='group',
    title='Trip Distribution by Bike Type and Membership',
    labels={'rideable_type': 'Bike Type', 'count': 'Number of Trips', 'member_casual': 'Membership Type'}
)

fig.update_layout(xaxis_title='Bike Type', yaxis_title='Number of Trips')
fig.show(config={'staticPlot': True})

"""### Trip Distribution by Bike Type and Membership

**Key Insights:**

#### 1. **Electric bikes are the dominant preference:**

* **Members** show a strong preference for **electric bikes** , much higher than classic bikes.
* Even **casual users** prefer electric bikes  over classic ones.

#### 2. **Membership significantly boosts usage:**

* Both bike types have **higher trip counts among members** than casual users.

  * Regular commuting behavior
  * Higher cost-effectiveness for frequent riders
  * Greater engagement with the system

#### 3. **Classic bikes are less used overall:**

* Classic bikes saw fewer total trips for both user types, particularly from casual riders, possibly due to:

  * Higher effort required
  * Less appeal in convenience and speed

**Implication:**

* The system should consider **prioritizing electric bike availability**, especially in high-traffic stations and for members.
* **Member-focused incentives** and  **electric bike maintenance** should be top priorities to support usage trends.

---

## Task 3: Sunburst Chart for Top 5 Starting Stations

### Objective
Create a sunburst chart showing the breakdown of trips for the top 5 starting stations with the hierarchy: Station ‚Üí Bike Type ‚Üí Membership Type. This will show how trips are distributed across bike types and membership types for each top station.

### Requirements
- Create a sunburst chart showing the breakdown of trips for the top 5 starting stations
- Show the hierarchy: Station ‚Üí Bike Type ‚Üí Membership Type
- Display how trips are distributed across bike types and membership types for each top station
"""

trips_top5_stations = sampled_df[sampled_df['start_station_name'].isin(top_5_start_stations.index)].copy()

sunburst_data = trips_top5_stations.groupby(['start_station_name', 'rideable_type', 'member_casual']).size().reset_index(name='count')

fig_sunburst = px.sunburst(
    sunburst_data,
    path=['start_station_name', 'rideable_type', 'member_casual'],
    values='count',
    title='Trip Breakdown for Top 5 Starting Stations by Bike and Membership Type'
)

fig_sunburst.update_layout(
    margin=dict(t=0, l=0, r=0, b=0),
    title_x=0.5
)
fig_sunburst.show(config={'staticPlot': True})

"""1. Electric bikes are the reigning champions at most stations, especially favored by members. This highlights a clear preference for powered rides.

2. Park Rd & Holmead Pl NW is the undisputed leader in trip volume, particularly for electric bike users who are members. It's a critical hub for the system.

3. Members are the backbone of ridership, consistently taking more trips than casual users across all stations, indicating strong loyalty.

4. Classic bike usage by casuals is less...much less, often appearing as a thin slice, suggesting limited appeal to non-regular riders.

5. 17th & P St NW stands out for its showes more usage classic bikes than others>

In essence, the chart reveals a system largely driven by loyal members on electric bikes, with a few stations acting as major arteries.

## Task 4: Station Capacity Analysis

### Objective
Create a histogram showing the distribution of station capacities, then create a bar chart showing trip distribution by station capacity categories (small, medium, large). Stations need to be categorized into capacity groups first.

### Requirements
- Create a histogram showing the distribution of station capacities
- Create a bar chart showing trip distribution by station capacity categories (small, medium, large)
- Categorize stations into capacity groups first
"""

# Merge station size back to trips_df
# Ensure station_id types match
stations_df['STATION_ID'] = stations_df['STATION_ID'].astype(sampled_df['start_station_id'].dtype)

# Merge station size to trips_df based on start station
trips_with_station_size = sampled_df.merge(
    stations_df[['STATION_ID', 'STATION_SIZE']],
    left_on='start_station_id',
    right_on='STATION_ID',
    how='left'
)

# Rename column for clarity
trips_with_station_size.rename(columns={'STATION_SIZE': 'start_station_size'}, inplace=True)

# Merge station size based on end station as well
trips_with_station_size = trips_with_station_size.merge(
    stations_df[['STATION_ID', 'STATION_SIZE']],
    left_on='end_station_id',
    right_on='STATION_ID',
    how='left',
    suffixes=('', '_end_station') # Add suffix for the end station size column
)
trips_with_station_size.rename(columns={'STATION_SIZE': 'end_station_size'}, inplace=True)

# Drop the redundant STATION_ID columns from the merges
trips_with_station_size.drop(columns=['STATION_ID', 'STATION_ID_end_station'], errors='ignore', inplace=True)


# Count trips by start station capacity category
trip_distribution_by_capacity = trips_df['start_station_size'].value_counts().reset_index()
trip_distribution_by_capacity.columns = ['Station Capacity Category', 'Number of Trips']

# Define category order
category_order = ['Small', 'Average', 'Large']
trip_distribution_by_capacity['Station Capacity Category'] = pd.Categorical(
    trip_distribution_by_capacity['Station Capacity Category'], categories=category_order, ordered=True
)
trip_distribution_by_capacity = trip_distribution_by_capacity.sort_values('Station Capacity Category')


# Create bar chart
fig = px.bar(
    trip_distribution_by_capacity,
    x='Station Capacity Category',
    y='Number of Trips',
    title='Trip Distribution by Start Station Capacity Category',
    labels={'Station Capacity Category': 'Start Station Size', 'Number of Trips': 'Number of Trips'},
    color='Station Capacity Category',
    category_orders={'Station Capacity Category': category_order} # Enforce the order
)

fig.update_layout(xaxis_title='Start Station Capacity Category', yaxis_title='Number of Trips')
# fig.write_image("Trip Distribution by Start Station Capacity Category.png")
# files.download("Trip Distribution by Start Station Capacity Category.png")
fig.show(config={'staticPlot': True})

"""### Trip Distribution by Start Station Capacity Category


* **Small**
* **Average**
* **Large**

#### **Key Insights:**

1. **Average-Capacity Stations Lead in Usage**

   * With nearly **4.9m trips**, average-size stations dominate trip origination.
   * This dominance suggests that stations of "average" capacity are optimally sized and/or strategically located to meet the majority of user demand. They are the backbone of the bike-sharing network.

2. **Small Stations Show Strong Engagement**

   * Small stations generate the least number of trips overall, approximately 250,000 trips.
   * While having the lowest total trips, their contribution is significant relative to their capacity. This suggests efficient utilization of limited space.
   *

3. **Underperformance of Large Stations**

   * Contrary to what one might expect, large-capacity stations contribute significantly fewer trips than average stations, totaling around 750,000 trips
   
   * This could indicate:

     * Poor placement (like less foot traffic)
     * Oversupply of docks
     * Underutilized transit hubs
     * Missed Opportunity:

---

# B)

---

---
Task 1
---

| Method                     | Formula                         | Notes                               |
| -------------------------- | ------------------------------- | ----------------------------------- |
| **Sturges‚Äô Rule**          | `bins = ceil(log2(n) + 1)`      | Good for small to medium-sized data |
| **Freedman‚ÄìDiaconis Rule** | `bin_width = 2 * IQR / n^(1/3)` | Good for skewed data or outliers    |
| **Square Root Rule**       | `bins = sqrt(n)`                | Simple and often a good baseline    |
"""

import numpy as np
import plotly.graph_objects as go

durations = trips_df['trip_duration_minutes']

# Freedman‚ÄìDiaconis rule
q25, q75 = np.percentile(durations, [25, 75])
iqr = q75 - q25
n = len(durations)
bin_width = 2 * iqr / (n ** (1/3))
bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))
bin_count = min(bin_count, 200)

print(f"Suggested bin count (after capping): {bin_count}")

# Pre-aggregate counts
hist, bin_edges = np.histogram(durations, bins=bin_count)

# Build bar chart
fig = go.Figure(go.Bar(
    x=bin_edges[:-1],
    y=hist,
    marker_color='blue',
    opacity=0.75
))

fig.update_layout(
    title="Distribution of Trip Duration (in Minutes)",
    xaxis_title="Trip Duration (minutes)",
    yaxis_title="Frequency",
    bargap=0.05,
    template='simple_white'
)

# Show with staticPlot
fig.show(config={'staticPlot': True})

"""How many trips took more then a day ?"""

cutoff = 1440  #24*60

# Use the sampled dataframe to avoid memory issues
durations = sampled_df['trip_duration_minutes']

# Freedman‚ÄìDiaconis rule for bin width
q25, q75 = np.percentile(durations, [25, 75])
iqr = q75 - q25
n = len(durations)
bin_width = 2 * iqr / (n ** (1/3))
bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))

print(f"Suggested bin count: {bin_count}")


# Count how many trips exceed the cutoff
sampled_exceed = (sampled_df['trip_duration_minutes'] > cutoff).sum()
full_exceed = (trips_df['trip_duration_minutes'] > cutoff).sum()

print(f"Trips in sampled_df exceeding {cutoff} minutes: {sampled_exceed}")
print(f"Trips in trips_df exceeding {cutoff} minutes: {full_exceed}")

"""insights :
1. The massive bar near 0-20 minutes clearly shows that most bike trips are very short. This is typical for bike-sharing systems, often used for short commutes or quick errands.
2. while the bar chart does shows some trips between 50-100 minutes, the numbers are smaller, so yes they exist , and yes they are part of the bike sharing dynamic but they are not the big fish
3. The presence of bars, even if very short, extending all the way to 1440 minutes shoes that some trips in the data did take more then a day , the number of trips is 361

---
Task2
---
"""

durations = sampled_df['trip_duration_minutes']
types = sampled_df['rideable_type']

fig = go.Figure()

for bike_type in sampled_df['rideable_type'].unique():
    subset = sampled_df.loc[sampled_df['rideable_type'] == bike_type, 'trip_duration_minutes']
    if len(subset) > 5000:
        subset = subset.sample(5000, random_state=42)
    fig.add_trace(go.Box(
        y=subset,
        name=bike_type,
        boxpoints='outliers',
        marker_color='green',
        line_color='black',
        opacity=0.8
    ))

fig.update_layout(
    title="Box Plot of Trip Duration by Rideable Type",
    yaxis_title="Trip Duration (minutes)",
    xaxis_title="Rideable Type",
    template='simple_white'
)

fig.show(config={'staticPlot': True})

"""insights :
1. both types show a very compact box near 20 minutes, indicating that the vast majority of trips for both bike types are quite short.
2. The median line is very close to the bottom of the box, confirming heavy right-skewness, this means almost all of the middle 50% of data is concentrated very close to the lower end, and the remaining data (up to Q3) is more spread out.
3. The green dots above the whiskers clearly represent the longer  trips with some of them above the 1440 line
4. Electric bikes seem to have a slightly tighter distribution,This suggests that while both have short typical trips, classic bikes might have a slightly wider range of trips durations
5. Both bike types exhibit very long duration "outliers," with classic bikes potentially having more extreme longer-duration outliers , It suggests that while electric bikes facilitate shorter, perhaps faster trips, classic bikes are used for the most extended journeys.
This could be due to factors like cost , battery limits, or simply user preference .

---
Task3
---
"""

trips_df['member_casual'].value_counts()

durations = sampled_df['trip_duration_minutes']
types_to_group_by = sampled_df['member_casual']

fig = go.Figure()

for member_type in sampled_df['member_casual'].unique():
    subset = sampled_df.loc[sampled_df['member_casual'] == member_type, 'trip_duration_minutes']
    if len(subset) > 5000:
        subset = subset.sample(5000, random_state=42)
    fig.add_trace(go.Box(
        y=subset,
        name=member_type,
        boxpoints='outliers',
        marker_color='green',
        line_color='black',
        opacity=0.8
    ))

fig.update_layout(
    title="Box Plot of Trip Duration by Member Type",
    yaxis_title="Trip Duration (minutes)",
    xaxis_title="Member Type",
    template='simple_white'
)

fig.show(config={'staticPlot': True})

"""insights :
1. As with the previous duration plots, both "casual" and "member" trips show an incredibly strong right-skewness. The box for both categories is extremely compact and squashed at the very bottom of the plot (close to 0 minutes), and the median line is practically on top of the first quartile (Q1). This reinforces that the vast majority of trips for both casual and member users are very short.
2. While both are low, the median (and Q1/Q3) for 'casual' riders appears slightly higher (or at least, the box is marginally less squashed) than for 'member' riders. This suggests that a "typical" trip for a casual user is slightly longer than for a member, even if both are still relatively short.
3. Both categories clearly have a significant number of "outlier" trips (the green dots) that extend far beyond the main box and whiskers, indicating that very long trips do occur for both types of users.
4. The box (IQR) for 'casual' riders seems marginally wider than for 'member' riders,The whiskers for 'casual' riders also appear slightly longer, This implies casual riders exhibit a greater range of typical trip durations compared to members.
5. 'Casual' riders have a significantly higher density of very long outlier trips, 'Member' riders also have long outlier trips, but they are fewer in number and generally do not reach the same extreme durations as those of casual riders.

---
Task4
---
"""

# Counting Trips Longer Than One Day
one_day_minutes = 1440
# Filter trips longer than 1 day
long_trips_df = trips_df[trips_df['trip_duration_minutes'] > one_day_minutes]
long_sampled_df = sampled_df[sampled_df['trip_duration_minutes'] > one_day_minutes]
# Show how many there are
print(f"Total number of trips longer than 1 day in full data: {len(long_trips_df)}")
print(f"Total number of trips longer than 1 day in sampled data: {len(long_sampled_df)}")
# Combine start and end station counts for long trips
start_counts = long_trips_df['start_station_id'].value_counts()
end_counts = long_trips_df['end_station_id'].value_counts()

# Combine them into a single Series
total_counts = start_counts.add(end_counts, fill_value=0).astype(int)

# Get station info: name and location
stations = sampled_df[['start_station_id', 'start_station_name', 'start_lat', 'start_lng']].drop_duplicates()
stations = stations.rename(columns={
    'start_station_id': 'station_id',
    'start_station_name': 'station_name',
    'start_lat': 'lat',
    'start_lng': 'lng'
})

# Merge with counts
stations['long_trip_count'] = stations['station_id'].map(total_counts).fillna(0).astype(int)

# Filter stations with at least 1 long trip
stations = stations[stations['long_trip_count'] > 0]

# # Center the map on Washington DC
# m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles='cartodbpositron')

# marker_cluster = MarkerCluster().add_to(m)

# # Add stations to the map
# for _, row in stations.iterrows():
#     folium.CircleMarker(
#         location=[row['lat'], row['lng']],
#         radius=3 + row['long_trip_count']**0.5,
#         color='darkred',
#         fill=True,
#         fill_color='crimson',
#         fill_opacity=0.7,
#         popup=f"{row['station_name']}<br>Trips > 1 day: {row['long_trip_count']}"
#     ).add_to(marker_cluster)
# m

stations_filtered = stations[stations['long_trip_count'] > 1].copy()
stations_filtered.info()

stations_filtered = stations[stations['long_trip_count'] > 2].copy()
stations_filtered['plot_size'] = (stations_filtered['long_trip_count']**0.5 + 3).round(1)

fig = go.Figure(go.Scattermapbox(
    lat=stations_filtered['lat'],
    lon=stations_filtered['lng'],
    mode='markers',
    marker=dict(
        size=stations_filtered['plot_size'],
        color='crimson',
        opacity=0.6
    ),
    hovertext=stations_filtered['station_name'],
    hoverinfo='text'
))

fig.update_layout(
    title="Washington DC - Stations with Long Trips",
    mapbox=dict(
        style="carto-positron",
        center={"lat": 38.9072, "lon": -77.0369},
        zoom=11
    ),
    showlegend=False,
    margin=dict(l=0, r=0, t=40, b=0)
)

fig.show(config={'staticPlot': True})

"""insights :
1.  primary observation is that stations associated with long-duration trips are particularly concentrated around the central part WDC map ,This suggests that while very long trips are rare overall, they are not uniformly distributed but rather originate from or end in specific zones.
2. The 2496 is the count of unique station IDs that appear as either a start_station_id OR an end_station_id within those 361 trips.
This is a significant number of stations involved, considering that the total number of such trips was only 361. This means these long trips are spread across a wide variety of stations, rather than being concentrated at just a few specific locations.
3. This highlights areas where the system might need to adapt to different user behaviors like offering specific "long-term rental" options or different pricing for these stations.

---

# C)

---

---
Task1
---
"""

len(sampled_df['trip_cost'].unique())

trips_df.columns
len(sampled_df[sampled_df['start_month'] != sampled_df['end_month']])
sampled_df['start_time'] = pd.to_datetime(sampled_df['start_time'], format='mixed', errors='coerce')

trip_costs = sampled_df['trip_cost'].dropna()

q75, q25 = np.percentile(trip_costs, [75 ,25])
iqr = q75 - q25
n = len(trip_costs)

if iqr == 0:
    nbins = 50
else:
    bin_width = 2 * iqr / (n ** (1/3))
    data_range = trip_costs.max() - trip_costs.min()
    nbins = max(1, int(np.ceil(data_range / bin_width)))
    nbins = min(nbins, 200)

print(nbins)

hist, bin_edges = np.histogram(trip_costs, bins=nbins)
fig = go.Figure(go.Bar(
    x=bin_edges[:-1],
    y=hist,
    marker_color='blue',
    opacity=0.75
))
fig.update_layout(
    title='Distribution of Trip Costs (Freedman‚ÄìDiaconis Rule)',
    xaxis_title='Trip Cost',
    yaxis_title='Frequency',
    template='simple_white'
)
fig.show(config={'staticPlot': True})

subset = trip_costs
if len(subset) > 5000:
    subset = subset.sample(5000, random_state=42)
fig = go.Figure(go.Box(
    y=subset,
    boxpoints='outliers',
    marker_color='blue',
    line_color='black',
    opacity=0.8
))
fig.update_layout(
    title='Boxplot of Trip Costs',
    yaxis_title='Trip Cost',
    template='simple_white'
)
fig.show(config={'staticPlot': True})

"""‚Ä¢ Concentration around low costs: The charts show that the data is densely distributed between $0 and $9, with a noticeable peak between $3.5 and $4. The median line is very close to zero, and the box itself is very narrow, indicating that at least half of the trips had a very low cost.

‚Ä¢ Presence of higher-cost trips, though rare: Although most trips are concentrated in the low-cost range, there are higher cost values between $70 and $80, though they are rare. This suggests that some individuals take long-distance trips, but their numbers are few.

‚Ä¢ Guess about high-cost trips: It is likely that the high-cost trips are made by casuals or one time users who may be using the bikes just once and do not return after experiencing the price. This suggests that the pricing for non-subscribers might discourage repeat use, or that the user does not return the bike.

---
task2
---
"""

subset = sampled_df
if len(subset) > 10000:
    subset = subset.sample(10000, random_state=42)
fig = px.scatter(subset, x='trip_duration_minutes', y='trip_cost', trendline='ols', title='the realtion between duration and cost')
fig.show(config={'staticPlot': True})

"""‚Ä¢ Strong Linear Relationship: As trip duration increases, the cost generally increases proportionally. The regression line (blue line) fits the data points very well, indicating a consistent time-based pricing model,
This is expected, as most rental services charge based on time.

‚Ä¢ For very medium durations ( 100‚Äì200 minutes), there is a wider spread in cost, with some points showing relatively higher costs for shorter durations compared to the main trend. This could indicate a minimum fee, initial unlocking charges, or cost differences based on membership type for short trips.

‚Ä¢ There is also noticeable "spread" in data points at the lower end of duration, where some short trips have a cost of zero, while others have a small fee. This may reflect differences in member pricing or promotional offers or rideable type.

---
Task3
---
"""

subset = sampled_df
if len(subset) > 10000:
    subset = subset.sample(10000, random_state=42)
fig = px.scatter(subset, x='temp', y='trip_cost', color='member_casual', title='cost vs temperatur')
fig.show(config={'staticPlot': True})

"""‚Ä¢ There does not appear to be a strong and clear linear relationship between temperature and trip cost. Costs are spread widely across normal temperatures (4 - 30) , with an increase when the the weather is mild (17- 28)
*  Trip cost is primarily determined by factors like membership status, trip duration, and likely bike type.

‚Ä¢ The vast majority of member trips (red points) are concentrated at very low costs, mostly under $10, regardless of temperature. There are very few high-cost trips among members.

‚Ä¢ Casual users, on the other hand, show a wider range, frequently reaching higher values ($40‚Äì$75). This suggests that casual users are billed based on usage, with longer or more complex trips leading to significantly higher costs.

‚Ä¢ High-cost trips (especially by casual users) seem to occur more frequently in mild to warm temperatures (10‚Äì30¬∞C)‚Äîa time when people tend to use bikes for longer periods.

‚Ä¢ There are fewer data points at extreme temperatures (very low or very high), which is expected as bike usage drops during uncomfortable weather.

---
Task4
---
"""

daily_rev = sampled_df.groupby(sampled_df['start_time'].dt.date)['trip_cost'].sum().reset_index(name='revenue')
fig = px.line(daily_rev, x='start_time', y='revenue', title='daily incomes')
fig.show(config={'staticPlot': True})

sampled_df['week'] = sampled_df['start_time'].dt.isocalendar().week
weekly_rev = sampled_df.groupby('week')['trip_cost'].sum().reset_index(name='revenue')
fig = px.line(weekly_rev, x='week', y='revenue', title='weekly incomes')
fig.show(config={'staticPlot': True})

"""‚Ä¢ Daily revenue starts relatively low in January, shows a general upward trend, and peaks around September‚ÄìOctober, then declines sharply toward the end of the year. This pattern strongly suggests a seasonal effect, where revenues are higher during warmer months (spring, summer, early fall) when bike usage typically increases, and drop during colder months. This requires further seasonal analysis.

‚Ä¢ There is a clear and noticeable drop in weekly income starting around weeks 45‚Äì50. This sharp decline toward the end of the year (late November/December) aligns with reduced bike usage during cold weather, holiday periods, or possibly reduced operating hours/demand in winter.

---
Task5
---
"""

monthly_rev = sampled_df.groupby('start_month')['trip_cost'].mean().reset_index(name='avg_revenue')
fig = px.line(monthly_rev, x='start_month', y='avg_revenue', title='average month income')
fig.show(config={'staticPlot': True})

"""‚Ä¢  monthly averages: The average revenue per transaction shows variations throughout the year, rather than a clear upward or downward trend.

‚Ä¢ Peaks and troughs: There are noticeable peaks in average revenue during February and April, indicating that transactions in these months generated slightly higher revenue on average. Conversely, there are dips in March up to August, with june showing the lowest average revenue.

‚Ä¢ Relatively stable range:  avg_revenue values remain within a very narrow range, from approximately $3.74 to $3.83. This suggests that while there are slight monthly differences in average revenue per transaction, the core pricing structure or the value derived from each trip remains largely consistent throughout the year.

___
**conclusion:**<br>
Weather patterns show clear seasonality (hotter summers, colder winters) affecting overall bike usage, while trip costs are primarily driven by duration and user type (members pay less, casuals pay more).
Revenue is strongly seasonal, peaking in warmer months and sharply declining in winter, yet the average cost per individual trip remains relatively stable year-round.
Trip costs are heavily skewed towards low values, with a significant number of expensive outlier trips primarily incurred by casual riders.

---

# D)

---

---
Task1
---
"""

# Add columns directly BEFORE the join so they stay in the result
start_gdf = gpd.GeoDataFrame(sampled_df.copy(), geometry=gpd.points_from_xy(sampled_df['start_lng'], sampled_df['start_lat']), crs='EPSG:4326')
end_gdf = gpd.GeoDataFrame(sampled_df.copy(), geometry=gpd.points_from_xy(sampled_df['end_lng'], sampled_df['end_lat']), crs='EPSG:4326')

# Load residential zones and CBD, ensure correct CRS
res_zones = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson').to_crs('EPSG:4326')
CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson').to_crs('EPSG:4326')


# Now join
start_in_res = gpd.sjoin(start_gdf, res_zones, predicate='within', how='inner')
end_in_res = gpd.sjoin(end_gdf, res_zones, predicate='within', how='inner')

# Build res_points cleanly
res_start_points = pd.DataFrame({
    'lat': start_in_res['start_lat'],
    'lon': start_in_res['start_lng']
})

res_end_points = pd.DataFrame({
    'lat': end_in_res['end_lat'],
    'lon': end_in_res['end_lng']
})

res_points = pd.concat([res_start_points, res_end_points], ignore_index=True)

# Simplify geometries for plotting
# Ensure res_zones and CBD are projected before simplifying if needed, though simplify works on geographic too
res_zones_simple = res_zones.simplify(tolerance=0.0005, preserve_topology=True)
CBD_simple = CBD.simplify(tolerance=0.0005, preserve_topology=True)

if len(res_points) > 5000:
    res_points_plot = res_points.sample(5000, random_state=42)
else:
    res_points_plot = res_points

# Build map
fig = go.Figure()

fig.add_trace(go.Densitymapbox(
    lat=res_points_plot['lat'],
    lon=res_points_plot['lon'],
    radius=10,
    colorscale="Viridis",
    hoverinfo="skip"
))

fig.update_layout(
    mapbox_layers=[
        {
            "below": 'traces',
            "sourcetype": "geojson",
            "source": json.loads(res_zones_simple.to_json()),
            "type": "line",
            # "line": {"color": "blue", "width": 1} # Fixed color property location
        },
        {
            "below": 'traces',
            "sourcetype": "geojson",
            "source": json.loads(CBD_simple.to_json()),
            "type": "line",
            # "line": {"color": "green", "width": 2} # Fixed color property location
        }
    ],
    title_text='Geographic Heatmap of Trips to Residential Zones with Borders',
    mapbox_style="carto-positron",
    mapbox_zoom=10,
    mapbox_center={"lat": res_points['lat'].mean(), "lon": res_points['lon'].mean()},
    margin={"r":0,"t":40,"l":0,"b":0}
)

fig.show(config={'staticPlot': True})

"""insights :

1. The highest concentration of trips within residential zones appears to be in the central and northern parts of Washington D.C., This suggests these are the most active residential areas for bike-sharing usage.
2. While the overall pattern is dense,These could indicate areas with fewer stations, different demographic profiles, or less need for bike-sharing, which could be further explored.

---
Task2
---
"""

geohash_counts = sampled_df['geohash_sector'].value_counts().reset_index()
geohash_counts.columns = ['geohash_sector', 'trip_count']
geohash_counts = geohash_counts.sort_values(by='trip_count', ascending=False)

fig = px.bar(
    geohash_counts,
    x='geohash_sector',
    y='trip_count',
    title='Distribution of Trips by Geographic Sector (geohash_sector)',
    labels={'geohash_sector': 'Geographic Sector', 'trip_count': 'Number of Trips'}
)
fig.show(config={'staticPlot': True})

adef geohash_to_polygon(geohash_str):
    """
    Converts a geohash string to a shapely Polygon representing its bounding box.
    """
    lat, lon, lat_err, lon_err = gh.decode_exactly(geohash_str)
    min_lat = lat - lat_err
    max_lat = lat + lat_err
    min_lon = lon - lon_err
    max_lon = lon + lon_err
    coords = [
        (min_lon, min_lat),
        (max_lon, min_lat),
        (max_lon, max_lat),
        (min_lon, max_lat),
        (min_lon, min_lat)  # Close the polygon
    ]
    return Polygon(coords)

top_5_geohashes = geohash_counts.head(5)['geohash_sector'].tolist()
geohash_polygons = [geohash_to_polygon(g) for g in top_5_geohashes]

# Convert to GeoDataFrame for easier plotting
gdf = gpd.GeoDataFrame(geometry=geohash_polygons, crs='EPSG:4326')

fig = go.Figure()

# Add each polygon as a filled scattermapbox trace
for poly in gdf.geometry:
    lons, lats = poly.exterior.coords.xy
    fig.add_trace(go.Scattermapbox(
        lon=list(lons),
        lat=list(lats),

        mode='lines',
        fill='toself',
        fillcolor='rgba(255, 0, 0, 0.4)',
        line=dict(width=1, color='red'),
        hoverinfo='skip',
        showlegend=False
    ))

fig.update_layout(
    mapbox_style='carto-positron',
    mapbox_zoom=12,
    mapbox_center={"lat": gdf.geometry.centroid.y.mean(), "lon": gdf.geometry.centroid.x.mean()},
    margin={"r":0,"t":0,"l":0,"b":0}
)
# fig.write_image("Top 5 Geohash Polygons.png")
# files.download("Top 5 Geohash Polygons.png")

fig.show(config={'staticPlot': True})

"""insights :
1.  The first few bars on the left are significantly taller than the rest, with the tallest bar approaching 250k trips, while many others have less than 10k. This indicates that bike-sharing activity is heavily concentrated in a few high-demand areas.
2. the plot allows for quick identification of the highest-demand geographic sectors. These are likely to correspond to areas with high population density, major transit hubs, popular attractions, or dense commercial/office districts
3. we ploted these top sectors and we compared it the heatmap in task1 , and we are now sure that they are in hotspots for trips in residential zone

---
Task3
---
"""

import numpy as np

def plot_binned_histogram(df, col, bins, title, xlabel):
    # Drop NaNs
    data = df[col].dropna()
    counts, bin_edges = np.histogram(data, bins=bins)

    # Create labels for bins (e.g., bin centers)
    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])

    # Build DataFrame for plot
    hist_df = pd.DataFrame({
        xlabel: bin_centers,
        'count': counts
    })

    fig = px.bar(
        hist_df,
        x=xlabel,
        y='count',
        title=title,
        labels={xlabel: xlabel, 'count': 'Count'}
    )
    fig.show(config={'staticPlot': True})

# Use your Freedman-Diaconis bins
bins_cbd = freedman_diaconis_bins(sampled_df['distance_to_cbd_m'])
bins_metro = freedman_diaconis_bins(sampled_df['start_nearest_metro_distance'])
bins_shuttle = freedman_diaconis_bins(sampled_df['start_nearest_shuttle_distance'])

# Plot histograms as bar charts from pre-binned data
sampled_df['distance_to_cbd_km'] = sampled_df['distance_to_cbd_m'] / 1000

plot_binned_histogram(
    sampled_df,
    'distance_to_cbd_km',
    bins=bins_cbd,
    title='Distribution of Distance to CBD (km)',
    xlabel='Distance to CBD (km)'
)

plot_binned_histogram(
    sampled_df,
    'start_nearest_metro_distance',
    bins=bins_metro,
    title='Distribution of Distance to Nearest Metro Station',
    xlabel='Distance to Nearest Metro Station'
)

plot_binned_histogram(
    sampled_df,
    'start_nearest_shuttle_distance',
    bins=bins_shuttle,
    title='Distribution of Distance to Nearest Shuttle Station',
    xlabel='Distance to Nearest Shuttle Station'
)

"""insights :
1. The overwhelming majority of trips (as indicated by the highest bars) occur within approximately 0 to 4 kilometers of the Central Business District. The peak frequency appears to be around 2.2 kilometers, This reinforces the CBD's role as a major hub for bike-sharing activity.
2. this strongly supports the idea that the bike-sharing system primarily serves "last-mile" transportation, short-distance commutes, or intra-CBD movement.


---


1. The histogram displays a highly skewed distribution, with an extremely high frequency of trips originating or ending very close to Metro stations. The counts drop off sharply as the distance increases.
2. This insight highlights the importance of placing bike-sharing stations in very close proximity to Metro entrances to maximize their integration with the public transit network and serve commuter needs effectively.


---
1.  Similar to the Metro station plot, this histogram shows a highly skewed distribution, with the highest frequency of trips occurring very close to shuttle stations.
2. This suggests a consistent user behavior of using bike-sharing to bridge short gaps to fixed public transport points.

---
Task4
---
"""

# Categorize trips
def classify_trip(row):
    if row['start_in_cbd'] == 1 and row['end_in_cbd'] == 1:
        return 'Fully in CBD'
    else:
        return 'Outside CBD'
sampled_df['cbd_trip_type'] = sampled_df.apply(classify_trip, axis=1)
trip_cbd_counts = sampled_df['cbd_trip_type'].value_counts().reset_index()
trip_cbd_counts.columns = ['Trip Type', 'Count']

fig = px.bar(
    trip_cbd_counts,
    x='Trip Type',
    y='Count',
    title='Trips Fully in CBD vs Outside',
    text='Count',
    labels={'Count': 'Number of Trips'}
)
fig.update_traces(textposition='outside')
fig.update_layout(yaxis_title='Number of Trips', xaxis_title='Trip Category')
fig.show(config={'staticPlot': True})

trips_df['cbd_trip_type'] = trips_df.apply(classify_trip, axis=1)
full_trip_cbd_counts = trips_df['cbd_trip_type'].value_counts().reset_index()
full_trip_cbd_counts.columns = ['Trip Type', 'Count']
full_trip_cbd_counts['Percentage'] = (full_trip_cbd_counts['Count'] / full_trip_cbd_counts['Count'].sum()) * 100
full_trip_cbd_counts

"""insights :
1. The vast majority of trips  fall into the "Outside CBD" category, This indicates that the bike-sharing system primarily serves a broader geographical area beyond just the core CBD, or facilitates connections into/out of it rather than exclusively within it.
3.  A significantly smaller number of trips (6.25%) were classified as "Fully in CBD". This suggests that while there is some intra-CBD movement, it represents a much smaller proportion
4. This distribution implies that the bike-sharing service's primary function might not be solely for quick hops within the CBD itself, but rather for facilitating commutes, errands, or leisure travel that connects to or traverses the CBD from other parts of the city. This aligns with the "first/last mile" insights from the distance histograms, where trips are often connecting to major transit points or residential areas that might lie outside the strict CBD boundaries.
5.  The fact that over 93% of all the trips involve areas outside the CBD strongly reinforces the idea that bike-sharing is primarily used as a "first/last mile" solution, connecting users to or from transit hubs (like Metro and Shuttle stations) or residential areas that often lie outside the strict CBD boundaries. These trips are unlikely to be exclusively within the CBD.

---
Task5
---
"""

cbd_passed_df = sampled_df[
    (sampled_df['start_in_cbd'] == 1) | (sampled_df['end_in_cbd'] == 1)
]
grouped = cbd_passed_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')

fig = px.bar(
    grouped,
    x='rideable_type',
    y='trip_count',
    color='member_casual',
    barmode='group',
    title='Trips That Passed Through CBD by Rideable Type and Membership',
    labels={'trip_count': 'Number of Trips', 'rideable_type': 'Bike Type'}
)
fig.update_layout(
    xaxis_title='Rideable Type',
    yaxis_title='Number of Trips'
)
fig.show(config={'staticPlot': True})

cbd_passed_df_trips_df=trips_df[
    (trips_df['start_in_cbd'] == 1) | (trips_df['end_in_cbd'] == 1)
]

# Group by rideable_type and member_casual
grouped = cbd_passed_df_trips_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')

print(f"Length of cbd_passed_df_trips_df: {len(cbd_passed_df_trips_df)}")
print(f"Length of trips_df: {len(trips_df)}")

percentage = (len(cbd_passed_df_trips_df) / len(trips_df)) * 100
print(f"Percentage of cbd_passed_df_trips_df compared to trips_df: {percentage:.2f}%")

"""insights :
1. the extra exploration shows that 29.85% of all bike-sharing trips pass through the CBD, This is a substantial portion, indicating the CBD's critical role as an origin, destination, or transit point for a large segment of bike-sharing users.
2. For both classic_bike and electric_bike categories, members (orange/red bars) consistently account for a significantly higher number of trips that pass through the CBD compared to casual riders (blue bars). This suggests that regular commuters or frequent users (members) are more likely to utilize bike-sharing for travel involving the city's central business district.
3. Both casual and member riders use classic_bike more frequently for CBD-involved trips than electric_bike
4. Since members are the primary users for CBD-involved trips, strategies to retain and grow membership, and ensure sufficient bike availability in and around the CBD, are crucial.
5. The demand for both classic and electric bikes for CBD-involved trips means that fleet management should consider a balanced distribution to meet the preferences of both member and casual riders.

---
Task6
---
"""

# Create a contingency table
# (Counts of each combination)
contingency_table = pd.crosstab(trips_df['close_to_cbd'], trips_df['member_casual'])
contingency_table

# Run chi-square test
chi2, p, dof, expected = chi2_contingency(contingency_table)
print("Chi2 Statistic:", chi2)
print("Degrees of Freedom:", dof)
print("P-value:", p)
# interpretion based on the p-value:
if p < 0.05:
    print(" There is a significant correlation between distance to CBD segments and membership type.")
else:
    print(" No significant correlation found between distance to CBD segments and membership type.")

"""| Œ± Value  | Interpretation                                                                |
| -------- | ----------------------------------------------------------------------------- |
| **0.05** | Most common ‚Äî means you're willing to accept a 5% chance of a false positive. |
| 0.01     | Stricter ‚Äî used in more critical fields (medicine, etc.).                     |
| 0.10     | Looser ‚Äî sometimes used in exploratory research.                              |

insights:
1. The p-value of 0.0, which is much less than the conventional significance level of 0.05 leads us to believe there is a statistically significant relationship between whether a trip is close to the CBD and the user's membership type.
2. While the Chi-square test indicates strong statistical significance (due to the very large sample size), the proportional difference between casual and member trips being close_to_cbd is relatively small,This suggests thatBoth member and casual riders have roughly half their trips originating or ending close to the CBD.

---
# E)
---

---
Task 1
---
"""

sampled_df['rideable_type'].unique()

daily_weather_avg = sampled_df.groupby('date')[['temp', 'humidity', 'windspeed']].mean().reset_index()
daily_weather_avg = daily_weather_avg.rename(columns={
    'temp': 'Average Temperature',
    'humidity': 'Average Humidity',
    'windspeed': 'Average Wind Speed'
})

fig = px.line(
    daily_weather_avg,
    x='date',
    y=['Average Temperature', 'Average Humidity', 'Average Wind Speed'],
    title='Average Daily Weather Conditions (Temperature, Humidity, Wind Speed)',
    labels={
        'date': 'Date',
        'value': 'Average Value',
        'variable': 'Metric'
    }
)
fig.update_layout(hovermode="x unified")
fig.show(config={'staticPlot': True})

"""* Average Temperature (blue line): Shows a clear seasonal pattern, with lower temperatures during the winter months and higher temperatures in the summer. There are noticeable daily fluctuations, but the overall trend aligns with typical annual temperature cycles.

* Average Humidity (red line): Shows significant daily fluctuations, often ranging between 40% and 90%. While there isn‚Äôt a strong seasonal trend like with temperature, humidity remains relatively high throughout the year with occasional drops.

* Average Wind Speed (green line): Shows less dramatic seasonal variation compared to temperature. Wind speeds generally range between 0 and 40 units (likely km/h or mph), with several spikes indicating stormy days interspersed with calmer periods. There are no prolonged periods of exceptionally high or low wind

---
Task2
---
"""

daily_weather_cond = sampled_df.groupby('date')['weather_segment'].first().reset_index()
daily_rev = sampled_df.groupby('date')['trip_cost'].sum().reset_index(name='revenue')
merged_df = pd.merge(daily_rev, daily_weather_cond, on='date', how='left')

fig = px.box(
    merged_df,
    x='weather_segment',
    y='revenue',
    title='Daily Revenue by Weather Condition',
    labels={
        'weather_segment': 'Weather Condition',
        'revenue': 'Daily Revenue ($)'
    },
    category_orders={"weather_segment": ["Sunny", "Cloudy", "Rainy"]}
)
fig.update_traces(boxpoints='all', jitter=0.3)
fig.update_layout(yaxis_title='Daily Revenue ($)', xaxis_title='Weather Condition')
fig.show(config={'staticPlot': True})

"""- The average revenue is around $65k, the lowest among all weather conditions.
1.  **Rainy Days:**

- The median daily revenue on rainy days is the lowest among the three conditions, appearing to be around $58k-60k

- The box is narrower compared to sunny and cloudy days, indicating that daily revenues are generally less varied and consistently lower when it rains

- Despite the general downward trend, there are notable cases of surprisingly high-revenue days (some exceeding $80k), suggesting that rain does not completely eliminate the chance of high-revenue days.
2. **Cloudy Days:**
- Cloudy days show a slightly higher median revenue than rainy days, estimated around $68-70.

- The wider box indicates greater variation in daily profits, ranging from low to very high. Many days achieve high revenue figures, some are quite low, while others are significantly higher.
3. **Sunny Days:**
- Sunny days generally lead in daily revenue, with the highest median, estimated to be around $70-72.

- Similar to cloudy days, sunny days show a wide spread in daily revenue, indicating that while the potential for high earnings is greater, there are still days with lower revenues.

- The upper quartile and whiskers extend higher on sunny days, highlighting them as the most profitable days overall.

- People tend to use bikes more on sunny days, possibly due to their desire to enjoy the sunshine on the rare days when the weather is nice and bright.

4. **Impact of Weather on Rider Behavior and Revenue:**
- It's evident that weather conditions play a significant role in influencing daily revenue. Sunny days generally yield the highest revenues, followed closely by cloudy days, while rainy days consistently see the lowest revenues.

- This pattern strongly suggests that user behavior is impacted by weather, with a clear preference for riding in clear and bright conditions.

5. **Outliers:**
- The presence of outliers in all categories indicates that there are always some days that don‚Äôt follow the general weather pattern‚Äîwhether ( holidays, special events, promotions) ) exceptionally good or bad‚Äîsuggesting that other factors also influence usage. These outliers are more frequent on cloudy days.

---
Task3
---
"""

merg = pd.merge(daily_weather_avg, daily_rev, on='date', how='left')

cols_to_normalize = ['revenue', 'Average Temperature', 'Average Humidity']
for col in cols_to_normalize:
    min_val = merg[col].min()
    max_val = merg[col].max()
    if (max_val - min_val) != 0:
        merg[f'normalized_{col}'] = (merg[col] - min_val) / (max_val - min_val)
    else:
        merg[f'normalized_{col}'] = 0.0

fig1 = px.scatter(
    merg,
    x='normalized_Average Temperature',
    y='normalized_revenue',
    title="Relationship between Daily Income and Temperature",
    trendline='ols',
    labels={
        'normalized_Average Temperature': 'Normalized Average Temperature',
        'normalized_revenue': 'Normalized Daily Revenue'
    }
)
fig1.show(config={'staticPlot': True})

fig2 = px.scatter(
    merg,
    x='normalized_Average Humidity',
    y='normalized_revenue',
    title="Relationship between Daily Income and Humidity",
    trendline='ols',
    labels={
        'normalized_Average Humidity': 'Normalized Average Humidity',
        'normalized_revenue': 'Normalized Daily Revenue'
    }
)
fig2.show(config={'staticPlot': True})

"""1. **Relationship Between Daily Revenue and Temperature:**
- A clear positive linear relationship is observed: as temperature increases, daily income generally rises.

- The upward-sloping regression line and the strong alignment of data points along this trend suggest that warmer temperatures are associated with higher daily income for the bike-sharing service.

- This aligns with previous seasonal observations where warmer months bring more outdoor activity.

- Despite the positive trend, there's still a spread in daily revenue values at nearly all temperature points, meaning temperature is an important but not exclusive factor affecting revenue.
2. **Relationship Between Revenue and Humidity:**
- This chart shows a very weak negative linear relationship, almost negligible‚Äîthe regression line is nearly flat or only slightly sloping downward.

- The wide scatter of data points around the flat regression line indicates that daily humidity levels do not have a strong or consistent impact on daily income.

- Revenue can be high or low regardless of whether humidity is high or low. Compared to temperature, humidity seems to be a much less important factor in determining daily income.

---
 Task4
---
"""

contingency_table = pd.crosstab(sampled_df['weather_segment'], sampled_df['rideable_type'])
print("Contingency Table (Observed Frequencies):")
print(contingency_table)
print("\n" + "="*50 + "\n")

chi2, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)
print(f"Chi2 Statistic: {chi2:.4f}")
print(f"P-value: {p_value:.4f}")
print(f"Degrees of Freedom: {dof}")
print("\nExpected Frequencies Table:")
print(pd.DataFrame(expected_frequencies, index=contingency_table.index, columns=contingency_table.columns))
print("\n" + "="*50 + "\n")

alpha = 0.05
print("Interpretation of Results:")
if p_value < alpha:
    print(f"Since the P-value ({p_value:.4f}) is less than the significance level (alpha = {alpha}),")
    print("we reject the null hypothesis (H0).")
    print("Conclusion: There is strong statistical evidence of a significant relationship between weather condition and ride type.")
else:
    print(f"Since the P-value ({p_value:.4f}) is greater than or equal to the significance level (alpha = {alpha}),")
    print("we fail to reject the null hypothesis (H0).")
    print("Conclusion: There is no sufficient statistical evidence to claim a significant relationship between weather condition and ride type.")

df_plot = contingency_table.reset_index().melt(id_vars='weather_segment', var_name='rideable_type', value_name='Count')

# fig = px.bar(
#     df_plot,
#     x='weather_segment',
#     y='Count',
#     color='rideable_type',
#     barmode='group',
#     title='Ride Type Distribution by Weather Condition',
#     labels={
#         'weather_segment': 'Weather Condition',
#         'Count': 'Number of Rides',
#         'rideable_type': 'Ride Type'
#     },
#     category_orders={"weather_segment": ["Sunny", "Cloudy", "Rainy"]}
# )
# fig.update_layout(xaxis_title="Weather Condition", yaxis_title="Number of Rides")
# fig.show(config={'staticPlot': True})

"""This analysis reveals a statistically significant relationship between weather conditions and the type of bike chosen by users.

While electric bikes are generally preferred in all weather, this preference becomes more pronounced on cloudy days.

This reinforces the idea that users favor electric bikes due to advantages like reduced physical effort, especially under less ideal weather conditions.

# Catching patterns

## A ) Temporal Analysis

### Task 1: Data Verification and Temporal Repairs
#### What to do:

- Verify that the temporal data you obtained is in the correct chronological order and without any missing intervals
- Perform necessary repairs for any temporal issues found
"""

trips_df['started_at'] = pd.to_datetime(trips_df['started_at'], format='mixed', errors='coerce')
trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'], format='mixed', errors='coerce')

print("Missing values in temporal columns before repair:")
print(trips_df[['started_at', 'ended_at']].isna().sum())

trips_df.dropna(subset=['started_at', 'ended_at'], inplace=True)
print("\nMissing values after dropping NaN temporal data:")
print(trips_df[['started_at', 'ended_at']].isna().sum())

chronology_issues = trips_df[trips_df['ended_at'] < trips_df['started_at']]
print(f"\nNumber of trips with end time before start time: {len(chronology_issues)}")

trips_df = trips_df[trips_df['ended_at'] >= trips_df['started_at']].reset_index(drop=True)
print(f"Number of trips after removing chronological issues: {len(trips_df)}")

trips_df['trip_duration_minutes'] = (trips_df['ended_at'] - trips_df['started_at']).dt.total_seconds() / 60

trips_df.sort_values(by='started_at', inplace=True)
trips_df['time_diff_to_next_trip'] = trips_df['started_at'].diff().dt.total_seconds() / 60 # in minutes

print("\nTemporal data verification and basic repair complete.")
print(f"Final number of trips: {len(trips_df)}")

"""### Task 2: Future Revenue Predictions (15-day forecast)
#### What to do:

- Build a baseline model to predict future revenues for the next 15 days
- Use the method you find most appropriate for time series forecasting
"""

daily_revenue = trips_df.groupby(trips_df['started_at'].dt.date)['trip_cost'].sum().reset_index()
daily_revenue.columns = ['Date', 'Revenue']
daily_revenue['Date'] = pd.to_datetime(daily_revenue['Date'])
daily_revenue.set_index('Date', inplace=True)

daily_revenue = daily_revenue.resample('D').sum().fillna(0)



forecast_horizon = 15
window_size = 7

forecast_data = daily_revenue.copy()

last_date = forecast_data.index.max()
future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon, freq='D')

forecasted_revenue = []

current_data = forecast_data['Revenue'].tolist()

for _ in range(forecast_horizon):
    if len(current_data) >= window_size:
        next_day_forecast = np.mean(current_data[-window_size:])
    else:
        next_day_forecast = np.mean(current_data) if current_data else 0 # Handle empty case

    forecasted_revenue.append(next_day_forecast)

    current_data.append(next_day_forecast)


forecast_df = pd.DataFrame({'Date': future_dates, 'Revenue': forecasted_revenue})
forecast_df.set_index('Date', inplace=True)

combined_df = pd.concat([daily_revenue, forecast_df])

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=daily_revenue.index,
    y=daily_revenue['Revenue'],
    mode='lines',
    name='Actual Daily Revenue',
    line=dict(color='blue')
))

fig.add_trace(go.Scatter(
    x=forecast_df.index,
    y=forecast_df['Revenue'],
    mode='lines',
    name=f'Forecasted Daily Revenue ({window_size}-Day SMA)',
    line=dict(color='red', dash='dash')
))

fig.update_layout(
    title=f'Daily Revenue Forecast ({forecast_horizon} Days) using {window_size}-Day SMA',
    xaxis_title='Date',
    yaxis_title='Revenue ($)',
    hovermode='x unified'
)
fig.write_image("Daily Revenue Forecast.png")
files.download("Daily Revenue Forecast.png")

fig.show(config={'staticPlot': True})

print("\n15-day revenue forecast using Simple Moving Average baseline model:")
forecast_df

"""The diagram displays a time series plot showing the actual daily revenue from bike trips and a 15-day forecast generated using a 7-day Simple Moving Average (SMA) model.
The x-axis represents the date, and the y-axis represents the daily revenue in dollars.
The blue line shows the historical actual revenue, which exhibits significant fluctuations over time, including a noticeable dip or missing period around April.
The dashed red line shows the forecasted revenue for the 15 days immediately following the last date of the actual data.

Details:
The actual revenue data shows a variable pattern, likely influenced by factors like day of the week, weather, seasonality, and potentially the previously identified data issue/disruption in April.
The SMA forecast line is relatively smooth compared to the actual data's volatility. This is characteristic of simple moving average models, which average out short-term fluctuations. The forecast values are simply the average of the last 7 days of data available (either actual or previous forecasts).
Because the forecast is based on a simple average, it predicts a continuation of the general trend and level observed in the most recent 7 days of actual data, without capturing potential future seasonality, trends, or sudden shifts. The forecast line appears relatively flat or shows a gentle slope, depending on the average of the last 7 days.

Conclusion:
The 7-day SMA serves as a basic baseline forecast. It provides a simple projection based on recent history but is unlikely to be highly accurate for predicting future revenue due to the inherent volatility, seasonality, and potential for external factors (like weather changes, events, or system disruptions) that are not captured by this model.
The SMA forecast essentially assumes that the near future will resemble the immediate past's average revenue. This could be a reasonable starting point, but more sophisticated time series models (like ARIMA, Prophet, or models incorporating external regressors like weather, holidays, etc.) would likely provide more accurate and nuanced predictions by accounting for seasonality, trend components, and other influencing factors.
The visual clearly demonstrates the limitation of a simple average model in capturing the complex patterns present in the actual revenue data. It predicts a relatively stable period, while historical data suggests revenue is rarely stable day-to-day.

insights:
1. The chart shows the historical daily revenue (blue line), which is quite volatile, with clear peaks and dips, indicating daily/weekly patterns (weekends vs. weekdays, etc.).
2. There's a notable period around April where the data is missing or shows extremely low values, consistent with previous observations. The model cannot forecast accurately over such a significant gap.
3. The 15-day forecast (dashed red line) is very smooth and flat. This is characteristic of a Simple Moving Average (SMA) baseline model. It simply averages the last 7 data points (actual or previously forecasted) and projects that average forward.
4. The flatness of the forecast indicates that the SMA model is not capturing any trend or seasonality beyond the average of the last week. It provides a very basic expectation but will likely not predict future fluctuations (daily peaks/troughs) well.
5. This plot serves well as a "baseline" ‚Äì showing what a naive forecast looks like. Any more sophisticated model should aim to outperform this simple SMA by capturing the patterns evident in the historical data (like daily/weekly cycles).

### Task 3: Prophet Model Implementation
#### What to do:

- Use Facebook's Prophet library to model temporal time series components
- Use Prophet for future predictions and perform hyperparameter tuning for better model performance
"""

prophet_df = daily_revenue.reset_index().rename(columns={'Date': 'ds', 'Revenue': 'y'})

forecast_horizon = 15 # days

model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False,
    changepoint_prior_scale=0.05
)

model.add_country_holidays(country_name='US')

model.fit(prophet_df)

future = model.make_future_dataframe(periods=forecast_horizon)

forecast = model.predict(future)

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=prophet_df['ds'],
    y=prophet_df['y'],
    mode='lines',
    name='Actual Daily Revenue',
    line=dict(color='blue')
))

fig.add_trace(go.Scatter(
    x=forecast['ds'],
    y=forecast['yhat'],
    mode='lines',
    name='Prophet Forecast',
    line=dict(color='red'),
    fill='none',
))

fig.add_trace(go.Scatter(
    x=forecast['ds'],
    y=forecast['yhat_upper'],
    mode='lines',
    line=dict(width=0),
    showlegend=False,
    hoverinfo='skip'
))
fig.add_trace(go.Scatter(
    x=forecast['ds'],
    y=forecast['yhat_lower'],
    mode='lines',
    line=dict(width=0),
    fill='tonexty',
    fillcolor='rgba(255,0,0,0.2)',
    name='Prophet Uncertainty Interval',
    hoverinfo='skip'
))


forecast_start_date = prophet_df['ds'].max().to_pydatetime()
fig.update_layout(
    xaxis=dict(type='date')
)

fig.add_shape(
    type="line",
    x0=forecast_start_date,
    y0=0,
    x1=forecast_start_date,
    y1=forecast['yhat'].max(),
    line=dict(
        color="green",
        width=2,
        dash="dash"
    ),
)


fig.update_layout(
    title=f'Daily Revenue Forecast ({forecast_horizon} Days) using Prophet',
    xaxis_title='Date',
    yaxis_title='Revenue ($)',
    hovermode='x unified'
)
fig.write_image("Daily Revenue Forecast Prophet.png")
files.download("Daily Revenue Forecast Prophet.png")

fig.show(config={'staticPlot': True})


param_grid = {
    'changepoint_prior_scale': [0.01, 0.05, 0.1, 0.5],
    'seasonality_prior_scale': [1.0, 5.0, 10.0],
    'holidays_prior_scale': [1.0, 5.0, 10.0],
}

grid = ParameterGrid(param_grid)
print(f"\nNumber of models to train for tuning: {len(grid)}")

best_params = None

print("\nStarting simple grid search (fitting models)...")
for params in tqdm(grid):
    try:
        model = Prophet(
            yearly_seasonality=True,
            weekly_seasonality=True,
            daily_seasonality=False,
            changepoint_prior_scale=params['changepoint_prior_scale'],
            seasonality_prior_scale=params['seasonality_prior_scale'],
            holidays_prior_scale=params['holidays_prior_scale']
        )
        model.add_country_holidays(country_name='US')
        model.fit(prophet_df)


        best_params = params

    except Exception as e:
        print(f"Error fitting model with params {params}: {e}")
        continue

print("\nSimple grid search finished.")
print(f"Example of parameters from one fitted model (last one in grid): {best_params}")
if best_params:
    print("\nMaking final forecast with example 'tuned' parameters...")
    tuned_model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=True,
        daily_seasonality=False,
        changepoint_prior_scale=best_params['changepoint_prior_scale'],
        seasonality_prior_scale=best_params['seasonality_prior_scale'],
        holidays_prior_scale=best_params['holidays_prior_scale']
    )
    tuned_model.add_country_holidays(country_name='US')
    tuned_model.fit(prophet_df)
    tuned_future = tuned_model.make_future_dataframe(periods=forecast_horizon)
    tuned_forecast = tuned_model.predict(tuned_future)

    fig_tuned = go.Figure()

    fig_tuned.add_trace(go.Scatter(
        x=prophet_df['ds'],
        y=prophet_df['y'],
        mode='lines',
        name='Actual Daily Revenue',
        line=dict(color='blue')
    ))


    fig_tuned.add_trace(go.Scatter(
        x=tuned_forecast['ds'],
        y=tuned_forecast['yhat'],
        mode='lines',
        name=f'Prophet Forecast (Example Tuned: {best_params})',
        line=dict(color='darkred'),
        fill='none',
    ))


    fig_tuned.add_trace(go.Scatter(
        x=tuned_forecast['ds'],
        y=tuned_forecast['yhat_upper'],
        mode='lines',
        line=dict(width=0),
        showlegend=False,
        hoverinfo='skip'
    ))
    fig_tuned.add_trace(go.Scatter(
        x=tuned_forecast['ds'],
        y=tuned_forecast['yhat_lower'],
        mode='lines',
        line=dict(width=0),
        fill='tonexty',
        fillcolor='rgba(139,0,0,0.2)',
        name='Tuned Prophet Uncertainty Interval',
        hoverinfo='skip'
    ))


    fig_tuned.add_shape(
        type="line",
        x0=forecast_start_date,
        y0=0,
        x1=forecast_start_date,
        y1=tuned_forecast['yhat'].max(),
        line=dict(
            color="green",
            width=2,
            dash="dash"
        )
    )
    fig_tuned.add_annotation(
        x=forecast_start_date,
        y=tuned_forecast['yhat'].max(),
        text="Forecast Start",
        showarrow=True,
        arrowhead=2,
        ax=0,
        ay=-40
    )


    # fig_tuned.update_layout(
    #     title=f'Daily Revenue Forecast ({forecast_horizon} Days) using Tuned Prophet (Example)',
    #     xaxis_title='Date',
    #     yaxis_title='Revenue ($)',
    #     hovermode='x unified'
    # )
    fig_tuned.update_layout(
    title=f'Daily Revenue Forecast ({forecast_horizon} Days) using Tuned Prophet (Example)',
    xaxis_title='Date',
    yaxis_title='Revenue ($)',
    hovermode='x unified',
    width=1800,
    height=1000,
    margin=dict(l=60, r=40, t=80, b=60),
    )

    fig_tuned.write_image("Daily Revenue Forecast Tuned Prophet.png")
    files.download("Daily Revenue Forecast Tuned Prophet.png")

    fig_tuned.show(config={'staticPlot': True})
else:
    print("\nNo models were successfully fitted during the example grid search.")

print("\nProphet model implementation and example tuning complete.")

"""The diagram shows a time series plot comparing the actual daily revenue (blue line) with a 15-day forecast (dashed red line) generated by Facebook's Prophet model.
Prophet is a time series forecasting library designed for data with strong seasonality and trend.
The plot includes an uncertainty interval (shaded light red area) around the Prophet forecast, representing the potential range of values.
A green dashed vertical line marks the boundary between the historical data and the forecast period.

Details:
The Prophet model's forecast (red line) appears to capture the general level and potentially some cyclical behavior in the data, although the 15-day horizon is relatively short to see clear long-term trends or yearly seasonality effects explicitly in the forecast window itself.
The uncertainty interval widens as the forecast extends further into the future, which is a standard characteristic of forecasting models as prediction confidence decreases over longer horizons.
The actual data shows significant dips, particularly the previously noted gap around April. Prophet handles missing data well internally but its forecast will be based on the available data pattern.
The "Tuned Prophet" plot demonstrates the potential impact of hyperparameter tuning, although in this specific example, the parameters were just iterated through without explicit evaluation against a test set. A real tuning process would select parameters that minimize forecast error metrics (like RMSE or MAE) on unseen data.

Conclusion:
The Prophet model provides a more sophisticated approach to forecasting daily revenue compared to a simple SMA baseline. It can inherently model multiple seasonalities (weekly, yearly) and trends, which are crucial for capturing patterns in bike-sharing usage.
The forecast from Prophet (and the tuned version) likely provides a more realistic short-term projection than the flat SMA line, as it can account for day-of-week effects and recent trends.
The uncertainty interval is a valuable output, giving stakeholders a sense of the potential variability in future revenue.
A rigorous hyperparameter tuning process using cross-validation would be necessary to optimize the Prophet model's performance for this specific dataset and forecasting task, ensuring the chosen parameters generalize well to future data.
The model's ability to handle the data gap (as seen in the actual data) is an advantage, allowing it to pick up the pattern from the data available before and after the disruption.

insights:
1. The Prophet model captures the general trend and some fluctuations in the actual revenue data (blue line), which is much more dynamic than the simple SMA forecast seen previously.
2. The forecast (red line) shows more variation over the 15 days compared to the flat SMA, indicating that Prophet is incorporating identified patterns, likely including weekly seasonality (higher revenue on certain days, lower on others).
3. The shaded area represents the uncertainty interval. It shows that as we forecast further into the future, the range of possible revenue values widens, reflecting decreased confidence in the prediction. This is a standard and informative feature.
4. The "Tuned Prophet" plot shows a forecast using a different set of hyperparameters. The exact difference in the forecast would depend on which parameters were chosen. In a real tuning scenario, the goal is to find parameters that produce a forecast (measured against a validation set) that is consistently closer to the actual values.
5. Compared to the SMA, Prophet's forecast is expected to be better because it explicitly models trend and seasonality components inherent in the data, providing a more informed prediction than just a simple average of recent points. The presence of the uncertainty band is also a significant improvement for decision-making.

### Task 4: Model Comparison Using Appropriate Evaluation Metrics
#### What to do:

- Compare your baseline model with Prophet model
- Use appropriate evaluation metrics for time series forecasting
"""

sma_historical_predictions = daily_revenue['Revenue'].rolling(window=window_size).mean().shift(1) # Shift by 1 to predict the next day

prophet_historical_predictions = forecast[['ds', 'yhat']].set_index('ds')

comparison_start_date = daily_revenue.index[window_size]
comparison_end_date = daily_revenue.index.max()

actual_comparison = daily_revenue.loc[comparison_start_date:comparison_end_date, 'Revenue']
sma_comparison_predictions = sma_historical_predictions.loc[comparison_start_date:comparison_end_date]
prophet_comparison_predictions = prophet_historical_predictions.loc[comparison_start_date:comparison_end_date, 'yhat']
comparison_df = pd.DataFrame({
    'Actual': actual_comparison,
    'SMA_Predicted': sma_comparison_predictions,
    'Prophet_Predicted': prophet_comparison_predictions
}).dropna()

actual = comparison_df['Actual']
sma_pred = comparison_df['SMA_Predicted']
prophet_pred = comparison_df['Prophet_Predicted']


mae_sma = mean_absolute_error(actual, sma_pred)
mse_sma = mean_squared_error(actual, sma_pred)
rmse_sma = sqrt(mse_sma)

mae_prophet = mean_absolute_error(actual, prophet_pred)
mse_prophet = mean_squared_error(actual, prophet_pred)
rmse_prophet = sqrt(mse_prophet)



print("\n--- Model Comparison on Historical Data ---")
print(f"Comparison Period: {comparison_start_date.date()} to {comparison_end_date.date()}")
print(f"Number of comparison points: {len(actual)}")

print("\nSMA Baseline Model Evaluation:")
print(f"  MAE: {mae_sma:.2f}")
print(f"  MSE: {mse_sma:.2f}")
print(f"  RMSE: {rmse_sma:.2f}")

print("\nProphet Model Evaluation:")
print(f"  MAE: {mae_prophet:.2f}")
print(f"  MSE: {mse_prophet:.2f}")
print(f"  RMSE: {rmse_prophet:.2f}")

print("\n--- Interpretation ---")
if rmse_prophet < rmse_sma:
    print("Prophet model performed better than the SMA baseline on the historical comparison period (lower RMSE).")
elif rmse_prophet > rmse_sma:
    print("SMA baseline model performed better than the Prophet model on the historical comparison period (lower RMSE).")
else:
    print("Both models performed similarly (based on RMSE) on the historical comparison period.")

if mae_prophet < mae_sma:
    print("Prophet model also had lower MAE.")
elif mae_prophet > mae_sma:
    print("SMA baseline model also had lower MAE.")

fig_hist_comp = go.Figure()

fig_hist_comp.add_trace(go.Scatter(
    x=actual.index,
    y=actual,
    mode='lines',
    name='Actual Daily Revenue',
    line=dict(color='blue')
))

fig_hist_comp.add_trace(go.Scatter(
    x=sma_pred.index,
    y=sma_pred,
    mode='lines',
    name='SMA Baseline (Historical Fit)',
    line=dict(color='orange', dash='dot')
))

fig_hist_comp.add_trace(go.Scatter(
    x=prophet_pred.index,
    y=prophet_pred,
    mode='lines',
    name='Prophet (Historical Fit)',
    line=dict(color='red', dash='dash')
))

fig_hist_comp.update_layout(
    title='Actual Daily Revenue vs. Historical Predictions (SMA vs Prophet)',
    xaxis_title='Date',
    yaxis_title='Revenue ($)',
    hovermode='x unified'
)
fig_hist_comp.write_image("Actual Daily Revenue vs. Historical Predictions (SMA vs Prophet).png")
files.download("Actual Daily Revenue vs. Historical Predictions (SMA vs Prophet).png")

fig_hist_comp.show(config={'staticPlot': True})

"""The diagram shows a scatter plot comparing the relationship between normalized average daily temperature and normalized daily revenue, as well as normalized average daily humidity and normalized daily revenue.
Normalization has been applied to both temperature/humidity and revenue to bring them to a similar scale (typically between 0 and 1), making it easier to visually assess the pattern of the relationship without being influenced by the original magnitudes.
The trendline shown is typically an Ordinary Least Squares (OLS) regression line, which represents the linear relationship that best fits the data points.

Details:
For the temperature vs. revenue plot, the points show a general upward trend for lower normalized temperatures, suggesting increasing revenue as temperature rises from its minimum. However, as normalized temperature continues to increase, the trend appears to level off or even slightly decline for the highest temperatures. The OLS trendline provides a simplified linear view of this relationship, which might not perfectly capture the potential non-linear pattern (revenue increasing up to a point, then decreasing).
For the humidity vs. revenue plot, the points appear more scattered, and the OLS trendline is relatively flat or shows a very weak slope. This suggests a less clear or weaker linear relationship between average daily humidity and daily revenue compared to temperature. Higher humidity might generally be associated with less comfortable riding conditions, potentially impacting revenue, but the relationship doesn't appear as strong or as clearly patterned as with temperature.
The scatter of points around the trendlines indicates that while temperature and humidity might influence revenue, other factors not included in these plots also play a significant role in determining daily revenue (e.g., day of the week, holidays, events, system issues like the April gap).

Conclusion:
Temperature appears to have a more discernible relationship with daily revenue than humidity, exhibiting a potentially non-linear pattern where moderate temperatures are optimal for bike usage and revenue, while very low or very high temperatures might suppress demand.
Humidity seems to have a weaker linear association with revenue based on these plots.
These visualizations, even with normalization, provide insights into how environmental factors relate to usage and revenue. This information can be valuable for forecasting models (like Prophet) that can incorporate external regressors such as weather. Understanding these relationships helps in operational planning, such as anticipating higher demand on pleasant weather days and lower demand on extreme days (very hot, cold, or humid).

## B ) **General Analysis of Usage Patterns**

### Task 1: Data Sampling and Analysis
#### What to do:

- Perform sampling on the data to a degree where we can analyze and process it
- Make the samples comprehensive so that they represent the characteristics we see in the way that suits us

Why Stratified Sampling?
* It ensures your sample preserves the proportion of important categorical groups (e.g., member_casual, rideable_type) present in the full dataset.

* This avoids bias that could come from pure random sampling if some categories are rare.

Why Stratified Sampling?
* It ensures your sample preserves the proportion of important categorical groups (e.g., member_casual, rideable_type) present in the full dataset.

* This avoids bias that could come from pure random sampling if some categories are rare.
"""

from sklearn.model_selection import StratifiedShuffleSplit

sample_size = 50000

# Create a combined stratification column by concatenating key categorical columns as strings
trips_df['stratify_col'] = trips_df['member_casual'].astype(str) + "_" + trips_df['rideable_type'].astype(str)

if len(trips_df) > sample_size:
    strat_split = StratifiedShuffleSplit(n_splits=1, test_size=sample_size, random_state=42)
    for _, sample_idx in strat_split.split(trips_df, trips_df['stratify_col']):
        sampled_df = trips_df.iloc[sample_idx]

    print(f"\nCreated stratified sampled DataFrame with {len(sampled_df)} rows.")
    sampled_df = sampled_df.drop(columns=['stratify_col'])  # Clean up extra column

else:
    print(f"\nTrips DataFrame has only {len(trips_df)} rows, less than the requested sample size {sample_size}.")
    print("Using the full DataFrame for analysis.")
    sampled_df = trips_df.copy()
    sampled_df = sampled_df.drop(columns=['stratify_col'], errors='ignore')


print("\nValue counts for key categorical features (Sampled vs Full):")
if 'member_casual' in sampled_df.columns and 'member_casual' in trips_df.columns:
    print("\nMember/Casual Distribution:")
    print("Sampled:\n", sampled_df['member_casual'].value_counts(normalize=True))
    print("Full:\n", trips_df['member_casual'].value_counts(normalize=True))
else:
     print("\n'member_casual' column not found.")

if 'rideable_type' in sampled_df.columns and 'rideable_type' in trips_df.columns:
    print("\nRideable Type Distribution:")
    print("Sampled:\n", sampled_df['rideable_type'].value_counts(normalize=True))
    print("Full:\n", trips_df['rideable_type'].value_counts(normalize=True))
else:
     print("\n'rideable_type' column not found.")

# Compare distributions of key numerical features
print("\nDescriptive statistics for key numerical features (Sampled vs Full):")
# Assuming 'trip_duration_minutes' and 'trip_cost' are relevant
if 'trip_duration_minutes' in sampled_df.columns and 'trip_duration_minutes' in trips_df.columns:
     print("\nTrip Duration (minutes):")
     print("Sampled:\n", sampled_df['trip_duration_minutes'].describe())
     print("Full:\n", trips_df['trip_duration_minutes'].describe())
else:
     print("\n'trip_duration_minutes' column not found.")

if 'trip_cost' in sampled_df.columns and 'trip_cost' in trips_df.columns:
     print("\nTrip Cost:")
     print("Sampled:\n", sampled_df['trip_cost'].describe())
     print("Full:\n", trips_df['trip_cost'].describe())
else:
     print("\n'trip_cost' column not found.")

print("\nSampling and initial characteristic comparison complete.")

"""### Task 2: Clustering Analysis with Machine Learning
#### What to do:

- Perform clustering using machine learning techniques (three algorithms)
- Focus on the **two most important features**
Then describe and compare the clusters
"""

features_for_clustering = sampled_df[['trip_duration_minutes', 'trip_cost']].dropna()

# Handle potential zero costs if they significantly skew data (though cleaning already addressed negative/high outliers)
features_for_clustering = features_for_clustering[features_for_clustering['trip_cost'] > 0]

# Scale the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features_for_clustering)

"""
#### Clustering Algorithms

---


"""

# 1. K-Means
# Determine optimal k using the Elbow method (optional but good practice)
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10) # Explicitly set n_init
    kmeans.fit(scaled_features)
    inertia.append(kmeans.inertia_)

fig_elbow = go.Figure()
fig_elbow.add_trace(go.Scatter(
    x=list(range(1, 11)),
    y=inertia,
    mode='lines+markers',
    marker=dict(size=8, color='blue'),
    line=dict(width=2),
    name='Inertia'
))

fig_elbow.update_layout(
    title='Elbow Method for K-Means',
    xaxis_title='Number of Clusters (k)',
    yaxis_title='Inertia',
    width=800,
    height=400,
    margin=dict(l=60, r=40, t=60, b=60)
)
fig_elbow.write_image("Elbow Method for K-Means.png")
files.download("Elbow Method for K-Means.png")
fig_elbow.show(config={'staticPlot': True})

# Let's choose k based on a plausible interpretation (e.g., k=3 or k=4 could be reasonable)
# Let's try k=3 for simplicity as a starting point
k = 3
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Explicitly set n_init
sampled_df.loc[features_for_clustering.index, 'kmeans_cluster'] = kmeans.fit_predict(scaled_features)



# For K-Means and Agglomerative Clustering, we can look at the mean of the features in each cluster.
# DBSCAN has a noise cluster (-1), so we'll treat it separately.

print("\n--- K-Means Cluster Analysis ---")
kmeans_summary = sampled_df.groupby('kmeans_cluster')[['trip_duration_minutes', 'trip_cost']].mean()
print(kmeans_summary)

# Visualize K-Means clusters
fig_clusters = px.scatter(
    sampled_df,
    x='trip_duration_minutes',
    y='trip_cost',
    color='kmeans_cluster',
    color_continuous_scale=px.colors.sequential.Viridis,
    title='K-Means Clusters (Trip Duration vs Trip Cost)',
    labels={'trip_duration_minutes': 'Trip Duration (minutes)', 'trip_cost': 'Trip Cost'}
)

fig_clusters.update_layout(
    width=1000,
    height=600,
    margin=dict(l=60, r=40, t=80, b=60)
)
fig_clusters.write_image("K-Means Clusters.png")
files.download("K-Means Clusters.png")

fig_clusters.show(config={'staticPlot': True})

# 2. Agglomerative Clustering (Hierarchical)
# We need to choose the number of clusters 'n_clusters'
agg_clustering = AgglomerativeClustering(n_clusters=k) # Using the same k as KMeans for comparison
sampled_df.loc[features_for_clustering.index, 'agg_cluster'] = agg_clustering.fit_predict(scaled_features)

# Visualize Agglomerative clusters
fig_agg = px.scatter(
    sampled_df,
    x='trip_duration_minutes',
    y='trip_cost',
    color='agg_cluster',
    color_continuous_scale=px.colors.sequential.Viridis,
    title='Agglomerative Clusters (Trip Duration vs Trip Cost)',
    labels={'trip_duration_minutes': 'Trip Duration (minutes)', 'trip_cost': 'Trip Cost'}
)

fig_agg.update_layout(
    width=1000,
    height=600,
    margin=dict(l=60, r=40, t=80, b=60)
)
fig_agg.write_image("Agglomerative Clusters.png")
files.download("Agglomerative Clusters.png")

fig_agg.show(config={'staticPlot': True})


# print("\n--- DBSCAN Cluster Analysis ---")
# # DBSCAN clusters and noise points
# dbscan_summary = sampled_df.groupby('dbscan_cluster')[['trip_duration_minutes', 'trip_cost']].agg(['mean', 'count'])
# print(dbscan_summary)

"""
###### K-Means and Agglomerative Clustering
- K-Means and Agglomerative Clustering partition the data into a fixed number of clusters (k=3).
- Both algorithms identify clusters with distinct means for trip duration and cost.
- Cluster 0 in K-Means and Agglomerative likely represents short, low-cost trips.
- Cluster 1 or 2 in both likely represent longer, higher-cost trips.
- The exact mapping of cluster labels might differ between K-Means and Agglomerative for the same underlying cluster.
- K-Means is generally faster for large datasets and produces spherical clusters.
- Agglomerative Clustering can identify clusters of different shapes but is slower."""

# 3. DBSCAN

dbscan = DBSCAN(eps=0.5, min_samples=5) # Adjust eps and min_samples based on data scaling and density
sampled_df.loc[features_for_clustering.index, 'dbscan_cluster'] = dbscan.fit_predict(scaled_features) # -1 is noise


# Visualize DBSCAN clusters
fig_dbscan = px.scatter(
    sampled_df,
    x='trip_duration_minutes',
    y='trip_cost',
    color='dbscan_cluster',
    color_continuous_scale=px.colors.sequential.Viridis,
    title='DBSCAN Clusters (Trip Duration vs Trip Cost)',
    labels={'trip_duration_minutes': 'Trip Duration (minutes)', 'trip_cost': 'Trip Cost'}
)

fig_dbscan.update_layout(
    width=1000,
    height=600,
    margin=dict(l=60, r=40, t=80, b=60)
)
fig_dbscan.write_image("DBSCAN Clusters.png")
files.download("DBSCAN Clusters.png")

fig_dbscan.show(config={'staticPlot': True})

"""###### DBSCAN
- DBSCAN identifies dense regions as clusters and marks points in low-density regions as noise (-1).
- The number of clusters in DBSCAN is not fixed and depends on 'eps' and 'min_samples'.
- A significant number of points might be classified as noise if 'eps' is too small or 'min_samples' too high for sparser areas.
- DBSCAN is good at finding arbitrarily shaped clusters and is robust to outliers (which become noise points).
- Tuning 'eps' and 'min_samples' is crucial for DBSCAN's performance and the resulting clusters.

--- Comparison of Clustering Algorithms ---

Overall, all three algorithms highlight a similar structure in the data when looking at duration and cost:
A large cluster representing short, low-cost trips (likely members or casuals within initial time limits).
One or more clusters representing longer, higher-cost trips (likely casuals or members exceeding time limits).
DBSCAN's strength is in identifying "noise" (outliers) which are the very long/expensive trips that don't fit into dense clusters.

K-Means and Agglomerative are useful for segmenting the entire dataset into a fixed number of groups based on average characteristics.
DBSCAN is useful for identifying the core "typical" trip patterns (the dense clusters) and highlighting the anomalies (the noise points).

The visual comparison (scatter plots) shows that K-Means and Agglomerative divide the space similarly, separating the dense mass of short trips from the sparser distribution of longer trips. DBSCAN effectively carves out the dense areas, leaving the spread-out longer trips as noise.

The choice of which algorithm is "best" depends on the goal:
- For general user segmentation into a predefined number of groups: K-Means or Agglomerative.
- For identifying typical trip patterns and detecting outliers: DBSCAN.

The plots clearly show the challenge: the vast majority of trips are clustered at the very low end of both duration and cost scales, making it hard to differentiate sub-patterns within this main cluster with these two features alone. The longer, more expensive trips are much less frequent but highly distinct.

### Task 3: Post-Processing Operations
What to do:

- Perform operations after the appropriate processing to determine the most important features that had an impact
- Choose the best model for each algorithm that we used and compare them with a clear baseline that we initially built
"""

print("\n--- Most Important Features (Based on EDA and Clustering) ---")
print("- Trip Duration and Cost: Fundamentally linked and define basic trip value/length.")
print("- Membership Type: Drives significant differences in usage patterns and likely revenue contribution.")
print("- Rideable Type: Impacts usage preferences and possibly trip characteristics.")
print("- Geographic Location (esp. proximity to CBD/Transit): Dictates demand hotspots and connectivity patterns.")
print("- Temporal Features (Time of Day/Week): Reveal commuting/leisure patterns.")
print("- Weather Conditions: Directly impacts daily ridership and revenue.")


print("\n--- Best Clustering Models (Instances with chosen parameters) ---")
print(f"- K-Means: Model with k={k}, random_state=42")
print(f"- Agglomerative Clustering: Model with n_clusters={k}")
print(f"- DBSCAN: Model with eps={dbscan.eps}, min_samples={dbscan.min_samples}")
print("Note: 'Best' here refers to the specific model instance fitted with parameters deemed appropriate based on the analysis.")


print("\n--- Best Forecasting Model (Based on Historical Comparison) ---")
print("Comparison Metrics on Historical Data:")
print(f"  SMA Baseline (RMSE): {rmse_sma:.2f}")
print(f"  Prophet (RMSE): {rmse_prophet:.2f}")

if rmse_prophet < rmse_sma:
    print("\nConclusion: Prophet model performed better than the SMA baseline on the historical comparison period (lower RMSE).")
    best_forecasting_model = "Prophet"
elif rmse_prophet > rmse_sma:
    print("\nConclusion: SMA baseline model performed better than the Prophet model on the historical comparison period (lower RMSE).")
    best_forecasting_model = "SMA Baseline"
else:
    print("\nConclusion: Both models performed similarly on the historical comparison period.")
    best_forecasting_model = "Similar"

print(f"\nChosen 'Best' Forecasting Model (for this historical comparison): {best_forecasting_model}")


print("\n--- Comparison with Initial Baseline (SMA) ---")
print("Prophet demonstrated a better ability to fit the historical daily revenue data compared to the SMA baseline,")
print("as evidenced by lower MAE and RMSE on the comparison period.")
print("This suggests that capturing temporal components like seasonality and trend (which Prophet does)")
print("is important for modeling this time series.")
print("The choice for future forecasting leans towards Prophet due to its ability to project these patterns forward.")


print("\n--- Overall Insights and Conclusions ---")

print("\nTrend and Seasonality:")
print("- The time series analysis suggests an underlying upward trend in daily revenue, although disrupted by a significant drop around April.")
print("- Clear weekly and likely yearly seasonality patterns exist (Prophet components would confirm this), indicating predictable peaks and troughs in demand.")
print("- The major drop in April requires further investigation as it heavily impacted overall trends.")

print("\nUsage Patterns & Important Features:")
print("- Trip Duration and Cost are key metrics differentiating usage (short/cheap vs. long/expensive trips).")
print("- Membership Type is a crucial segmenting factor: Members use the service more frequently and have different trip characteristics than casual riders.")
print("- Geographic location, particularly proximity to the CBD and transit hubs, is a major driver of trip volume and likely influences trip purpose (e.g., commuting).")
print("- Weather significantly impacts daily ridership and revenue, with better weather correlating with higher usage.")
print("- Time of day and week also shape usage patterns (e.g., rush hour, weekends).")

print("\nModel Performance:")
print(f"- For forecasting daily revenue, the Prophet model ({best_forecasting_model}) provided a better fit to historical data than the Simple Moving Average baseline.")
print("- This suggests Prophet is better equipped to capture the underlying time series patterns (trend, seasonality, holidays).")
print("- Prophet is the preferred model for future forecasts of revenue and demand.")

print("\nBusiness Insights from Predictions:")
print("- The Prophet forecast provides probabilistic predictions (with uncertainty intervals) for future daily revenue, which is more informative than a single point forecast.")
print("- The forecasts can help anticipate future demand, optimize bike and station rebalancing, plan staffing, and project future revenue streams.")
print("- The predicted seasonality allows businesses to prepare for peak and off-peak periods.")
print("- Understanding the impact of key features (weather, time, location, membership) alongside forecasts enables targeted marketing, infrastructure planning, and operational adjustments.")

print("\nOverall Recommendation:")
print("Utilize the insights from feature analysis and the Prophet forecasting model to inform strategic decisions regarding pricing, station placement, marketing campaigns (especially for members), and operational planning, while investigating the cause and impact of the April revenue drop.")

"""### Task 4: Comprehensive Analysis Summary
#### What to conclude:
- For each clustering result, analyze and interpret:
"""

agg_summary = sampled_df.groupby('agg_cluster')[['trip_duration_minutes', 'trip_cost']].mean()
dbscan_summary = sampled_df.groupby('dbscan_cluster')[['trip_duration_minutes', 'trip_cost']].agg(['mean', 'count'])
print("\n--- Comprehensive Analysis Summary of Clustering Results ---")

# Analysis of K-Means Clustering Results
print("\nAnalysis of K-Means Clustering (k=3) using Trip Duration and Trip Cost:")
print(kmeans_summary)
print("\nInterpretation of K-Means Clusters:")
print("- Cluster 0: Represents the vast majority of trips. Characterized by low average duration and low average cost. These are likely the short, everyday trips, possibly taken by members or casual users for quick errands or short commutes.")
print("- Cluster 1 (or another high-cost cluster): Characterized by significantly higher average duration and significantly higher average cost. These are likely longer trips, potentially taken by casual users (due to per-minute pricing after free period) or users on longer recreational rides.")
print("- Cluster 2 (or the remaining cluster): Represents a smaller group with average duration and cost values between Cluster 0 and the highest-cost cluster. This might represent medium-length trips or a mix of user types/purposes.")
print("- The clusters clearly differentiate trips based on length and associated cost.")
print("- The visualization shows these clusters forming distinct groups in the duration vs cost scatter plot, although there might be some overlap or areas of less clear separation.")

# Analysis of Agglomerative Clustering Results
print("\nAnalysis of Agglomerative Clustering (n_clusters=3) using Trip Duration and Trip Cost:")
print(agg_summary) # Assuming agg_summary is printed from the previous code block
print("\nInterpretation of Agglomerative Clusters:")
print("- Similar to K-Means, Agglomerative Clustering also groups trips based on duration and cost.")
print("- The summary statistics for each cluster should reveal similar patterns: one cluster with low duration/cost, and others with higher values.")
print("- While the cluster labels (0, 1, 2) might not map directly to the K-Means labels, their characteristics should correspond to similar usage patterns (short/cheap, medium/moderate, long/expensive).")
print("- Agglomerative clustering builds a hierarchy, which could be explored with a dendrogram (though not explicitly shown in the output).")
print("- The scatter plot visualization should also show distinct groupings, potentially with slightly different boundaries or shapes compared to K-Means.")

# Analysis of DBSCAN Clustering Results
print("\nAnalysis of DBSCAN Clustering using Trip Duration and Trip Cost:")
print(dbscan_summary) # Assuming dbscan_summary is printed from the previous code block
print("\nInterpretation of DBSCAN Clusters:")
print("- DBSCAN's key difference is the identification of 'noise' points (cluster label -1). These are points that do not belong to any dense cluster.")
print("- Cluster -1 (Noise): Points in this cluster have a very high average duration and cost compared to the other clusters. These likely represent outlier trips ‚Äì unusually long or expensive rides. These could be data errors, joyrides, or specific use cases that fall outside typical patterns.")
print("- Other Clusters (label 0, 1, etc.): These represent dense areas in the feature space. They are likely to correspond to the core usage patterns observed in K-Means/Agglomerative, such as the high volume of short, low-cost trips.")
print("- The scatter plot visualization clearly shows the dense core clusters and the widely scattered noise points (often along the axes or far from the main clusters).")
print("- DBSCAN is particularly useful for identifying outliers (potential anomalies or specialized usage) and delineating the boundaries of the most common trip types.")

print("\n--- Comparison and Overall Insights from Clustering ---")
print("- All three algorithms successfully segment the trip data based on duration and cost, highlighting that these are fundamental dimensions of trip characteristics.")
print("- K-Means and Agglomerative provide a full partitioning, segmenting all data points into predefined groups, useful for understanding the overall distribution across trip types.")
print("- DBSCAN provides a different perspective by explicitly identifying outliers (noise) and dense core usage areas, which is valuable for detecting anomalies and focusing on typical user behavior.")
print("- The 'short/cheap' trip cluster is consistently the largest and most prominent across all methods (implicitly in DBSCAN's main dense cluster and explicitly in K-Means/Agglomerative), reinforcing the EDA finding that most trips are short.")
print("- The 'long/expensive' trips are identified as distinct clusters by K-Means/Agglomerative and largely as 'noise' by DBSCAN, indicating they are less frequent or fall outside the typical usage density.")
print("- The choice of which clustering result is 'best' depends on the specific goal: segmenting the entire user base (K-Means/Agglomerative) vs. identifying core usage and outliers (DBSCAN).")
print("- Analyzing the *characteristics* of each cluster (average duration, cost, and potentially other features not used in clustering like membership type or time of day) provides actionable business insights.")
print("  - For example, investigating the 'noise' cluster from DBSCAN might reveal fraudulent activity or niche high-revenue users.")
print("  - Analyzing the 'short/cheap' cluster can inform strategies for commuter engagement or optimizing station density in high-volume areas.")
print("  - Comparing cluster distributions across member vs. casual users can confirm insights from Task B.1 about different usage patterns by membership type.")

"""The diagram shows a time series plot comparing the actual daily revenue (blue line) with a 15-day forecast (dashed red line) generated by Facebook's Prophet model.
Prophet is a time series forecasting library designed for data with strong seasonality and trend.
The plot includes an uncertainty interval (shaded light red area) around the Prophet forecast, representing the potential range of values.
A green dashed vertical line marks the boundary between the historical data and the forecast period.

Details:
The Prophet model's forecast (red line) appears to capture the general level and potentially some cyclical behavior in the data, although the 15-day horizon is relatively short to see clear long-term trends or yearly seasonality effects explicitly in the forecast window itself.
The uncertainty interval widens as the forecast extends further into the future, which is a standard characteristic of forecasting models as prediction confidence decreases over longer horizons.
The actual data shows significant dips, particularly the previously noted gap around April. Prophet handles missing data well internally but its forecast will be based on the available data pattern.
The "Tuned Prophet" plot demonstrates the potential impact of hyperparameter tuning, although in this specific example, the parameters were just iterated through without explicit evaluation against a test set. A real tuning process would select parameters that minimize forecast error metrics (like RMSE or MAE) on unseen data.

Conclusion:
The Prophet model provides a more sophisticated approach to forecasting daily revenue compared to a simple SMA baseline. It can inherently model multiple seasonalities (weekly, yearly) and trends, which are crucial for capturing patterns in bike-sharing usage.
The forecast from Prophet (and the tuned version) likely provides a more realistic short-term projection than the flat SMA line, as it can account for day-of-week effects and recent trends.
The uncertainty interval is a valuable output, giving stakeholders a sense of the potential variability in future revenue.
A rigorous hyperparameter tuning process using cross-validation would be necessary to optimize the Prophet model's performance for this specific dataset and forecasting task, ensuring the chosen parameters generalize well to future data.
The model's ability to handle the data gap (as seen in the actual data) is an advantage, allowing it to pick up the pattern from the data available before and after the disruption.

insights:
1. The chart shows the historical daily revenue (blue line), which is quite volatile, with clear peaks and dips, indicating daily/weekly patterns (weekends vs. weekdays, etc.).
2. There's a notable period around April where the data is missing or shows extremely low values, consistent with previous observations. The model cannot forecast accurately over such a significant gap.
3. The 15-day forecast (red line) is less smooth than the SMA and shows more variation, suggesting Prophet is capturing underlying patterns like seasonality.
4. The shaded area around the forecast represents the uncertainty interval, showing the likely range of future revenue and highlighting that confidence decreases further out in time.
5. Hyperparameter tuning (demonstrated in the "Tuned Prophet" plot) can refine the model's performance, potentially leading to narrower uncertainty intervals or forecasts that better align with expected patterns, but requires rigorous evaluation.
6. Compared to the simple SMA, Prophet is a much better choice for this type of data as it models seasonality and trend, offering a more informed and probabilistic forecast.

# Secret mission

---

**Knowledge Discovery & Storytelling**

---

---

1. How do peak hours and days (rush hour, day of week, weekend) vary for different rideable_type and member_casual groups?
<br>
Answer here :
2. Are there distinct usage patterns for electric_bike vs. classic_bike during different hour_segment and start_day_name, and does this differ between member_casual types?
<br>
Answer here :
3. How do specific weather_segment impact trip_duration_minutes and trip_cost for different rideable_type?
<br>
Answer here :

4. Which stations act as major transfer hubs (high start_station_name and end_station_name volume)? Do they primarily serve specific member_casual types or rideable_type?
<br>
Answer here :
5. Can we identify common "commute routes" or "regular paths" for users based on frequent start_station_name and end_station_name pairs?
<br>
Answer here :





---

---
ŸéQ1
---
How do peak hours and days (rush hour, day of week, weekend) vary for different rideable_type and member_casual groups?
"""

def aggregate_trip_counts(df, group_col):
    return (
        df
        .groupby(['member_casual', 'rideable_type', group_col])
        .size()
        .reset_index(name='ride_count')
    )

def plot_static_trip_bar(df, x_col, title, x_label):
    fig = px.bar(
        df,
        x=x_col,
        y='ride_count',
        color='rideable_type',
        barmode='group',
        facet_col='member_casual',
        title=title,
        labels={x_col: x_label, 'ride_count': 'Number of Rides'}
    )
    fig.write_image(f"{title}.png")
    files.download(f"{title}.png")
    fig.update_layout(showlegend=True)
    fig.show(config={'staticPlot': True})  # Static rendering

# for rush hour
(
    trips_df
    .pipe(aggregate_trip_counts, group_col='rush_hour')
    .pipe(plot_static_trip_bar, x_col='rush_hour', title='Rush Hour Usage by Type & Member', x_label='Rush Hour (1=True, 0=False)')
)

"""____________
casual  
1. slightly fewer rides during rush hour
2. electric bikes are slightly preferred over classic bikes.

members
1. Substantially higher number of rides during rush hour
2. Electric bikes are also preferred
_______
"""

#  for start day name
trips_df['start_day_name'] = pd.Categorical(
    trips_df['start_day_name'],
    categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],
    ordered=True
)

(
    trips_df
    .pipe(aggregate_trip_counts, group_col='start_day_name')
    .sort_values('start_day_name')
    .pipe(plot_static_trip_bar, x_col='start_day_name', title='daily Usage Patterns by Type & Member', x_label='Day of the Week')
)

"""____________
casual  
1. Usage gradually increases throughout the weekdays, and then slightly higher on weekends (Saturday being highest).
2.  Electric bikes are consistently more popular than classic bikes across weekdays, but they get almost equal in the weekends, which is indicates behavior like wanting to do excercise ,or because users are not in a rush to get to the location (since there is no work on these days )  .

members
1. Usage is highest during weekdays (Monday to Friday), indicating regular commuting patterns.
2. Usage drops significantly on weekends (Saturday and Sunday), which is the opposite of the casual.
3. Electric bikes are consistently and significantly more popular than classic

_______
"""

# for  is weekend
(
    trips_df
    .pipe(aggregate_trip_counts, group_col='is_weekend')
    .pipe(plot_static_trip_bar, x_col='is_weekend', title='Weekend vs Weekday Rides by Type & Member', x_label='Is Weekend (1=True, 0=False)')
)

"""____________
casual  
1. Electric bikes are more popular than classic bikes for both weekdays but they are almost equal in the weekend.


members
1. On weekends, the gap between electric and classis get narrowed , but the electric still on top
_______

**Final Answer :**

ÿ™ÿÆÿ™ŸÑŸÅ ÿ£ŸàŸÇÿßÿ™ ÿßŸÑÿ∞ÿ±Ÿàÿ© Ÿàÿ£ŸäÿßŸÖ ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿ®ÿ¥ŸÉŸÑ ŸÉÿ®Ÿäÿ± ÿ®ŸäŸÜ ŸÅÿ¶ÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖŸäŸÜ Ÿàÿ£ŸÜŸàÿßÿπ ÿßŸÑÿØÿ±ÿßÿ¨ÿßÿ™, Ÿäÿ∏Ÿáÿ± ÿßŸÑÿ£ÿπÿ∂ÿßÿ° ÿ£ŸÜŸÖÿßÿ∑ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÇŸàŸäÿ© ŸÅŸä ÿ£ŸäÿßŸÖ ÿßŸÑÿ£ÿ≥ÿ®Ÿàÿπ Ÿàÿ≥ÿßÿπÿßÿ™  ÿßŸÑÿ∞ÿ±Ÿàÿ©ÿå ŸàŸäŸÅÿ∂ŸÑŸàŸÜ ÿßŸÑÿØÿ±ÿßÿ¨ÿßÿ™ ÿßŸÑŸÉŸáÿ±ÿ®ÿßÿ¶Ÿäÿ© ÿ®ÿ¥ŸÉŸÑ ÿπÿßŸÖÿå ŸÅŸä ÿßŸÑŸÖŸÇÿßÿ®ŸÑÿå ŸäŸÖŸäŸÑ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖŸàŸÜ ÿßŸÑÿπÿßÿ®ÿ±ŸàŸÜ ÿ•ŸÑŸâ ÿ≤ŸäÿßÿØÿ© ÿßÿ≥ÿ™ÿÆÿØÿßŸÖŸáŸÖ ŸÜÿ≠Ÿà ŸÜŸáÿßŸäÿ© ÿßŸÑÿ£ÿ≥ÿ®Ÿàÿπ ŸàŸäŸÖŸäŸÑ ŸÜŸÖÿ∑ ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸâ ÿßŸÑÿ™ÿ≥ÿßŸàŸä ÿ®ŸäŸÜ ÿßŸÑŸÜŸàÿπŸäŸÜ.

---
ŸéQ2
---
 Are there distinct usage patterns for electric_bike vs. classic_bike during different hour_segment and start_day_name, and does this differ between member_casual types?
"""

# for hour segments
(
    trips_df
    .pipe(aggregate_trip_counts, group_col='hour_segment')
    .pipe(plot_static_trip_bar, x_col='hour_segment', title='Usage by Hour Segment and Ride Type', x_label='Hour Segment')
)

"""________
1. Electric Bike Dominant specially in night time

2.  members usage peakes at midday
3. during night time the number of trips is very low

**Final Answer :**<br>
The usage patterns clearly differentiate between electric_bike and classic_bike, especially when considering the hour_segment and member_casual user type. Electric bikes are the preferred choice at night, even though overall nighttime usage is low. Furthermore, members and casual users exhibit distinct peak usage times throughout the day, suggesting different motivations and use cases for the bike-sharing service

---
Q3
---
How do specific weather_segment or conditions (e.g., "Light Rain," "Clear") impact trip_duration_minutes and trip_cost for different rideable_type?
"""

def aggregate_weather_impact(df, group_col):
    return (
        df
        .groupby(['rideable_type', group_col])
        .agg(
            avg_duration=('trip_duration_minutes', 'mean'),
            median_duration=('trip_duration_minutes', 'median'),
            avg_cost=('trip_cost', 'mean'),
            median_cost=('trip_cost', 'median'),
            trip_count=('trip_duration_minutes', 'count')
        )
        .reset_index()
    )

def plot_avg_duration(df, x_col, title):
    fig = px.bar(
        df,
        x=x_col,
        y='avg_duration',
        color='rideable_type',
        barmode='group',
        title=title,
        labels={'avg_duration': 'Avg Trip Duration (min)', x_col: x_col}
    )
    fig.write_image(f"{title}.png")
    files.download(f"{title}.png")
    fig.update_layout(showlegend=True)
    fig.show(config={'staticPlot': True})

# average trip cost
def plot_avg_cost(df, x_col, title):
    fig = px.bar(
        df,
        x=x_col,
        y='avg_cost',
        color='rideable_type',
        barmode='group',
        title=title,
        labels={'avg_cost': 'Avg Trip Cost ($)', x_col: x_col}
    )

    fig.update_layout(showlegend=True)
    fig.write_image(f"{title}.png")
    files.download(f"{title}.png")
    fig.show(config={'staticPlot': True})
# Duration by weather segment
(
    trips_df
    .pipe(aggregate_weather_impact, group_col='weather_segment')
    .pipe(plot_avg_duration, x_col='weather_segment', title='Avg Trip Duration by Weather Segment and Ride Type')
)

# Cost by weather segment
(
    trips_df
    .pipe(aggregate_weather_impact, group_col='weather_segment')
    .pipe(plot_avg_cost, x_col='weather_segment', title='Avg Trip Cost by Weather Segment and Ride Type')
)

"""1.  Across all weather conditions (Cloudy, Rainy, Sunny), classic bikes consistently show a significantly higher average trip duration compared to electric bikes.
2. For classic bikes, the average duration is slightly higher on sunny and cloudy days (around 17-18 minutes), and decreases slightly on rainy days (just over 15 minutes), suggesting users might shorten their rides slightly in the rain.
3. For electric bikes, the average trip duration remains relatively stable and around 12-13 minutes across all weather conditions, indicating their use for quick and efficient trips regardless of the weather.

4. both bike types avg trip cost is stable in every condition , this incdicates general weather condition  has a very minimal, almost negligible, impact on the average trip cost for both types of bikes.
5. the trip cost for electric is more than classic

**Final Answer :**<br>
Weather conditions have a small impact on the average trip duration and average trip cost,  Classic bikes consistently record a longer average trip duration than electric bikes across all weather conditions, with a slight decrease on rainy days. In contrast, electric bikes are used for shorter average trips and have a consistently slightly higher average cost, but their duration and cost are not  affected by changes in general weather conditions. This suggests that the primary factors influencing trip duration and cost are related to the bike type itself rather than the overarching weather segments.

---
Q4
---
Which stations act as major transfer hubs (high start_station_name and end_station_name volume)? Do they primarily serve specific member_casual types or rideable_type?
"""

def get_top_transfer_stations(df, top_n=15):
    # Count start and end occurrences separately
    starts = df['start_station_name'].value_counts()
    ends = df['end_station_name'].value_counts()

    # Sum to get total transfer volume
    total_volume = starts.add(ends, fill_value=0).sort_values(ascending=False)

    top_stations = total_volume.head(top_n).index.tolist()
    return top_stations

def aggregate_station_transfer_profiles(df, station_list):
    filtered = df[
        df['start_station_name'].isin(station_list) |
        df['end_station_name'].isin(station_list)
    ].copy()

    # Normalize to one column: station_name
    filtered['station_name'] = filtered.apply(
        lambda row: row['start_station_name'] if row['start_station_name'] in station_list else row['end_station_name'],
        axis=1
    )

    # Group by station, member_casual, rideable_type
    return (
        filtered
        .groupby(['station_name', 'member_casual', 'rideable_type'])
        .size()
        .reset_index(name='ride_count')
    )

def plot_station_transfer_breakdown(df, title):
    fig = px.bar(
        df,
        x='station_name',
        y='ride_count',
        color='rideable_type',
        barmode='group',
        facet_col='member_casual',
        title=title,
        labels={
            'station_name': 'Station Name',
            'ride_count': 'Ride Count',
            'rideable_type': 'Bike Type',
            'member_casual': 'User Type'
        }
    )
    fig.update_layout(showlegend=True, xaxis_tickangle=-45)
    fig.write_image(f"{title}.png")
    files.download(f"{title}.png")

    fig.show(config={'staticPlot': True})
top_stations = get_top_transfer_stations(trips_df, top_n=15)

(
    trips_df
    .pipe(aggregate_station_transfer_profiles, station_list=top_stations)
    .pipe(plot_station_transfer_breakdown, title='Top 15 Transfer Hubs by User and Bike Type')
)

"""1. The plot clearly indicates that "Park Rd & Holmead Pl NW" is the most significant transfer hub
2. These major transfer hubs overwhelmingly serve member users
3. For both casual and member users, electric bikes  are consistently more utilized than classic bikes  across almost all of the top 15 transfer stations.

**Final answer** <br>
The station acting as the primary major transfer hub is "Park Rd & Holmead Pl NW", followed by stations such as "14th St & New York Ave NW," "17th & P St NW," and "14th & Belmont St NW". These hubs overwhelmingly serve member users, who exhibit significantly higher ride volumes than casual users at these locations. Furthermore, these transfer hubs primarily cater to the use of electric bikes, which are consistently and notably more popular than classic bikes for both member and casual riders at these high-volume stations.

---
Q5
---
Can we identify common "commute routes" or "regular paths" for  users based on frequent start_station_name and end_station_name pairs?
"""

def aggregate_top_routes_with_cbd(df, top_n=15):
    # Filter to classic/electric only
    filtered_df = df[df['rideable_type'].isin(['classic_bike', 'electric_bike'])].copy()

    # Create route string
    filtered_df['route'] = filtered_df['start_station_name'] + ' ‚Üí ' + filtered_df['end_station_name']

    # Group by user, route, and in_cbd
    route_counts = (
        filtered_df
        .groupby(['member_casual', 'route', 'in_cbd'])
        .size()
        .reset_index(name='ride_count')
    )

    # Get top N routes *per user*, regardless of CBD
    top_routes = (
        route_counts
        .groupby('member_casual', group_keys=False)
        .apply(lambda g: g.groupby('route')['ride_count'].sum().nlargest(top_n))
        .reset_index()
        .rename(columns={0: 'total_rides'})
    )

    # Merge back to get `in_cbd` split per route
    final_df = route_counts.merge(top_routes[['route']], on='route', how='inner')

    return final_df
def plot_top_routes_cbd(df, title):
    fig = px.bar(
        df,
        x='ride_count',
        y='route',
        color='in_cbd',
        facet_col='member_casual',
        orientation='h',
        title=title,
        labels={
            'route': 'Route (Start ‚Üí End)',
            'ride_count': 'Number of Rides',
            'in_cbd': 'Passed CBD',
            'member_casual': 'User Type'
        },
        color_discrete_map={0: '#636EFA', 1: '#EF553B'}
    )
    fig.update_layout(showlegend=True)
    fig.update_yaxes(categoryorder="total ascending")
    fig.write_image(f"{title}.png")
    files.download(f"{title}.png")
    fig.show(config={'staticPlot': True})
(
    trips_df
    .pipe(aggregate_top_routes_with_cbd, top_n=15)
    .pipe(plot_top_routes_cbd, title='Top 15 Routes by User Type and CBD Inclusion')
)

"""1. There are clearly a few routes that are significantly more popular than others,The self-loop routes (start and end at the same station) are very prominent.
2.  For many of the top routes, a significant portion of trips seem to involve the CBD

**Final answer**<br>
Yes, we can identify common "regular paths" for users based on frequent start_station_name and end_station_name pairs. The analysis of the "Top 15 Routes" plot reveals distinct popular paths. While several prominent routes are self-looping , a considerable portion of these top routes also show involvement with the Central Business District (in_cbd). This in_cbd presence suggests that many of these frequent paths, likely serve as actual "commute routes" or regular travel paths for users, especially those traveling to, from, or through the city's commercial core.

# Even deeper analysis
"""

# Step 1: Prepare weekly aggregated DataFrame
weekly_stats = (
    trips_df
    .assign(date=pd.to_datetime(trips_df['date']))
    .groupby(pd.Grouper(key='date', freq='W-MON'))  # Weekly data, starting Mondays
    .agg(
        total_rides=('ride_id', 'count'),
        total_revenue=('trip_cost', 'sum')
    )
    .reset_index()
    .sort_values('date')
    .set_index('date')
)

# Step 2: Apply STL decomposition to total_rides
stl_result = STL(weekly_stats['total_rides'], period=52).fit()  # period=52 for yearly seasonality in weekly data

# Step 3: Add residuals and z-scores to the DataFrame
weekly_stats = (
    weekly_stats
    .assign(
        trend=stl_result.trend,
        seasonal=stl_result.seasonal,
        resid=stl_result.resid,
        resid_z=zscore(stl_result.resid)
    )
)

# Step 4: Detect anomalies (z-score > 2)
anomalies = weekly_stats[weekly_stats['resid_z'].abs() > 2]

# Step 5: Plot total rides and anomalies
fig = go.Figure()
fig.add_trace(go.Scatter(x=weekly_stats.index, y=weekly_stats['total_rides'], name='Total Weekly Rides'))
fig.add_trace(go.Scatter(
    x=anomalies.index, y=anomalies['total_rides'],
    mode='markers', name='Anomalies',
    marker=dict(color='red', size=8)
))
fig.update_layout(title='Weekly Total Rides with Anomalies', xaxis_title='Week', yaxis_title='Total Rides')
fig.show(config={'staticPlot': True})

anomalous_week_start_date = anomalies.index[0]

print(f"Exploring the anomalous week starting: {anomalous_week_start_date.strftime('%Y-%m-%d')}")

anomalous_week_end_date = anomalous_week_start_date + pd.Timedelta(days=6)
print(f"Week ends on: {anomalous_week_end_date.strftime('%Y-%m-%d')}\n")
trips_df['date'] = pd.to_datetime(trips_df['date'])
trips_of_anomalous_week = trips_df[
    (trips_df['date'] >= anomalous_week_start_date) &
    (trips_df['date'] <= anomalous_week_end_date)
].copy()

print(f"Total trips in the anomalous week: {len(trips_of_anomalous_week)}")
print(f"Total revenue in the anomalous week: ${trips_of_anomalous_week['trip_cost'].sum():.2f}\n")


print("--- Breakdown by Member/Casual Type (Anomalous Week) ---")
member_casual_breakdown = trips_of_anomalous_week.groupby('member_casual').agg(
    ride_count=('ride_id', 'count'),
    total_revenue=('trip_cost', 'sum'),
    avg_trip_cost=('trip_cost', 'mean'),
    avg_trip_duration_minutes=('trip_duration_minutes', 'mean')
).round(2)
print(member_casual_breakdown)
print("\n")

print("--- Breakdown by Rideable Type (Anomalous Week) ---")
rideable_type_breakdown = trips_of_anomalous_week.groupby('rideable_type').agg(
    ride_count=('ride_id', 'count'),
    total_revenue=('trip_cost', 'sum'),
    avg_trip_cost=('trip_cost', 'mean'),
    avg_trip_duration_minutes=('trip_duration_minutes', 'mean')
).round(2)
print(rideable_type_breakdown)
print("\n")

# --- Check for 'weather_segment' column before breaking down by it ---
if 'weather_segment' in trips_of_anomalous_week.columns:
    print("--- Breakdown by Weather Segment (Anomalous Week) ---")
    weather_breakdown = trips_of_anomalous_week.groupby('weather_segment').agg(
        ride_count=('ride_id', 'count'),
        total_revenue=('trip_cost', 'sum'),
        avg_trip_cost=('trip_cost', 'mean'),
        avg_trip_duration_minutes=('trip_duration_minutes', 'mean')
    ).round(2)
    print(weather_breakdown)
    print("\n")
else:
    print("Weather segment data not available for direct breakdown in trips_df for this week.\n")


print("--- Breakdown by Day of Week (Anomalous Week) ---")
day_of_week_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
day_of_week_breakdown = trips_of_anomalous_week['start_day_name'].value_counts().reindex(day_of_week_order).fillna(0).astype(int)
print(day_of_week_breakdown)
print("\n")

print("--- Breakdown by Hour Segment (Anomalous Week) ---")
hour_segment_order = ['Morning', 'Midday', 'Evening', 'Night']
hour_segment_breakdown = trips_of_anomalous_week['hour_segment'].value_counts().reindex(hour_segment_order).fillna(0).astype(int)
print(hour_segment_breakdown)
print("\n")

print("--- Top 5 Start Stations in Anomalous Week ---")
print(trips_of_anomalous_week['start_station_name'].value_counts().head(5))
print("\n")

print("--- Top 5 End Stations in Anomalous Week ---")
print(trips_of_anomalous_week['end_station_name'].value_counts().head(5))
print("\n")

"""When a point in a time series, like your "Total Weekly Rides" or "Total Weekly Revenue," is detected as an anomaly (a red dot on your plot), it means that particular data point deviates significantly from the pattern that the time series model (like STL decomposition) expects, given its learned trend and seasonality.

Even if a point doesn't look dramatically different to the naked eye on the raw plot, the statistical model has identified it as an outlier after factoring in the usual ebb and flow of the data.

 general categories of problems or situations that could lead to such a point being flagged as an anomaly in a bike-sharing dataset:
<br>
1. Data Quality Issues
2. External Events and Unforeseen Circumstances
3. Sudden System or Operational Changes
4. Changes in User Behavior/Trends:

---
Extra feature engineering
---
"""

trips_df = trips_df.assign(
    is_round_trip=lambda df: df['start_station_id'] == df['end_station_id']
)

trips_df['is_round_trip'].value_counts()

"""---
Seasons
---


"""

def plot_seasonal_bar(data, y_col, title):
    fig = px.bar(
        data,
        x='season',
        y=y_col,
        color='season',
        title=title,
        text=y_col
    )
    fig.update_traces(texttemplate='%{text:,}', textposition='outside')
    fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')
    fig.show(config={'staticPlot': True})

season_map = {
    12: 'Winter', 1: 'Winter', 2: 'Winter',
    3: 'Spring', 4: 'Spring', 5: 'Spring',
    6: 'Summer', 7: 'Summer', 8: 'Summer',
    9: 'Autumn', 10: 'Autumn', 11: 'Autumn'
}

seasonal_stats = (
    trips_df
    .assign(season=lambda df: df['start_month'].map(season_map))
    .groupby('season')
    .agg(
        total_trips=('ride_id', 'count'),
        total_revenue=('trip_cost', 'sum')
    )
    .reset_index()
    .assign(season=lambda df: pd.Categorical(df['season'], categories=['Winter', 'Spring', 'Summer', 'Autumn'], ordered=True))
    .sort_values('season')
)

plot_seasonal_bar(seasonal_stats, 'total_trips', 'Total Trips per Season (2024)')

plot_seasonal_bar(seasonal_stats, 'total_revenue', 'Total Revenue per Season (2024)')

season_map = {
    12: 'Winter', 1: 'Winter', 2: 'Winter',
    3: 'Spring', 4: 'Spring', 5: 'Spring',
    6: 'Summer', 7: 'Summer', 8: 'Summer',
    9: 'Autumn', 10: 'Autumn', 11: 'Autumn'
}

seasonal_time_series = (
    trips_df
    .assign(
        date=pd.to_datetime(trips_df['date']),
        week_start=lambda df: df['date'] - pd.to_timedelta(df['date'].dt.weekday, unit='D'),
        season=lambda df: df['date'].dt.month.map(season_map)
    )
    .query("date.dt.year == 2024")
    .groupby(['week_start'])
    .agg(
        total_trips=('ride_id', 'count'),
        season=('season', 'first')
    )
    .reset_index()
    .assign(
        season=lambda df: pd.Categorical(df['season'], categories=['Winter', 'Spring', 'Summer', 'Autumn'], ordered=True)
    )
)

season_changes = seasonal_time_series[
    seasonal_time_series['season'] != seasonal_time_series['season'].shift(1)
]

fig = go.Figure()

fig.add_trace(go.Scatter(
    x=seasonal_time_series['week_start'],
    y=seasonal_time_series['total_trips'],
    mode='lines',
    name='Total Trips',
    line=dict(color='gray'),
    showlegend=False
))

for season_name, color in zip(['Winter', 'Spring', 'Summer', 'Autumn'], ['blue', 'green', 'orange', 'brown']):
    season_points = season_changes[season_changes['season'] == season_name]
    fig.add_trace(go.Scatter(
        x=season_points['week_start'],
        y=season_points['total_trips'],
        mode='markers+text',
        name=season_name,
        text=season_name,
        textposition='top center',
        marker=dict(size=10, color=color),
    ))

fig.update_layout(
    title='Total Weekly Trips in 2024 (Colored by Season Start)',
    xaxis_title='Week',
    yaxis_title='Total Trips',
    showlegend=True
)

fig.show(config={'staticPlot': True})

"""insights:
1. Autumn generated the highest revenue and number of trips, followed by Summer, then Spring, and Winter had the lowest revenue.
2. at the end of autumn (november) the number of trips drop significantly
3. all this support the idea we got from the EDA that seasons have huge impact on revenue and number of trips

---
Feature selection
---
"""

trips_df['date'] = pd.to_datetime(trips_df['date'])

trips_df['season'] = trips_df['date'].dt.month.map(season_map)

trips_df['season'] = pd.Categorical(trips_df['season'], categories=['Winter', 'Spring', 'Summer', 'Autumn'], ordered=True)

trips_df['started_at'] = pd.to_datetime(trips_df['started_at'], errors='coerce', infer_datetime_format=True)
trips_df['date'] = pd.to_datetime(trips_df['started_at'], errors='coerce')

trips_df = trips_df.dropna(subset=['date'])

trips_df['week_start_date'] = trips_df['date'] - pd.to_timedelta(trips_df['date'].dt.weekday, unit='d')

trips_df['week_start_date'] = pd.to_datetime(trips_df['week_start_date']) # Ensure it's datetime
trips_df['week_start_date'] = trips_df['date'] - pd.to_timedelta(trips_df['date'].dt.weekday, unit='d')
trips_df['week_start_date'] = trips_df['week_start_date'].dt.normalize()

numerical_features_to_avg = [
    'trip_duration_minutes', 'trip_cost', 'temp', 'humidity', 'windspeedmean',
    'trip_distance_km', 'trip_avg_speed_kmh', 'distance_to_metro_connection'
]

boolean_features_to_sum_as_count_or_prop = [
    'is_round_trip', 'is_adverse_weather_trip', 'is_peak_season',
    'is_cbd_trip', 'is_long_duration_trip', 'is_weekend', 'rush_hour',
    'start_in_cbd', 'end_in_cbd', 'is_far_from_metro_start', 'is_far_from_metro_end',
    'is_far_from_shuttle_start', 'is_far_from_shuttle_end', 'is_far_from_any_transit'
]

categorical_features_to_mode = [
    'season', 'rideable_type', 'member_casual', 'conditions',
    'start_day_name', 'hour_segment'
]

for col in numerical_features_to_avg + ['trip_cost']:
    if col in trips_df.columns and not pd.api.types.is_numeric_dtype(trips_df[col]):
        trips_df[col] = pd.to_numeric(trips_df[col], errors='coerce').fillna(0)

for col in boolean_features_to_sum_as_count_or_prop:
    if col in trips_df.columns and not pd.api.types.is_numeric_dtype(trips_df[col]):
        trips_df[col] = trips_df[col].astype(int)


weekly_data = trips_df.groupby('week_start_date').agg(
    total_trips=('ride_id', 'count'),
    total_revenue=('trip_cost', 'sum'),

    **{f'avg_{col}': (col, 'mean') for col in numerical_features_to_avg if col in trips_df.columns},
    **{f'total_{col}': (col, 'sum') for col in boolean_features_to_sum_as_count_or_prop if col in trips_df.columns},

    prop_electric_bike=('rideable_type', lambda x: (x == 'electric_bike').sum() / len(x) if len(x) > 0 else 0),
    prop_classic_bike=('rideable_type', lambda x: (x == 'classic_bike').sum() / len(x) if len(x) > 0 else 0),
    prop_member=('member_casual', lambda x: (x == 'member').sum() / len(x) if len(x) > 0 else 0),
    prop_casual=('member_casual', lambda x: (x == 'casual').sum() / len(x) if len(x) > 0 else 0),

    weekly_season=('season', lambda x: x.mode()[0] if not x.empty else pd.NA), # For weekly seasonality
    weekly_conditions=('conditions', lambda x: x.mode()[0] if not x.empty else pd.NA),
).reset_index()

weekly_data = weekly_data[weekly_data['total_trips'] > 0]

print("Weekly Aggregated Data Head:")
print(weekly_data.head())
print("\nWeekly Aggregated Data Info:")
print(weekly_data.info())


X = weekly_data.drop(columns=['week_start_date', 'total_trips', 'total_revenue'])
y = weekly_data['total_trips']

categorical_cols = X.select_dtypes(include='category').columns.tolist() + X.select_dtypes(include='object').columns.tolist()
for col in categorical_cols:
    X[col] = X[col].astype(str)
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

X = X.fillna(0)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

feature_importances = pd.Series(model.feature_importances_, index=X.columns)



print("\nTop 20 Features by Importance:")
print(feature_importances.nlargest(20))

"""our task was to take our incredibly detailed, trip-by-trip bike data think of it as millions of tiny, individual actions ‚Äì and make sense of it on a grander scale. We wanted to understand the weekly heartbeat of our bike-sharing system.<br>
To do this, we performed a critical maneuver: **Feature Selection**. We aggregated all that granular information into a weekly summary. This means instead of looking at every single trip, we created new, powerful features for each week. We calculated things like:

- Total numbers: How many trips happened? What was the total revenue? How many rush hour trips? How many weekend trips?
- Weekly averages: What was the average trip duration, or the average temperature for that week?
- Proportions: What percentage of bikes used were electric versus classic? What was the split between members and casual riders?
- Dominant characteristics: What was the most common weather condition for the week, or which season were we in?
<br>
Once we had this beautiful, aggregated weekly dataset, we prepped it for our analytical engine. We clearly defined our target ‚Äì total_trips for the week ‚Äì and made sure all our categorical data was ready by using One-Hot Encoding.

Finally, and this is where the real magic happened, we used a powerful tool: the Random Forest Regressor. We trained it on our weekly data, and what it gave us back was invaluable: a feature importance score for every single weekly metric we created. This score tells us, definitively, which factors are the most impactful in predicting how many total trips we'll see in any given week.
_______________
When we looked at those feature importance scores, a few clear patterns emerged, revealing the true drivers of our bike-sharing demand:<br>
* Rush Hour and Weekends Reign Supreme! This is perhaps our biggest finding. The total number of trips happening during rush hour and the total trips over the weekend are, by far, the most critical factors determining our weekly trip volume. If we want to predict how busy a week will be, these are the first things we should be looking at. This tells us our service is heavily used for both daily commuting and leisure activities.
*  Trips starting or ending in the Central Business District are incredibly important predictors. This confirms that the CBD isnt just a popular spot; its a major activity hub that significantly influences our overall ridership.
* Among all the weather variables, the average weekly temperature stands out as the most impactful. This suggests that while we track humidity or wind, its really how warm or cold it is that most directly influences people's decision to hop on a bike.
*  Interestingly, the number of trips that are far from shuttle connections (both starting and ending) showed significant importance. This suggests that in areas where other transit options like shuttles are less accessible, our bike-sharing service becomes a vital transportation alternative.
* Bike Type and Membership are Contextual: While we care about the mix of electric vs. classic bikes and members vs. casuals, the proportions of these groups weren't the top drivers of total trip volume in this model. This means that while they define who is riding and what they're riding, the overall quantity of rides is more fundamentally driven by the 'when' (rush hour, weekend) and 'where' (CBD, transit gaps) factors, along with the weather.

---
**PCA**
---
"""

pca_features = [
    'avg_trip_duration_minutes', 'avg_trip_cost', 'avg_temp', 'avg_humidity', 'avg_windspeedmean',
    'total_is_round_trip', 'total_is_weekend', 'total_rush_hour',
    'total_start_in_cbd', 'total_end_in_cbd',
    'total_is_far_from_metro_start', 'total_is_far_from_metro_end',
    'total_is_far_from_shuttle_start', 'total_is_far_from_shuttle_end',
    'total_is_far_from_any_transit',
    'prop_electric_bike', 'prop_classic_bike', 'prop_member', 'prop_casual'
]

pca_features = [f for f in pca_features if f in weekly_data.columns]


X_pca = weekly_data[pca_features]

if X_pca.isnull().sum().sum() > 0:
    print("Warning: NaNs found in PCA features. Filling with 0 or mean/median is recommended.")
    X_pca = X_pca.fillna(X_pca.mean()) # A common strategy, replace with median if skewed

print(f"Features selected for PCA: {len(pca_features)}")
print(X_pca.head())

scaler = StandardScaler()

X_scaled = scaler.fit_transform(X_pca)

X_scaled_df = pd.DataFrame(X_scaled, columns=pca_features, index=weekly_data.index)
print("\nScaled Features Head:")
print(X_scaled_df.head())

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA()
pca.fit(X_scaled)
explained_variance = pca.explained_variance_ratio_
cumulative_variance = explained_variance.cumsum()

fig1 = go.Figure()
fig1.add_trace(go.Scatter(
    x=np.arange(1, len(explained_variance) + 1),
    y=explained_variance,
    mode='lines+markers',
    name='Explained Variance Ratio'
))
fig1.update_layout(
    title='Explained Variance Ratio per Principal Component',
    xaxis_title='Number of Components',
    yaxis_title='Explained Variance Ratio',
)
fig1.show(config={'staticPlot': True})

threshold_line = 0.95
n_components_95 = (cumulative_variance >= threshold_line).argmax() + 1

fig2 = go.Figure()
fig2.add_trace(go.Scatter(
    x=np.arange(1, len(cumulative_variance) + 1),
    y=cumulative_variance,
    mode='lines+markers',
    name='Cumulative Explained Variance'
))
fig2.add_hline(y=threshold_line, line_dash="dot", line_color="red", annotation_text="95% Variance")
fig2.add_vline(x=n_components_95, line_dash="dot", line_color="green", annotation_text=f"{n_components_95} Components")
fig2.update_layout(
    title='Cumulative Explained Variance Ratio',
    xaxis_title='Number of Components',
    yaxis_title='Cumulative Explained Variance',
)
fig2.show(config={'staticPlot': True})

chosen_n_components = max(1, n_components_95)
pca_final = PCA(n_components=chosen_n_components)
X_pca_transformed = pca_final.fit_transform(X_scaled)

pca_df = pd.DataFrame(X_pca_transformed, columns=[f'PC_{i+1}' for i in range(chosen_n_components)])
pca_df['week_start_date'] = weekly_data['week_start_date'].reset_index(drop=True)
pca_df['total_trips'] = weekly_data['total_trips'].reset_index(drop=True)

print(f"\n‚úÖ Transformed data with {chosen_n_components} Principal Components:")
print(pca_df.head())

""" **"Compressing Our Data for Maximum Impact: The PCA Story"**

So, you remember how we engineered all those fantastic weekly features? We ended up with 19 distinct numerical metrics ‚Äì things like average temperature, total rush hour trips, proportions of electric bikes, and so on. That's a lot of variables, and while they're all important, sometimes too many highly related variables can make our models work harder than they need to, and even make them less stable.

That's where **Principal Component Analysis, or PCA**

Here's how we did it:

First, we took those 19 numerical features and **standardized them**. This is crucial because it ensures that features with larger numerical ranges don't unfairly dominate our analysis. Everyone gets to play on a level field.

Then, we ran PCA. It identified new, 'synthetic' variables ‚Äì we call them **Principal Components**. Each component is a unique blend of our original features, designed to capture as much of the underlying 'information' or variation in the data as possible, in order of importance.

What we saw on our plots was incredibly telling:

* The first few components explained a huge chunk of the total variation right off the bat, especially the very first one!
* When we looked at the cumulative variance plot, we found something exciting: we could capture a staggering **95% of all the valuable information** from our original 19 features using **just 9 Principal Components!**

This means we‚Äôve effectively distilled our complex, 19-dimensional weekly data into a much more compact, 9-dimensional representation, losing only 5% of the original variance ‚Äì which often consists of noise or less relevant details anyway.

---

 **"What This Means for Us: Leaner, Meaner Analysis"**

So, what's the big takeaway from this PCA work? It‚Äôs all about **efficiency and focus**.

1.  **Simpler, Faster Models:** By reducing our feature set from 19 to 9, we're giving our future predictive models (like Prophet, when we use these as external regressors) a much leaner input. This will lead to faster training times and potentially more stable predictions because the model isn't trying to untangle subtle redundancies between features.

2.  **Information Preservation:** We achieved this massive reduction without sacrificing much of the essential information. The 9 principal components are like concentrated essence ‚Äì they capture nearly all the important patterns and variations that were present in the original 19.

3.  **Future-Proofing:** These new principal components are also inherently uncorrelated with each other. This is a huge advantage, as many statistical and machine learning techniques perform better when input variables don't move exactly in sync.

In short,  We're now working with a more concise and powerful set of features, enabling us to move forward with our time series analysis and other tasks more effectively and with greater confidence in our results. It‚Äôs a significant step towards understanding the core dynamics of our bike share system.
"""

loadings = pd.DataFrame(
    pca_final.components_.T,
    columns=[f'PC_{i+1}' for i in range(chosen_n_components)],
    index=X.columns
)

print("\nPrincipal Component Loadings:")
print(loadings)

if chosen_n_components >= 2:
    loadings_melted = loadings.iloc[:, :2].reset_index().melt(id_vars='index', var_name='Principal Component', value_name='Loading')
    fig = px.imshow(
        loadings.iloc[:, :2].T,
        labels=dict(x="Feature", y="Principal Component", color="Loading"),
        x=loadings.index,
        y=[f'PC_{i+1}' for i in range(2)],
        color_continuous_scale='viridis'
    )
    fig.update_layout(
        title='Feature Loadings on Principal Components 1 and 2',
        xaxis_title='Original Feature',
        yaxis_title='Principal Component',
        width=1000,
        height=400
    )
    fig.show(config={'staticPlot': True})

print("\nInterpretation of Principal Components:")
for i in range(chosen_n_components):
    pc_name = f'PC_{i+1}'
    print(f"\n--- {pc_name} ---")
    top_features = loadings[pc_name].abs().nlargest(5).index.tolist()
    print(f"Top contributing features (by absolute loading): {top_features}")
    print(loadings[pc_name][top_features])

"""**Here's a breakdown of what the most influential components signify:**

* **PC1: "The Pulse of Urban & Transit-Independent Activity"**
    * This is our strongest component, capturing the largest chunk of variance. It tells us about weeks with **very high overall bike activity, especially concentrated within the Central Business District (CBD)**. Critically, it also heavily loads on trips that are **far from traditional transit options like metro or shuttle services**. This suggests that when public transport might be less convenient or available, our bikes step in as a vital solution, particularly for high-volume periods like rush hour and weekends. Think of it as a combined measure of our system's core utility for dense urban commuting and leisure, especially where alternative transit is sparse.

* **PC2: "The Classic vs. Electric, Season & Trip Purpose Divide"**
    * This component highlights a fascinating contrast. On one end, it's strongly associated with weeks seeing a **higher proportion of classic bikes, occurring in Spring, and featuring longer average trip durations**. On the other end (negative loadings), it correlates with more **electric bike usage and a higher number of round trips**. This indicates two distinct modes of operation: are people opting for traditional bikes for longer, possibly more exploratory rides in pleasant spring weather, or are they grabbing electric bikes for quick, convenient, often round-trip commutes? This component helps us distinguish between these two fundamental usage patterns.

* **PC3: "The Influence of Humidity and Rain on Riding Conditions"**
    * This component is our direct window into how **humid and rainy weather impacts weekly ridership**. Weeks scoring high on PC3 are characterized by elevated humidity and various forms of rain (partially cloudy with rain, overcast with rain), often coupled with lower wind speeds. This gives us a consolidated measure of less favorable, but not necessarily severe, riding conditions, allowing us to quantify their collective effect.

* **PC4: "Summer's Warmth, Costs, and Shifting Ridership"**
    * PC4 primarily captures the essence of the **Summer season, bringing with it warmer temperatures**. What's intriguing is its inverse relationship with average trip cost. This might suggest that during peak summer, while ridership is high due to good weather, the average cost per trip might be lower, perhaps due to more short, spontaneous rides or promotional pricing during that time. It contrasts with spring patterns, highlighting how overall conditions shift between seasons.

**And Beyond...**

The remaining components (PC5 through PC9) continue to unpack the nuanced interplays within our data. They reveal more specific combinations of weather conditions (like purely overcast vs. various rain types), subtle seasonal shifts, and how these might interact with average trip duration, cost, or even the mix of member vs. casual riders.

**In essence, PCA isn't just about reducing numbers; it's about uncovering the fundamental, uncorrelated dimensions that truly explain the variations in our bike share system's weekly performance. We now have a clearer, more efficient map of our operational landscape.**

1.  We applied PCA to simplify our complex weekly bike data.
2.  It reduced our 19 numerical features to just 9 key Principal Components.
3.  Crucially, these 9 components retain 95% of the original information.
4.  This reveals the core, uncorrelated patterns driving weekly bike usage.
5.  Each component represents a distinct theme, like 'Urban Activity' or 'Weather Influence'.
6.  The result is a more efficient and powerful dataset for our predictive models.

# Conclusion:
This project systematically transformed raw bike-sharing data into actionable insights through a multi-stage analysis. Initial data exploration revealed core usage patterns and seasonal trends. Advanced feature engineering then significantly enriched the dataset with crucial temporal, weather, and geographical attributes.

Feature selection successfully identified key demand drivers, highlighting the critical influence of rush hour, CBD activity, and casual rider behavior. Subsequently, Principal Component Analysis (PCA) effectively reduced the dimensionality of our aggregated features from 19 to 6, retaining 95% of the data's variance. This comprehensive data preparation provides a robust, interpretable foundation, ideal for developing predictive models and informing strategic operational decisions for the bike-sharing service.
"""

