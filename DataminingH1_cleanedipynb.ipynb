{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF3CEiZb4GtS"
      },
      "source": [
        "# **Loading libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xs-YTsgO8QA"
      },
      "outputs": [],
      "source": [
        "%pip install gdown\n",
        "%pip install tqdm scikit-learn\n",
        "%pip install geopandas\n",
        "%pip install geohash2\n",
        "%pip install folium\n",
        "%pip install python-geohash\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import gdown\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "from google.colab import drive\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "from tqdm import tqdm\n",
        "import geohash2\n",
        "from sklearn.cluster import KMeans\n",
        "import json\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "from scipy.stats import chi2_contingency\n",
        "import geohash as gh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I05pgLlnO8QA"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTfRsiq6CQa"
      },
      "source": [
        "# **Loading the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9lConBSO8QB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "downloading the dataset\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkCADdlJzQGc"
      },
      "outputs": [],
      "source": [
        "folder_id = '1O3w5OKnS__hzlL8kTSfGCUc_iX8XNjEN'\n",
        "output_dir = 'Homework'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "print(f\"Attempting to download content from folder ID: {folder_id} into {output_dir}\")\n",
        "try:\n",
        "    gdown.download_folder(id=folder_id, output=output_dir, quiet=False, use_cookies=False)\n",
        "    print(f\"\\nSuccessfully downloaded content to: /content/{output_dir}\")\n",
        "    print(\"You can now find the downloaded content in the 'downloaded_external_folder' directory in your Colab files browser.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during download: {e}\")\n",
        "    print(\"Please ensure the Google Drive folder is publicly accessible or shared with 'Anyone with the link can view'.\")\n",
        "\n",
        "stations_info=pd.read_csv(\"Homework/data/Capital_Bikeshare_Locations.csv\")\n",
        "#\n",
        "# Load tabular data\n",
        "weather_df = pd.read_csv(\"Homework/data/Washington,DC,USA 2024-01-01 to 2024-12-31.csv\")\n",
        "trips_df = pd.read_parquet('Homework/data/daily-rent.parquet')\n",
        "\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "\n",
        "stations_df = pd.read_csv(\"Homework/data/Capital_Bikeshare_Locations.csv\")\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "Shuttle_Bus_Stops=pd.read_csv(\"Homework/data/Shuttle_Bus_Stops.csv\")\n",
        "Metro_Bus_Stops =pd.read_csv(\"Homework/data/Metro_Bus_Stops.csv\")\n",
        "#Loading Residential and Visitor Parking Zones\n",
        "Residential_Visitor_Parking_Zones  = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjq-QmE5b9K"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Downloading the combined and modified dataset (for ease of use )\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI0R9q-L5l8o"
      },
      "outputs": [],
      "source": [
        "file_id = \"114g7JYuZ00i864przAIJQYymib_5h6Qa\"  # Replace with your actual file ID\n",
        "output_file = \"trips_df.csv\"  # You can change the output file name\n",
        "\n",
        "gdown.download(id=file_id, output=output_file, quiet=False)\n",
        "trips_df = pd.read_csv(output_file)\n",
        "print(f\"File downloaded to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL1oTCjr6OFa"
      },
      "source": [
        "# **Preprocessing , Cleaning & inspecting the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gt_DuEv4w9P"
      },
      "source": [
        "\n",
        "There is a problem with missing start/id , almost 20% of the data are nulls so we must find a way to fill these up\n",
        "\n",
        "**spatial join**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "using lang and lati we can match it to the nearest station and then assign this id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h20Hhlhg1IFk"
      },
      "outputs": [],
      "source": [
        "trips_df = trips_df.dropna(subset=['end_lat', 'end_lng'])\n",
        "\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw3RvCyR1T9h"
      },
      "outputs": [],
      "source": [
        "# EPSG:4326 = lat/lon\n",
        "trips_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "stations_gdf = gpd.GeoDataFrame(\n",
        "    stations_df,\n",
        "    geometry=gpd.points_from_xy(stations_df['LONGITUDE'], stations_df['LATITUDE']),\n",
        "    crs='EPSG:4326'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA47hcEu1ldC"
      },
      "outputs": [],
      "source": [
        "# Find nearest station to each ride\n",
        "trips_with_nearest_station = gpd.sjoin_nearest(\n",
        "    trips_gdf, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"distance\"\n",
        ")\n",
        "\n",
        "# Now we fill missing station_id with nearest one\n",
        "trips_df['start_station_id'] = trips_df['start_station_id'].fillna(\n",
        "    trips_with_nearest_station['STATION_ID']\n",
        ")\n",
        "# Creating a mapping from STATION_ID to STATION_NAME\n",
        "id_to_name = stations_df.set_index('STATION_ID')['NAME'].to_dict()\n",
        "\n",
        "# Fill in missing start_station_name using start_station_id\n",
        "trips_df['start_station_name'] = trips_df['start_station_name'].fillna(\n",
        "    trips_df['start_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8isYZzA4yfR"
      },
      "source": [
        "Repeating the process to end id and name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIcXJiy95mU9"
      },
      "outputs": [],
      "source": [
        "trips_gdf_end = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "trips_with_nearest_end_station = gpd.sjoin_nearest(\n",
        "    trips_gdf_end, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"end_distance\"\n",
        ")\n",
        "\n",
        "trips_df['end_station_id'] = trips_df['end_station_id'].fillna(\n",
        "    trips_with_nearest_end_station['STATION_ID']\n",
        ")\n",
        "trips_df['end_station_name'] = trips_df['end_station_name'].fillna(\n",
        "    trips_df['end_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df=trips_df.drop_duplicates()\n",
        "trips_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkda9AV_6sc4"
      },
      "source": [
        "we will continue inspecting the rest of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo2_a0EG6qbm"
      },
      "outputs": [],
      "source": [
        "stations_df=stations_df.drop_duplicates()\n",
        "stations_df.isna().sum()  # we dont need to drop null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4BwUW136uQH"
      },
      "outputs": [],
      "source": [
        "weather_df=weather_df.drop_duplicates()\n",
        "weather_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQGps8Rt62eU"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrMSX_Av6vdY"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhqxpEU267b0"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf = parking_zones_gdf.drop(columns=['CREATOR', 'CREATED','EDITOR','EDITED'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_WPYmtO8iLt"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Laqgf-vPBIl"
      },
      "source": [
        "---\n",
        "**The outside WDC problem :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgmtRPGpPKqE"
      },
      "outputs": [],
      "source": [
        "# The bounding box method\n",
        "DC_LAT_MIN = 38.7916\n",
        "DC_LAT_MAX = 38.9955\n",
        "DC_LNG_MIN = -77.1198\n",
        "DC_LNG_MAX = -76.9094\n",
        "# Filtering  Points Outside the Bounding Box\n",
        "out_of_bounds_start = ~(\n",
        "    (trips_df['start_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &\n",
        "    (trips_df['start_lng'].between(DC_LNG_MIN, DC_LNG_MAX))\n",
        ")\n",
        "\n",
        "out_of_bounds_end = ~(\n",
        "    (trips_df['end_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &\n",
        "    (trips_df['end_lng'].between(DC_LNG_MIN, DC_LNG_MAX))\n",
        ")\n",
        "\n",
        "# Combine both to detect any trip with at least one bad coordinate\n",
        "outlier_mask = out_of_bounds_start | out_of_bounds_end\n",
        "outliers = trips_df[outlier_mask]\n",
        "\n",
        "# Inspect the Outliers\n",
        "print(f\"Number of outlier trips: {len(outliers)}\")\n",
        "outliers[['ride_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng']].head()\n",
        "# Total number of trips in the dataset\n",
        "total_trips = len(trips_df)\n",
        "\n",
        "# Number of outliers detected\n",
        "num_outliers = len(outliers)\n",
        "\n",
        "# Calculate the percentage of outliers\n",
        "percentage_outliers = (num_outliers / total_trips) * 100\n",
        "\n",
        "print(f\"Percentage of outlier trips: {percentage_outliers:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6sn_C7ASFUj"
      },
      "outputs": [],
      "source": [
        "# we will drop them\n",
        "trips_df = trips_df[~outlier_mask].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcTb9jx8S9KT"
      },
      "outputs": [],
      "source": [
        "# checking ride_id\n",
        "print(\"Duplicate ride_ids:\", trips_df['ride_id'].duplicated().sum())\n",
        "print(\"Missing ride_ids:\", trips_df['ride_id'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIRvlXxGVCng"
      },
      "source": [
        "*there an issue with dublicated ride_id so we will only keep the first occurrence*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-Dv6GvgUzWk"
      },
      "outputs": [],
      "source": [
        "# Keep first occurrence or drop based on your context:\n",
        "trips_df = trips_df.drop_duplicates(subset='ride_id', keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4RYU-8_TK-i"
      },
      "outputs": [],
      "source": [
        "print(\"Null times:\", trips_df[['started_at', 'ended_at']].isna().sum())\n",
        "print(\"Negative durations:\", (trips_df['ended_at'] < trips_df['started_at']).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY510PKnTOV9"
      },
      "outputs": [],
      "source": [
        "print(\"Missing start station:\", trips_df['start_station_id'].isna().sum())\n",
        "print(\"Missing end station:\", trips_df['end_station_id'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E87_lOcUTZ5P"
      },
      "outputs": [],
      "source": [
        "zero_coords = trips_df[(trips_df['start_lat'] == 0) | (trips_df['start_lng'] == 0)]\n",
        "print(\"Zero coordinates:\", len(zero_coords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcjBcf4NTcqH"
      },
      "outputs": [],
      "source": [
        "# checkign if rideable_type and member_casual has weird values\n",
        "print(\"Ride types:\", trips_df['rideable_type'].unique())\n",
        "print(\"Member types:\", trips_df['member_casual'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhYVVGgM9mwY"
      },
      "outputs": [],
      "source": [
        "weather_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHgrh-3o-F1Y"
      },
      "outputs": [],
      "source": [
        "# first we make sure all the dates are in the same format (by checking the length)\n",
        "datetime_lengths = weather_df[\"datetime\"].astype(str).apply(len)\n",
        "print(datetime_lengths.value_counts())\n",
        "weather_df[\"date\"] = pd.to_datetime(weather_df[\"datetime\"])\n",
        "print(weather_df[\"date\"].dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU_XzGfm-3st"
      },
      "outputs": [],
      "source": [
        "trips_df[\"start_time\"] = pd.to_datetime(trips_df[\"started_at\"])\n",
        "trips_df[\"end_time\"] = pd.to_datetime(trips_df[\"ended_at\"])\n",
        "# ensuring that CRS is EPSG:4326\n",
        "if parking_zones_gdf.crs != \"EPSG:4326\":\n",
        "    parking_zones_gdf = parking_zones_gdf.to_crs(\"EPSG:4326\")\n",
        "# Spatial Join to Map Stations to Parking Zones\n",
        "# Spatial join: add zone info to each station\n",
        "stations_with_zone = gpd.sjoin(\n",
        "    stations_gdf,\n",
        "    parking_zones_gdf[[\"NAME\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"\n",
        ")\n",
        "# Rename column for clarity\n",
        "stations_with_zone = stations_with_zone.rename(columns={\"zone_name\": \"residential_zone\"})\n",
        "# Joining Weather Data\n",
        "# Extract date from start_time for weather join\n",
        "trips_df[\"date\"] = trips_df[\"start_time\"].dt.date\n",
        "weather_df[\"date\"] = weather_df[\"date\"].dt.date\n",
        "\n",
        "# Join weather by date\n",
        "trips_df = trips_df.merge(weather_df, on=\"date\", how=\"left\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MHwqZPyBL_t"
      },
      "outputs": [],
      "source": [
        "trips_df[['start_station_id', 'end_station_id', 'start_station_name', 'end_station_name']].isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnhDD_fQBZvY"
      },
      "outputs": [],
      "source": [
        "trips_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLYxFqxxhgQ0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Feature engineering**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRUlFc1tO8QN"
      },
      "source": [
        "\n",
        "---\n",
        "B1\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48-Qa2HGpClZ"
      },
      "outputs": [],
      "source": [
        "# B1\n",
        "\n",
        "# From started_at\n",
        "trips_df['start_year'] = trips_df['started_at'].dt.year\n",
        "trips_df['start_month'] = trips_df['started_at'].dt.month\n",
        "trips_df['start_day_num'] = trips_df['started_at'].dt.day\n",
        "trips_df['start_day_name'] = trips_df['started_at'].dt.day_name()\n",
        "\n",
        "# From ended_at\n",
        "trips_df['end_year'] = trips_df['ended_at'].dt.year\n",
        "trips_df['end_month'] = trips_df['ended_at'].dt.month\n",
        "trips_df['end_day_num'] = trips_df['ended_at'].dt.day\n",
        "trips_df['end_day_name'] = trips_df['ended_at'].dt.day_name()\n",
        "trips_df.head(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6HWUwZfBbaR"
      },
      "source": [
        "\n",
        "---\n",
        "B2\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7eipZArwwpS"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_duration_minutes'] = (trips_df['end_time'] - trips_df['start_time']).dt.total_seconds() / 60\n",
        "trips_df['trip_duration_minutes']=trips_df['trip_duration_minutes'].round(2)\n",
        "trips_df['trip_duration_minutes'].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN-VHZhttU30"
      },
      "source": [
        "**The trip_duration_minutes problem**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ0TSaictSZr"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_duration_minutes'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dugc51rytbJp"
      },
      "source": [
        "*we can clearly see that there is a problem with the tripd_durations, the min is a negative value and that is not right*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltwKRGertgvi"
      },
      "outputs": [],
      "source": [
        "# Show trips with negative or 0 duration\n",
        "invalid_durations = trips_df[trips_df['trip_duration_minutes'] <= 0]\n",
        "print(f\"Invalid rows: {len(invalid_durations)}\")\n",
        "invalid_durations[['ride_id', 'started_at', 'ended_at', 'trip_duration_minutes']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-wTEW_ZtjcF"
      },
      "outputs": [],
      "source": [
        "# Filter only valid trips\n",
        "trips_df = trips_df[trips_df['trip_duration_minutes'] > 0]\n",
        "trips_df['trip_duration_minutes'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MC8yVG5pmu6"
      },
      "source": [
        "---\n",
        "B3\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iMUivT3_god"
      },
      "outputs": [],
      "source": [
        "trips_df['member_casual'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy1RkWaJodJF"
      },
      "outputs": [],
      "source": [
        "trips_df['rideable_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Q2d88_r7xx"
      },
      "outputs": [],
      "source": [
        "# Initialize base cost\n",
        "# Start with 0 cost\n",
        "trips_df['trip_cost'] = 0.0\n",
        "\n",
        "# Define fixed costs\n",
        "trips_df.loc[trips_df['member_casual'] == 'member', 'trip_cost'] = 3.95\n",
        "trips_df.loc[trips_df['member_casual'] == 'casual', 'trip_cost'] = 1.00\n",
        "\n",
        "# Add extra cost for duration\n",
        "# for members :\n",
        "# Create condition for member rides longer than 45 mins\n",
        "cond_member_extra = (trips_df['member_casual'] == 'member') & (trips_df['trip_duration_minutes'] > 45)\n",
        "\n",
        "# Electric bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'electric_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.10\n",
        "\n",
        "# Classic bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'classic_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.05\n",
        "# Electric bike for casuals\n",
        "cond_casual_electric = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'electric_bike')\n",
        "trips_df.loc[cond_casual_electric, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.15\n",
        "\n",
        "# Classic bike for casuals\n",
        "cond_casual_classic = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'classic_bike')\n",
        "trips_df.loc[cond_casual_classic, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.05\n",
        "# Add Central Business District (CBD) fee\n",
        "# Preparaing your geometry points\n",
        "# Create GeoDataFrame of start points\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "# #  Load CBD Polygon\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=4326)  # Ensures it's in WGS 84\n",
        "\n",
        "\n",
        "# Convert to GeoDataFrames with correct CRS\n",
        "start_gdf = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "end_gdf = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "\n",
        "# Load CBD polygon and project to EPSG:6933\n",
        "# CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "# CBD = CBD.to_crs(epsg=6933)\n",
        "# cbd_polygon = CBD.geometry.unary_union  # Get full boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph9GeYKu2WJl"
      },
      "outputs": [],
      "source": [
        "# Load CBD polygon and project to EPSG:6933\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.unary_union  # Get full boundary\n",
        "CBD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "306f9C9Nuh0x"
      },
      "outputs": [],
      "source": [
        "# Check spatial containment in EPSG:6933\n",
        "trips_df['start_in_cbd'] = start_gdf['start_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "trips_df['end_in_cbd'] = end_gdf['end_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "\n",
        "# Final condition and cost update\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df.loc[trips_df['in_cbd'], 'trip_cost'] += 0.5\n",
        "trips_df['trip_cost'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYgY5Pp2jR5W"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_cost'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck8sSCqNzuXo"
      },
      "source": [
        "*we can see a clear issue in the data ,  and super high values (4.3 mil in the max ) and std is very high (4837.62) , so we must identify this outliers and deal with them*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-WAd13voMS"
      },
      "outputs": [],
      "source": [
        "# High-cost trips\n",
        "high_cost = trips_df[trips_df['trip_cost'] > 1000].copy()\n",
        "print(high_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n",
        "\n",
        "# Negative-cost trips\n",
        "neg_cost = trips_df[trips_df['trip_cost'] < 0].copy()\n",
        "# try to only print the len\n",
        "print(neg_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRq1RuQcwwZT"
      },
      "outputs": [],
      "source": [
        "# Total rows\n",
        "total_rows = len(trips_df)\n",
        "# Define thresholds\n",
        "high_cost_threshold = 10000\n",
        "negative_cost_threshold = 0\n",
        "\n",
        "# Find outliers\n",
        "high_cost_outliers = trips_df[trips_df['trip_cost'] > high_cost_threshold]\n",
        "negative_cost_outliers = trips_df[trips_df['trip_cost'] < negative_cost_threshold]\n",
        "\n",
        "# Count\n",
        "num_high_cost = len(high_cost_outliers)\n",
        "num_negative_cost = len(negative_cost_outliers)\n",
        "total_outliers = num_high_cost + num_negative_cost\n",
        "\n",
        "# Percentages\n",
        "percent_high_cost = (num_high_cost / total_rows) * 100\n",
        "percent_negative_cost = (num_negative_cost / total_rows) * 100\n",
        "percent_total_outliers = (total_outliers / total_rows) * 100\n",
        "\n",
        "print(f\"High cost outliers: {num_high_cost} ({percent_high_cost:.2f}%)\")\n",
        "print(f\"Negative cost outliers: {num_negative_cost} ({percent_negative_cost:.2f}%)\")\n",
        "print(f\"Total outliers: {total_outliers} ({percent_total_outliers:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkFhEsEC0YA0"
      },
      "source": [
        "since they make a very small amount of the data (0.0%) they can be classifed as false data and we can drop them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzmRagLeyj4C"
      },
      "outputs": [],
      "source": [
        "# Drop outliers by reassigning the filtered DataFrame back to df\n",
        "trips_df = trips_df[(trips_df['trip_cost'] <= high_cost_threshold) & (trips_df['trip_cost'] >= negative_cost_threshold)]\n",
        "trips_df['trip_cost'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-inlm6X1UtR"
      },
      "source": [
        "---\n",
        "B4\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb4fPoBD1PaZ"
      },
      "outputs": [],
      "source": [
        "stations_df['CAPACITY'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THTyaahb2BmM"
      },
      "outputs": [],
      "source": [
        "# Basic histogram using Plotly\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Distribution of Station Capacity')\n",
        "fig.update_layout(xaxis_title='Capacity', yaxis_title='Count', bargap=0.1)\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYG6CLLl1UHj"
      },
      "source": [
        "*Choosing the right threshold*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBzKGS5A4g2K"
      },
      "outputs": [],
      "source": [
        "# Drop NaNs\n",
        "capacity_data = stations_df['CAPACITY'].dropna()\n",
        "# Histogram\n",
        "hist_data = go.Histogram(x=capacity_data, nbinsx=30, name='Histogram', opacity=0.6)\n",
        "# Density Curve\n",
        "kde = gaussian_kde(capacity_data)\n",
        "x_vals = np.linspace(capacity_data.min(), capacity_data.max(), 1000)\n",
        "kde_data = go.Scatter(x=x_vals, y=kde(x_vals) * len(capacity_data) * (x_vals[1] - x_vals[0]),\n",
        "                      mode='lines', name='KDE Curve')\n",
        "\n",
        "# Plot both\n",
        "fig = go.Figure(data=[hist_data, kde_data])\n",
        "fig.update_layout(title='Capacity Distribution with KDE',\n",
        "                  xaxis_title='Capacity', yaxis_title='Count')\n",
        "# Example thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.30)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "print(low_thresh,high_thresh)\n",
        "fig.add_vline(x=low_thresh, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=high_thresh, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8xY7YVb1hxP"
      },
      "source": [
        "method 1 : using quantiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULXZ605f4sqp"
      },
      "outputs": [],
      "source": [
        "# Calculate the thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.33)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "\n",
        "def classify_capacity(cap):\n",
        "    if cap <= low_thresh:\n",
        "        return 'Small'\n",
        "    elif cap <= high_thresh:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "stations_df['STATION_SIZE'].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtJ_U9Is1qdi"
      },
      "source": [
        "method 2 : based on domain knowledge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LssiAiXc6shB"
      },
      "outputs": [],
      "source": [
        "def classify_capacity(cap):\n",
        "    if cap <= 15:\n",
        "        return 'Small'\n",
        "    elif cap <= 25:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "stations_df['STATION_SIZE'].value_counts()\n",
        "print(stations_df['STATION_SIZE'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsYLrpKY1vqS"
      },
      "source": [
        "combine with trips df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7vWPiUD19OW"
      },
      "source": [
        "try1 : using the station_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vouMGhoaqdxB"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create a simplified DataFrame for merging\n",
        "station_size_map = stations_df[['STATION_ID', 'STATION_SIZE']].copy()\n",
        "# Step 3: Merge for start_station_size\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_map.rename(columns={\n",
        "        'STATION_ID': 'start_station_id',\n",
        "        'STATION_SIZE': 'start_station_size'\n",
        "    }),\n",
        "    on='start_station_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Step 4: Merge for end_station_size\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_map.rename(columns={\n",
        "        'STATION_ID': 'end_station_id',\n",
        "        'STATION_SIZE': 'end_station_size'\n",
        "    }),\n",
        "    on='end_station_id',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjJnWcdNt128"
      },
      "outputs": [],
      "source": [
        "print(\"Missing start_station_size:\", trips_df['start_station_size'].isna().sum())\n",
        "print(\"Missing end_station_size:\", trips_df['end_station_size'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAxOiIeqyVaE"
      },
      "outputs": [],
      "source": [
        "# What kind of start_station_id had no match?\n",
        "print(trips_df[trips_df['start_station_size'].isna()][['start_station_id']].drop_duplicates().head(10))\n",
        "\n",
        "# Same for end_station\n",
        "print(trips_df[trips_df['end_station_size'].isna()][['end_station_id']].drop_duplicates().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lK7oC-15RR"
      },
      "source": [
        "try2 : with names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuNfrlpwyuIk"
      },
      "outputs": [],
      "source": [
        "stations_df['NAME'] = stations_df['NAME'].str.strip().str.lower()\n",
        "trips_df['start_station_name'] = trips_df['start_station_name'].str.strip().str.lower()\n",
        "trips_df['end_station_name'] = trips_df['end_station_name'].str.strip().str.lower()\n",
        "\n",
        "# Map start station size using name\n",
        "station_size_name_map = stations_df[['NAME', 'STATION_SIZE']]\n",
        "\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_name_map.rename(columns={'NAME': 'start_station_name', 'STATION_SIZE': 'start_station_size_name'}),\n",
        "    on='start_station_name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Same for end station\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_name_map.rename(columns={'NAME': 'end_station_name', 'STATION_SIZE': 'end_station_size_name'}),\n",
        "    on='end_station_name',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t904hbWnty9o"
      },
      "outputs": [],
      "source": [
        "trips_df['end_station_size'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyd6ovXSzhG7"
      },
      "outputs": [],
      "source": [
        "trips_df['start_station_size'] = trips_df['start_station_size_name']\n",
        "trips_df['end_station_size'] = trips_df['end_station_size_name']\n",
        "\n",
        "# Then drop the temp columns\n",
        "trips_df.drop(columns=['start_station_size_name', 'end_station_size_name'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbDfN8QEubCh"
      },
      "outputs": [],
      "source": [
        "print(trips_df[['start_station_size', 'end_station_size']].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7RoQ7P92IHU"
      },
      "source": [
        "using names was better but we still have some null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5_IiBmU7Npw"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Station Capacity Distribution')\n",
        "fig.add_vline(x=15, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=25, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMBlw701c9vN"
      },
      "source": [
        "---\n",
        "B5\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fylKxgKMecgw"
      },
      "outputs": [],
      "source": [
        "Shuttle_Bus_Stops.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIUA0TNYe2RO"
      },
      "outputs": [],
      "source": [
        "Metro_Bus_Stops['BSTP_LAT'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_buZ1x_CAVFQ"
      },
      "source": [
        "\n",
        "Approaches\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Approach                    | Time Complexity | Vectorized | Fast    |\n",
        "| --------------------------- | --------------- | ---------- | ------- |\n",
        "| Brute Force (Your original) | O(N Ã— M)        | âŒ No       | ðŸŒ Slow |\n",
        "| BallTree (New)              | O(N log M)      | âœ… Yes      | âš¡ Fast  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_i2yvv2O8QX"
      },
      "source": [
        "Project all your coordinates to EPSG:6933\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEGPg81slsNc"
      },
      "outputs": [],
      "source": [
        "# Create start and end point geometries\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "\n",
        "# Create GeoDataFrames\n",
        "gdf_start = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "gdf_end = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "\n",
        "# Add x/y columns\n",
        "trips_df['start_x'] = gdf_start.geometry.x\n",
        "trips_df['start_y'] = gdf_start.geometry.y\n",
        "trips_df['end_x'] = gdf_end.geometry.x\n",
        "trips_df['end_y'] = gdf_end.geometry.y\n",
        "\n",
        "\n",
        "# projecting   metro and shuttle station coordinates:\n",
        "\n",
        "# Convert station lat/lng to projected coordinates\n",
        "def project_coords(coords_list):\n",
        "    gdf = gpd.GeoDataFrame(geometry=[Point(lon, lat) for lat, lon in coords_list], crs='EPSG:4326')\n",
        "    gdf = gdf.to_crs(epsg=6933)\n",
        "    return np.array([(geom.x, geom.y) for geom in gdf.geometry])\n",
        "# coords\n",
        "# Metro stop coordinates\n",
        "metro_coords = Metro_Bus_Stops[['BSTP_LAT', 'BSTP_LON']].dropna().values\n",
        "\n",
        "# Shuttle stop coordinates\n",
        "shuttle_coords = Shuttle_Bus_Stops[['LATITUDE', 'LONGITUDE']].dropna().values\n",
        "\n",
        "metro_coords_projected = project_coords(metro_coords)\n",
        "shuttle_coords_projected = project_coords(shuttle_coords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN48kQYPl5A0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def euclidean_tree_batch(source_df, stop_coords, x_col, y_col, batch_size=10000):\n",
        "    tree = BallTree(stop_coords, metric='euclidean')\n",
        "\n",
        "    distances = []\n",
        "    n = len(source_df)\n",
        "    tqdm.pandas(desc=f\"Computing distances for {x_col}\")\n",
        "\n",
        "    for i in tqdm(range(0, n, batch_size), desc=\"Batch processing\", unit=\"batch\"):\n",
        "        batch = source_df.iloc[i:i+batch_size]\n",
        "        batch_points = batch[[x_col, y_col]].values\n",
        "\n",
        "        dists, _ = tree.query(batch_points, k=1)\n",
        "        distances.extend(dists.flatten().tolist())\n",
        "\n",
        "    return distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvjyWh64l7Qd"
      },
      "outputs": [],
      "source": [
        "# Start â†’ Metro\n",
        "trips_df['start_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End â†’ Metro\n",
        "trips_df['end_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'end_x', 'end_y'\n",
        ")\n",
        "\n",
        "# Start â†’ Shuttle\n",
        "trips_df['start_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End â†’ Shuttle\n",
        "trips_df['end_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'end_x', 'end_y'\n",
        ")\n",
        "\n",
        "# converting to from meters to km (our choice)\n",
        "trips_df['start_nearest_metro_distance'] = trips_df['start_nearest_metro_distance'] / 1000\n",
        "trips_df['end_nearest_metro_distance'] = trips_df['end_nearest_metro_distance'] / 1000\n",
        "trips_df['start_nearest_shuttle_distance'] = trips_df['start_nearest_shuttle_distance'] / 1000\n",
        "trips_df['end_nearest_shuttle_distance'] = trips_df['end_nearest_shuttle_distance'] / 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4bgJlGMAlPE"
      },
      "outputs": [],
      "source": [
        "trips_df[\n",
        "    ['start_nearest_metro_distance',\n",
        "     'end_nearest_metro_distance',\n",
        "     'start_nearest_shuttle_distance',\n",
        "     'end_nearest_shuttle_distance']\n",
        "].describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3_w8Db_YRhh"
      },
      "outputs": [],
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "cols = ['start_nearest_metro_distance', 'end_nearest_metro_distance',\n",
        "        'start_nearest_shuttle_distance', 'end_nearest_shuttle_distance']\n",
        "for col in cols:\n",
        "    fig = go.Figure(\n",
        "        data=[go.Histogram(\n",
        "            x=sampled_df[col],\n",
        "            nbinsx=100,\n",
        "            marker=dict(color='skyblue'),\n",
        "            opacity=0.75\n",
        "        )]\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=col,\n",
        "        xaxis_title=col,\n",
        "        yaxis_title='Count (Log Scale)',\n",
        "        yaxis_type='log',\n",
        "        bargap=0.1,\n",
        "        width=800,\n",
        "        height=400\n",
        "    )\n",
        "    fig.show(config={'staticPlot':True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAFaQhYwnyJ5"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import pandas as pd # Assuming trips_df is a pandas DataFrame\n",
        "\n",
        "# Re-define thresholds for clarity\n",
        "start_nearest_metro_distance_thr = 1 # meters\n",
        "end_nearest_metro_distance_thr = 1   # meters\n",
        "start_nearest_shuttle_distance_thr = 9 # meters\n",
        "end_nearest_shuttle_distance_thr = 9 # meters\n",
        "\n",
        "# --- Step 1: Identify \"far from\" trips based on current thresholds ---\n",
        "# Using the corrected end_nearest_shuttle_distance_thr for the end shuttle distance\n",
        "far_metro_start_trips = trips_df[trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr]\n",
        "far_metro_end_trips = trips_df[trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr]\n",
        "far_shuttle_start_trips = trips_df[trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr]\n",
        "far_shuttle_end_trips = trips_df[trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr]\n",
        "\n",
        "# Combine all \"far from transit\" trips for a general map (for demonstration)\n",
        "# Using a logical OR to get any trip that is far from ANY of these points\n",
        "far_from_transit_trips = trips_df[\n",
        "    (trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr) |\n",
        "    (trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr) |\n",
        "    (trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr) |\n",
        "    (trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr)\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "print(f\"Number of trips far from metro (start): {len(far_metro_start_trips)}\")\n",
        "print(f\"Number of trips far from metro (end): {len(far_metro_end_trips)}\")\n",
        "print(f\"Number of trips far from shuttle (start): {len(far_shuttle_start_trips)}\")\n",
        "print(f\"Number of trips far from shuttle (end): {len(far_shuttle_end_trips)}\")\n",
        "print(f\"Total unique trips identified as 'far from transit': {len(far_from_transit_trips)}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Create a Folium Map ---\n",
        "\n",
        "# Get the approximate center of Washington D.C. for the map's initial view\n",
        "# You can use the mean of your start/end lat/lngs or a known DC coordinate\n",
        "dc_center_lat = trips_df['start_lat'].mean() # Or a more precise known center for DC\n",
        "dc_center_lng = trips_df['start_lng'].mean() # Or a more precise known center for DC\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[dc_center_lat, dc_center_lng], zoom_start=12)\n",
        "\n",
        "# Add markers for start points of trips identified as \"far from transit\"\n",
        "# Due to the large number of potential points (40k), plotting individual markers for all\n",
        "# might be slow or make the map unreadable.\n",
        "# We'll plot a sample or use a MarkerCluster for better performance.\n",
        "# Let's start by plotting a *sample* of these points if far_from_transit_trips is very large,\n",
        "# or use MarkerCluster. For a first look, a small sample is good.\n",
        "\n",
        "# If you have too many points, consider sampling for initial visualization\n",
        "# Or, even better for density visualization, use MarkerCluster or HeatMap (if allowed for density, check project rules)\n",
        "# Since you're using Plotly for charts and Folium for maps, heatmap should be fine.\n",
        "\n",
        "# Let's just add a few to see the logic work, or use MarkerCluster for all:\n",
        "\n",
        "# OPTION A: Plotting a limited sample (good for a quick check if map gets cluttered)\n",
        "# sample_size = 1000 # Adjust as needed\n",
        "# if len(far_from_transit_trips) > sample_size:\n",
        "#     sample_to_plot = far_from_transit_trips.sample(sample_size, random_state=42)\n",
        "# else:\n",
        "#     sample_to_plot = far_from_transit_trips\n",
        "\n",
        "# for idx, row in sample_to_plot.iterrows():\n",
        "#     folium.CircleMarker(\n",
        "#         location=[row['start_lat'], row['start_lng']],\n",
        "#         radius=2, # Small radius\n",
        "#         color='red',\n",
        "#         fill=True,\n",
        "#         fill_color='red',\n",
        "#         fill_opacity=0.6,\n",
        "#         tooltip=f\"Start: {row['start_station_name']} (Far from Transit)\"\n",
        "#     ).add_to(m)\n",
        "\n",
        "# OPTION B: Using MarkerCluster for better visualization of many points\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "marker_cluster_start = MarkerCluster().add_to(m)\n",
        "marker_cluster_end = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add start points\n",
        "for idx, row in far_from_transit_trips.iterrows():\n",
        "    if pd.notnull(row['start_lat']) and pd.notnull(row['start_lng']):\n",
        "        folium.CircleMarker(\n",
        "            location=[row['start_lat'], row['start_lng']],\n",
        "            radius=2,\n",
        "            color='red', # Color for start points\n",
        "            fill=True,\n",
        "            fill_color='red',\n",
        "            fill_opacity=0.6,\n",
        "            tooltip=f\"Start: {row['start_station_name']} (Far from Transit)\"\n",
        "        ).add_to(marker_cluster_start)\n",
        "\n",
        "# Add end points (optional, you might want separate layers or colors if combining)\n",
        "# For now, let's just show start points to avoid overwhelming the map.\n",
        "# If you want to see end points, you could use a different color or a separate MarkerCluster\n",
        "# for idx, row in far_from_transit_trips.iterrows():\n",
        "#     if pd.notnull(row['end_lat']) and pd.notnull(row['end_lng']):\n",
        "#         folium.CircleMarker(\n",
        "#             location=[row['end_lat'], row['end_lng']],\n",
        "#             radius=2,\n",
        "#             color='blue', # Color for end points\n",
        "#             fill=True,\n",
        "#             fill_color='blue',\n",
        "#             fill_opacity=0.6,\n",
        "#             tooltip=f\"End: {row['end_station_name']} (Far from Transit)\"\n",
        "#         ).add_to(marker_cluster_end)\n",
        "\n",
        "\n",
        "# Save the map to an HTML file or display in a Jupyter Notebook\n",
        "# m.save(\"far_from_transit_trips_map.html\")\n",
        "# In a Jupyter/IPython environment, you can also just display `m` directly\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgaKHoAnoORK"
      },
      "outputs": [],
      "source": [
        "# --- Define your new thresholds based on your histogram observations ---\n",
        "# Example new thresholds (YOU WILL REPLACE THESE WITH YOUR OWN INSIGHTS)\n",
        "new_metro_start_thr = 1  # meters (e.g., if you see a clear drop after 1.2km)\n",
        "new_metro_end_thr = 1    # meters\n",
        "new_shuttle_start_thr = 9 # meters (e.g., if you see a drop after 20km)\n",
        "new_shuttle_end_thr = 9 # meters\n",
        "\n",
        "# --- Create the new boolean features ---\n",
        "trips_df['is_far_from_metro_start'] = trips_df['start_nearest_metro_distance'] > new_metro_start_thr\n",
        "trips_df['is_far_from_metro_end'] = trips_df['end_nearest_metro_distance'] > new_metro_end_thr\n",
        "trips_df['is_far_from_shuttle_start'] = trips_df['start_nearest_shuttle_distance'] > new_shuttle_start_thr\n",
        "trips_df['is_far_from_shuttle_end'] = trips_df['end_nearest_shuttle_distance'] > new_shuttle_end_thr\n",
        "\n",
        "# You can also create a combined flag for any \"far from transit\"\n",
        "trips_df['is_far_from_any_transit'] = (\n",
        "    trips_df['is_far_from_metro_start'] |\n",
        "    trips_df['is_far_from_metro_end'] |\n",
        "    trips_df['is_far_from_shuttle_start'] |\n",
        "    trips_df['is_far_from_shuttle_end']\n",
        ")\n",
        "\n",
        "# Verify the counts of the new features\n",
        "print(\"\\nCounts for new 'far from' features:\")\n",
        "print(trips_df[['is_far_from_metro_start', 'is_far_from_metro_end',\n",
        "                'is_far_from_shuttle_start', 'is_far_from_shuttle_end',\n",
        "                'is_far_from_any_transit']].sum())\n",
        "\n",
        "# Display the first few rows with the new columns to confirm\n",
        "print(\"\\nTrips DataFrame with new features:\")\n",
        "print(trips_df[['ride_id', 'start_nearest_metro_distance', 'is_far_from_metro_start',\n",
        "                'start_nearest_shuttle_distance', 'is_far_from_shuttle_start',\n",
        "                'is_far_from_any_transit']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw0kmmiTpELo"
      },
      "outputs": [],
      "source": [
        "trips_df['is_far_from_metro_start'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smMf9ipl0yav"
      },
      "source": [
        "---\n",
        "B6\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lXEKzGY2Lhs"
      },
      "outputs": [],
      "source": [
        "print(trips_df['start_point'].iloc[0], type(trips_df['start_point'].iloc[0]))\n",
        "print(trips_df['end_point'].iloc[0], type(trips_df['end_point'].iloc[0]))\n",
        "print(type(cbd_polygon))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOAsnPfBp6Py"
      },
      "outputs": [],
      "source": [
        "# STEP 0: Make sure the CBD polygon is projected correctly\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.iloc[0]  # assuming a single polygon\n",
        "# STEP 1: Create a GeoDataFrame from the trip points (start and end)\n",
        "# start_gdf = gpd.GeoDataFrame(trips_df, geometry=trips_df['start_point'], crs=\"EPSG:4326\")\n",
        "# end_gdf   = gpd.GeoDataFrame(trips_df, geometry=trips_df['end_point'], crs=\"EPSG:4326\")\n",
        "\n",
        "# Rebuild the point geometries from lat/lng in EPSG:4326\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "\n",
        "# Project everything to EPSG:6933\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "start_gdf = start_gdf.to_crs(epsg=6933)\n",
        "end_gdf = end_gdf.to_crs(epsg=6933)\n",
        "\n",
        "# CBD polygon (in same projection)\n",
        "cbd_polygon = CBD.geometry.unary_union\n",
        "# Check containment\n",
        "trips_df['start_in_cbd'] = start_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "trips_df['end_in_cbd']   = end_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "\n",
        "# Final result\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df['in_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M_uabQt6Bza"
      },
      "source": [
        "---\n",
        "B7\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oxDKxPlsm3r"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Compute the CBD centroid (already in EPSG:6933)\n",
        "cbd_centroid = cbd_polygon.centroid  # geometry in meters (EPSG:6933)\n",
        "\n",
        "# --- Step 2: Recreate end point GeoDataFrame and project to EPSG:6933\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=6933)\n",
        "\n",
        "# --- Step 3: Compute Euclidean distance in meters\n",
        "trips_df['distance_to_cbd_m'] = end_gdf.geometry.distance(cbd_centroid)\n",
        "\n",
        "# --- Step 4: Set distance to None where start AND end are in the CBD\n",
        "mask = trips_df['start_in_cbd'] & trips_df['end_in_cbd']\n",
        "trips_df.loc[mask, 'distance_to_cbd_m'] = None\n",
        "\n",
        "# --- Step 5: Inspect result\n",
        "trips_df['distance_to_cbd_m'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szoai97X63Qj"
      },
      "source": [
        "\n",
        "\n",
        "**Threasholding strategies**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wnOOXiY7I-C"
      },
      "source": [
        "kinda of an elbow method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPKO880k6_k8"
      },
      "outputs": [],
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "# Extract the data\n",
        "data = sampled_df['distance_to_cbd_m'].dropna()\n",
        "\n",
        "# Create histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=100,\n",
        "    name='Histogram',\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75\n",
        ")\n",
        "\n",
        "# Create KDE line (manual since Plotly doesnâ€™t support KDE directly)\n",
        "kde = gaussian_kde(data)\n",
        "x_vals = np.linspace(data.min(), data.max(), 1000)\n",
        "kde_vals = kde(x_vals) * len(data) * (x_vals[1] - x_vals[0])  # scale to match histogram\n",
        "\n",
        "kde_trace = go.Scatter(\n",
        "    x=x_vals,\n",
        "    y=kde_vals,\n",
        "    mode='lines',\n",
        "    name='KDE',\n",
        "    line=dict(color='darkblue')\n",
        ")\n",
        "\n",
        "# Vertical reference lines\n",
        "vline1 = go.Scatter(\n",
        "    x=[2000, 2000],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='2km Threshold',\n",
        "    line=dict(color='red', dash='dash')\n",
        ")\n",
        "\n",
        "vline2 = go.Scatter(\n",
        "    x=[2764, 2764],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='Median',\n",
        "    line=dict(color='green', dash='dash')\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=[hist, kde_trace, vline1, vline2])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distance to CBD at End of Trip',\n",
        "    xaxis_title='distance_to_cbd_m',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    legend=dict(x=0.7, y=0.95)\n",
        ")\n",
        "\n",
        "fig.show( config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvBXxgrW7IIf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "i will choose the median beacause looking at the histogram we can see the counts drops\n",
        "\"\"\"\n",
        "threshold = 2764\n",
        "# Apply binary classification\n",
        "trips_df['close_to_cbd'] = trips_df['distance_to_cbd_m'].apply(\n",
        "    lambda d: None if pd.isna(d) else d <= threshold\n",
        ")\n",
        "trips_df['close_to_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68oomfV78m6-"
      },
      "outputs": [],
      "source": [
        "print(trips_df['close_to_cbd'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ellq2QJVzAe3"
      },
      "source": [
        "---\n",
        "B8\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZVkChHv0YkX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Washington, D.C. is roughly:\n",
        "\n",
        "~16 km (north-south)\n",
        "\n",
        "~13 km (east-west)\n",
        "\n",
        "So, a geohash precision of 5â€“8 is appropriate.\n",
        "\"\"\"\n",
        "def encode_geohashes(df, lat_col, lon_col, precisions):\n",
        "    for p in precisions:\n",
        "        col_name = f'geohash_p{p}'\n",
        "        df[col_name] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lon_col], p), axis=1)\n",
        "    return df\n",
        "\n",
        "# Try precisions from 5 to 8\n",
        "precisions_to_test = [5, 6, 7, 8]\n",
        "trips_df = encode_geohashes(trips_df, 'start_lat', 'start_lng', precisions_to_test)\n",
        "for p in precisions_to_test:\n",
        "    print(f\"Precision {p}: {trips_df[f'geohash_p{p}'].nunique()} unique regions\")\n",
        "\"\"\"\n",
        "If the number is too small â†’ you're over-aggregating.\n",
        "\n",
        "If it's too big (e.g. thousands) â†’ too fine â†’ hard to summarize meaningfully.\n",
        "\"\"\"\n",
        "\n",
        "for p in precisions_to_test:\n",
        "    counts = trips_df[f'geohash_p{p}'].value_counts()\n",
        "    print(f\"Precision {p} â†’ median trips per geohash: {counts.median()}\")\n",
        "\"\"\"\n",
        "This tells us how balanced the spatial bins are.\n",
        "\n",
        "we ideally want 50â€“500 trips per cell.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShObmS0z4FJ1"
      },
      "source": [
        "| Precision | Median Trips per Geohash | Interpretation                                                     |\n",
        "| --------- | ------------------------ | ------------------------------------------------------------------ |\n",
        "| **5**     | 1761                     | âš ï¸ Too coarse â€” merges many neighborhoods into one.                |\n",
        "| **6**     | 196                      | âœ… Good balance â€” each area has enough trips for reliable analysis. |\n",
        "| **7**     | 7                        | âš ï¸ Very fine â€” may be too sparse for most practical summaries.     |\n",
        "| **8**     | 2                        | ðŸš« Too sparse â€” most areas will be noise or empty.                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3yKdEI63oaL"
      },
      "outputs": [],
      "source": [
        "# we will choose 6t\n",
        "trips_df['geohash_sector'] = trips_df['geohash_p6']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRZhifmX5KRC"
      },
      "source": [
        "---\n",
        "\n",
        "B9\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpEGkPhx4RQ5"
      },
      "outputs": [],
      "source": [
        "# Group by Sector and Date\n",
        "# Assume you have a 'date' column (convert if needed)\n",
        "trips_df['date'] = pd.to_datetime(trips_df['date'])\n",
        "\n",
        "# Count trips per day per sector\n",
        "daily_counts = trips_df.groupby(['geohash_p6', 'date']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Now compute average daily trips per geohash sector\n",
        "avg_daily_trips = daily_counts.groupby('geohash_p6')['trip_count'].mean().reset_index()\n",
        "avg_daily_trips.rename(columns={'trip_count': 'avg_daily_trips'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctAtFgF5rwM"
      },
      "source": [
        "Choose Segmentation Method (for Red / Yellow / Gray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfhDqYSj5nJs"
      },
      "source": [
        "\n",
        "| Method                         | Description                          | Pros             | Use Case             |\n",
        "| ------------------------------ | ------------------------------------ | ---------------- | -------------------- |\n",
        "| **Quantiles** (e.g., tertiles) | Divide into 3 equal-sized groups     | Simple, fair     | Balanced datasets    |\n",
        "| **Natural Breaks (Jenks)**     | Optimize separation between clusters | Data-aware       | Uneven distributions |\n",
        "| **KMeans Clustering (k=3)**    | Machine learning-based segmentation  | Optimal grouping | Large datasets       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g__UfD5W5a6n"
      },
      "outputs": [],
      "source": [
        "# quantiles :\n",
        "# Assign labels based on quantiles\n",
        "quantiles = avg_daily_trips['avg_daily_trips'].quantile([1/3, 2/3])\n",
        "low_thresh = quantiles.iloc[0]\n",
        "high_thresh = quantiles.iloc[1]\n",
        "\n",
        "def classify_volume(val):\n",
        "    if val < low_thresh:\n",
        "        return 'gray'   # Low volume\n",
        "    elif val < high_thresh:\n",
        "        return 'yellow' # Medium volume\n",
        "    else:\n",
        "        return 'red'    # High volume\n",
        "\n",
        "avg_daily_trips['volume_segment'] = avg_daily_trips['avg_daily_trips'].apply(classify_volume)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAYD3Ls1-Rpi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract the data\n",
        "data = avg_daily_trips['avg_daily_trips'].dropna()\n",
        "\n",
        "# Histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=30,\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75,\n",
        "    name='Avg Daily Trips'\n",
        ")\n",
        "\n",
        "# Vertical threshold lines\n",
        "vline_low = go.Scatter(\n",
        "    x=[low_thresh, low_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='Low Threshold',\n",
        "    line=dict(color='gray', dash='dash')\n",
        ")\n",
        "\n",
        "vline_high = go.Scatter(\n",
        "    x=[high_thresh, high_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='High Threshold',\n",
        "    line=dict(color='orange', dash='dash')\n",
        ")\n",
        "\n",
        "# Combine into figure\n",
        "fig = go.Figure(data=[hist, vline_low, vline_high])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distribution of Avg Daily Trips per Geohash Sector',\n",
        "    xaxis_title='Avg Daily Trips',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    bargap=0.1\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYWaSyEE569w"
      },
      "outputs": [],
      "source": [
        "X = avg_daily_trips[['avg_daily_trips']].values\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42  , n_init=10).fit(X)\n",
        "avg_daily_trips['kmeans_label'] = kmeans.labels_\n",
        "\n",
        "# Map to red/yellow/gray using sorted cluster means\n",
        "label_map = dict(zip(\n",
        "    np.argsort(kmeans.cluster_centers_.flatten()),\n",
        "    ['gray', 'yellow', 'red']\n",
        "))\n",
        "avg_daily_trips['kmeans_segment'] = avg_daily_trips['kmeans_label'].map(label_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaBBJvFnZ81X"
      },
      "outputs": [],
      "source": [
        "avg_daily_trips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2epWDpMGakGT"
      },
      "outputs": [],
      "source": [
        "trips_df['geohash_p6'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONedOBCPcTl-"
      },
      "outputs": [],
      "source": [
        "# Merge segments into trips_df\n",
        "trips_df = trips_df.merge(\n",
        "    avg_daily_trips[['geohash_p6','volume_segment','kmeans_segment']],\n",
        "    on='geohash_p6',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eK2NYQkgpue"
      },
      "outputs": [],
      "source": [
        "trips_df['kmeans_segment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WSRw0iZhLjR"
      },
      "outputs": [],
      "source": [
        "trips_df['volume_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1Nby-lsEf9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "B10\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKSxW3W-dWCx"
      },
      "outputs": [],
      "source": [
        "trips_df['conditions'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh1m4kfBfY8I"
      },
      "outputs": [],
      "source": [
        "def classify_weather(condition):\n",
        "    condition = condition.lower()  # lowercase for safety\n",
        "    if 'rain' in condition or 'snow' in condition:\n",
        "        return 'rainy'\n",
        "    elif 'overcast' in condition or 'cloudy' in condition:\n",
        "        return 'cloudy'\n",
        "    elif 'clear' in condition:\n",
        "        return 'sunny'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "# Apply binning\n",
        "trips_df['weather_segment'] = trips_df['conditions'].apply(classify_weather)\n",
        "trips_df['weather_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep9rr_N9s9v_"
      },
      "source": [
        "---\n",
        "\n",
        "B11\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cd81R4R4PWY"
      },
      "source": [
        "quick inspection of the data dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_f0gqxJNym1"
      },
      "outputs": [],
      "source": [
        "sorted_ended_at_df = trips_df[['ended_at']].sort_values(by='ended_at')\n",
        "print(\"--- Sorted 'ended_at' DataFrame (first 5 rows) ---\")\n",
        "print(sorted_ended_at_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Find the earliest and latest dates ---\n",
        "earliest_date = sorted_ended_at_df['ended_at'].min()\n",
        "latest_date = sorted_ended_at_df['ended_at'].max()\n",
        "\n",
        "print(f\"The earliest date in 'ended_at' is: {earliest_date}\")\n",
        "print(f\"The latest date in 'ended_at' is: {latest_date}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0A2F38oTZ5x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Step 2: Sort the DataFrame by 'started_at' ---\n",
        "sorted_started_at_df = trips_df[['started_at']].sort_values(by='started_at')\n",
        "print(\"--- Sorted 'started_at' DataFrame (first 5 rows) ---\")\n",
        "print(sorted_started_at_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Find the earliest and latest dates using 'started_at' ---\n",
        "earliest_date_started = sorted_started_at_df['started_at'].min()\n",
        "latest_date_started = sorted_started_at_df['started_at'].max()\n",
        "\n",
        "print(f\"The earliest date in 'started_at' is: {earliest_date_started}\")\n",
        "print(f\"The latest date in 'started_at' is: {latest_date_started}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37IY3SeYr1C1"
      },
      "outputs": [],
      "source": [
        "# Make sure 'ended_at' is datetime\n",
        "# trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'])\n",
        "trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'], format='mixed', errors='coerce')\n",
        "\n",
        "\n",
        "# Extract just the date (without time)\n",
        "trips_df['end_date'] = trips_df['ended_at'].dt.date\n",
        "daily_income_weather = trips_df.groupby(['end_date', 'weather_segment'])['trip_cost'].sum().reset_index()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxfbTLjmtBDz"
      },
      "outputs": [],
      "source": [
        "# convert\n",
        "# Make sure end_date is datetime\n",
        "daily_income_weather['end_date'] = pd.to_datetime(daily_income_weather['end_date'])\n",
        "\n",
        "fig_long = px.line(\n",
        "    daily_income_weather,\n",
        "    x='end_date',\n",
        "    y='trip_cost',\n",
        "    color='weather_segment',\n",
        "    title='Daily Total Trip Cost by Weather Condition (Long Format)',\n",
        "    labels={'end_date': 'Date', 'trip_cost': 'Total Income', 'weather_segment': 'Weather'}\n",
        ")\n",
        "\n",
        "fig_long.update_layout(xaxis_title='Date', yaxis_title='Trip Cost', hovermode='x unified')\n",
        "fig_long.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6WfXRDRtHwQ"
      },
      "outputs": [],
      "source": [
        "# Pivot to wide format\n",
        "wide_df = daily_income_weather.pivot(index='end_date', columns='weather_segment', values='trip_cost').fillna(0)\n",
        "wide_df = wide_df.sort_index()\n",
        "\n",
        "# Build traces\n",
        "fig_wide = go.Figure()\n",
        "\n",
        "for condition in wide_df.columns:\n",
        "    fig_wide.add_trace(go.Scatter(\n",
        "        x=wide_df.index,\n",
        "        y=wide_df[condition],\n",
        "        mode='lines',\n",
        "        name=condition\n",
        "    ))\n",
        "\n",
        "fig_wide.update_layout(\n",
        "    title='Daily Total Trip Cost by Weather Condition (Wide Format)',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Trip Cost',\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white',\n",
        "    legend_title='Weather'\n",
        ")\n",
        "\n",
        "fig_wide.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMDgJxs8OXx_"
      },
      "source": [
        "Which one is better for our problem  ?<br>\n",
        "the Long Format is the most suitable and effective,This is because it allows for direct visual comparison of revenue trends across different weather types over time on a single graph, making it easier to spot patterns and seasonal impacts. The long format is also considered more intuitive for time-series visualization. Conversely, the \"wide format\" is deemed less clear due to potential visual clutter when many categories are present.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBXs3HzQO-wt"
      },
      "source": [
        "---\n",
        "B12\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruD09zYz5E68"
      },
      "source": [
        "Feature 1 : rush_hour\n",
        "<br> Indicates if the ride occurred during typical commuting hours (7â€“10 AM or 4â€“7 PM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN5x_9uTt2RV"
      },
      "outputs": [],
      "source": [
        "\n",
        "trips_df['start_time'] = pd.to_datetime(trips_df['start_time'], errors='coerce')\n",
        "\n",
        "trips_df['rush_hour'] = (\n",
        "    trips_df['start_time'].dt.hour.between(7, 10) |\n",
        "    trips_df['start_time'].dt.hour.between(16, 19)\n",
        ").astype(int)\n",
        "trips_df['rush_hour'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCxcP4_M5MK-"
      },
      "source": [
        "Feature 2 : hour_segment <br>\n",
        "Categorize ride start times into broader buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQtxwZZnPxnT"
      },
      "outputs": [],
      "source": [
        "def get_hour_segment(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Midday'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "trips_df['hour_segment'] = trips_df['start_time'].dt.hour.apply(get_hour_segment)\n",
        "trips_df['hour_segment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGwHHc2n5ThH"
      },
      "source": [
        "Feature 3 : is_weekend<br>\n",
        "Helps spot usage patterns on weekends vs weekdays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiVY5Od0QKAe"
      },
      "outputs": [],
      "source": [
        "trips_df['is_weekend'] = trips_df['start_time'].dt.dayofweek >= 5\n",
        "trips_df['is_weekend'] = trips_df['is_weekend'].astype(int)\n",
        "trips_df['is_weekend'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49oBoftHUUaG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**EDA**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOhRmjguH-c"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Sampling the data\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABgwYinmuLkR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sampled data stats\n",
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxL-4ceXUYhn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# A )\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Top 5 Starting Stations Analysis\n",
        "\n",
        "### Objective\n",
        "Identify the top 5 stations with the highest number of trip departures (starting stations) and create a bar chart showing statistical information for these top 5 stations.\n",
        "\n",
        "### Requirements\n",
        "- Identify the top 5 stations with the highest number of trip departures (starting stations)\n",
        "- Create a bar chart showing statistical information for these top 5 stations\n",
        "- Display the count of trips starting from each station\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cwZZ30VTEp_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Identify the top 5 starting stations\n",
        "start_station_counts = sampled_df['start_station_name'].value_counts()\n",
        "top_5_start_stations = start_station_counts.head(5)\n",
        "\n",
        "print(\"Top 5 Starting Stations:\")\n",
        "print(top_5_start_stations)\n",
        "\n",
        "# Create a bar chart for the top 5 starting stations\n",
        "fig = px.bar(\n",
        "    top_5_start_stations,\n",
        "    x=top_5_start_stations.index,\n",
        "    y=top_5_start_stations.values,\n",
        "    title='Top 5 Starting Stations by Trip Count',\n",
        "    labels={'x': 'Station Name', 'y': 'Number of Trips'},\n",
        "    color=top_5_start_stations.values,  # Color bars by count\n",
        "    color_continuous_scale=px.colors.sequential.Viridis # Optional: choose a color scale\n",
        ")\n",
        "\n",
        "# Update layout for better readability\n",
        "fig.update_layout(\n",
        "    xaxis={'categoryorder':'total descending'}, # Ensure bars are ordered by count\n",
        "    xaxis_title='Station Name',\n",
        "    yaxis_title='Number of Trips',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top 5 Starting Stations by Trip Count\n",
        "\n",
        "**Key Insight:**\n",
        "\n",
        "* The station **\"Park Rd & Holmead Pl NW\"** is by far the **most popular starting point**, with approximately **300 trips**, significantly ahead of the next stations.\n",
        "\n",
        "**Detailed Breakdown:**\n",
        "\n",
        "* The remaining top 4 stations:\n",
        "\n",
        "  * **\"14th & Belmont St NW\"**\n",
        "  * **\"18th St & Wyoming Ave NW\"**\n",
        "  * **\"Columbus Circle / Union Station\"**\n",
        "  * **\"Lamont & Mt Pleasant NW\"**\n",
        "\n",
        "  Each of these falls in the **190â€“210 trip range**, indicating moderate popularity and relatively close usage levels among them.\n",
        "\n",
        "**Implication:**\n",
        "\n",
        "* The high volume at **Park Rd & Holmead Pl NW** could reflect factors such as:\n",
        "\n",
        "  * Proximity to residential or commercial zones\n",
        "  * Availability of bike lanes or connectivity\n",
        "  * Strategic location near transit hubs or universities\n",
        "\n",
        "This station likely plays a **central role in trip generation**, which may warrant priority in terms of maintenance, expansions, or targeted promotions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Trip Distribution by Bike Type and Membership\n",
        "\n",
        "### Objective\n",
        "Calculate the distribution of trips by bike type (classic vs electric) and membership type (member vs casual), then create a single bar chart showing these distributions.\n",
        "\n",
        "### Requirements\n",
        "- Calculate the distribution of trips by:\n",
        "  - Bike type (classic vs electric)\n",
        "  - Membership type (member vs casual)\n",
        "- Create one bar chart showing these distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the distribution\n",
        "distribution = sampled_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='count')\n",
        "\n",
        "# Create the bar chart\n",
        "fig = px.bar(\n",
        "    distribution,\n",
        "    x='rideable_type',\n",
        "    y='count',\n",
        "    color='member_casual',\n",
        "    barmode='group',\n",
        "    title='Trip Distribution by Bike Type and Membership',\n",
        "    labels={'rideable_type': 'Bike Type', 'count': 'Number of Trips', 'member_casual': 'Membership Type'}\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_title='Bike Type', yaxis_title='Number of Trips')\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trip Distribution by Bike Type and Membership\n",
        "\n",
        "**Key Insights:**\n",
        "\n",
        "#### 1. **Electric bikes are the dominant preference:**\n",
        "\n",
        "* **Members** show a strong preference for **electric bikes** (â‰ˆ7,500 trips), much higher than classic bikes.\n",
        "* Even **casual users** prefer electric bikes (â‰ˆ3,700 trips) over classic ones.\n",
        "\n",
        "#### 2. **Membership significantly boosts usage:**\n",
        "\n",
        "* Both bike types have **higher trip counts among members** than casual users.\n",
        "* Members took nearly **double the trips** compared to casuals, suggesting:\n",
        "\n",
        "  * Regular commuting behavior\n",
        "  * Higher cost-effectiveness for frequent riders\n",
        "  * Greater engagement with the system\n",
        "\n",
        "#### 3. **Classic bikes are less used overall:**\n",
        "\n",
        "* Classic bikes saw fewer total trips for both user types, particularly from casual riders, possibly due to:\n",
        "\n",
        "  * Higher effort required\n",
        "  * Less appeal in convenience and speed\n",
        "\n",
        "**Implication:**\n",
        "\n",
        "* The system should consider **prioritizing electric bike availability**, especially in high-traffic stations and for members.\n",
        "* **Member-focused incentives** or **electric bike maintenance** should be top priorities to support usage trends.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Sunburst Chart for Top 5 Starting Stations\n",
        "\n",
        "### Objective\n",
        "Create a sunburst chart showing the breakdown of trips for the top 5 starting stations with the hierarchy: Station â†’ Bike Type â†’ Membership Type. This will show how trips are distributed across bike types and membership types for each top station.\n",
        "\n",
        "### Requirements\n",
        "- Create a sunburst chart showing the breakdown of trips for the top 5 starting stations\n",
        "- Show the hierarchy: Station â†’ Bike Type â†’ Membership Type\n",
        "- Display how trips are distributed across bike types and membership types for each top station\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Filter data for only the top 5 starting stations\n",
        "trips_top5_stations = sampled_df[sampled_df['start_station_name'].isin(top_5_start_stations.index)].copy()\n",
        "\n",
        "# Group data for the sunburst chart\n",
        "sunburst_data = trips_top5_stations.groupby(['start_station_name', 'rideable_type', 'member_casual']).size().reset_index(name='count')\n",
        "\n",
        "# Create the sunburst chart\n",
        "fig_sunburst = px.sunburst(\n",
        "    sunburst_data,\n",
        "    path=['start_station_name', 'rideable_type', 'member_casual'],  # Hierarchy\n",
        "    values='count',\n",
        "    title='Trip Breakdown for Top 5 Starting Stations by Bike and Membership Type'\n",
        ")\n",
        "\n",
        "# Update layout for better appearance\n",
        "fig_sunburst.update_layout(\n",
        "    margin=dict(t=0, l=0, r=0, b=0),\n",
        "    title_text='Trip Breakdown for Top 5 Starting Stations by Bike and Membership Type',\n",
        "    title_x=0.5 # Center the title\n",
        ")\n",
        "\n",
        "fig_sunburst.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trip Breakdown by Start Station, Bike Type, and Membership\n",
        "\n",
        "The sunburst chart provides a **multidimensional view** of trip distribution across three variables:\n",
        "\n",
        "1. **Starting Station**\n",
        "2. **Bike Type (electric or classic)**\n",
        "3. **Membership Type (casual or member)**\n",
        "\n",
        "#### **Key Observations:**\n",
        "\n",
        "1. **Electric Bikes Dominate at Most Stations**\n",
        "\n",
        "   * At nearly every station, **electric bikes** occupy the largest segment.\n",
        "   * Most **member** trips originate using electric bikes, indicating a clear usage preference.\n",
        "\n",
        "2. **Park Rd & Holmead Pl NW Leads in Total Trips**\n",
        "\n",
        "   * This station has the **widest outer ring**, especially in electric bike usage by members.\n",
        "   * Indicates it is a **strategic hub**â€”possibly due to location, connectivity, or accessibility.\n",
        "\n",
        "3. **Membership-Driven Usage**\n",
        "\n",
        "   * Across all stations, **member users consistently take more trips** than casual users.\n",
        "   * Reflects a higher engagement and potential for loyalty-driven programs.\n",
        "\n",
        "4. **Small Classic Bike Usage by Casuals**\n",
        "\n",
        "   * In many stations (e.g., 14th & Belmont St NW), classic bikes used by casuals represent a **very thin slice**.\n",
        "   * Suggests low attractiveness of classic bikes to non-regular riders.\n",
        "\n",
        "5. **Columbus Circle / Union Station: Balanced Mix**\n",
        "\n",
        "   * This station shows a relatively **balanced use** of both bike types across membership types.\n",
        "   * Indicates it serves diverse rider profilesâ€”possibly due to being a major transit point.\n",
        "   \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Station Capacity Analysis\n",
        "\n",
        "### Objective\n",
        "Create a histogram showing the distribution of station capacities, then create a bar chart showing trip distribution by station capacity categories (small, medium, large). Stations need to be categorized into capacity groups first.\n",
        "\n",
        "### Requirements\n",
        "- Create a histogram showing the distribution of station capacities\n",
        "- Create a bar chart showing trip distribution by station capacity categories (small, medium, large)\n",
        "- Categorize stations into capacity groups first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Merge station size back to trips_df\n",
        "# Ensure station_id types match\n",
        "stations_df['STATION_ID'] = stations_df['STATION_ID'].astype(sampled_df['start_station_id'].dtype)\n",
        "\n",
        "# Merge station size to trips_df based on start station\n",
        "trips_with_station_size = sampled_df.merge(\n",
        "    stations_df[['STATION_ID', 'STATION_SIZE']],\n",
        "    left_on='start_station_id',\n",
        "    right_on='STATION_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Rename column for clarity\n",
        "trips_with_station_size.rename(columns={'STATION_SIZE': 'start_station_size'}, inplace=True)\n",
        "\n",
        "# Merge station size based on end station as well\n",
        "trips_with_station_size = trips_with_station_size.merge(\n",
        "    stations_df[['STATION_ID', 'STATION_SIZE']],\n",
        "    left_on='end_station_id',\n",
        "    right_on='STATION_ID',\n",
        "    how='left',\n",
        "    suffixes=('', '_end_station') # Add suffix for the end station size column\n",
        ")\n",
        "trips_with_station_size.rename(columns={'STATION_SIZE': 'end_station_size'}, inplace=True)\n",
        "\n",
        "# Drop the redundant STATION_ID columns from the merges\n",
        "trips_with_station_size.drop(columns=['STATION_ID', 'STATION_ID_end_station'], errors='ignore', inplace=True)\n",
        "\n",
        "\n",
        "# Count trips by start station capacity category\n",
        "trip_distribution_by_capacity = trips_with_station_size['start_station_size'].value_counts().reset_index()\n",
        "trip_distribution_by_capacity.columns = ['Station Capacity Category', 'Number of Trips']\n",
        "\n",
        "# Define category order\n",
        "category_order = ['Small', 'Average', 'Large']\n",
        "trip_distribution_by_capacity['Station Capacity Category'] = pd.Categorical(\n",
        "    trip_distribution_by_capacity['Station Capacity Category'], categories=category_order, ordered=True\n",
        ")\n",
        "trip_distribution_by_capacity = trip_distribution_by_capacity.sort_values('Station Capacity Category')\n",
        "\n",
        "\n",
        "# Create bar chart\n",
        "fig = px.bar(\n",
        "    trip_distribution_by_capacity,\n",
        "    x='Station Capacity Category',\n",
        "    y='Number of Trips',\n",
        "    title='Trip Distribution by Start Station Capacity Category',\n",
        "    labels={'Station Capacity Category': 'Start Station Size', 'Number of Trips': 'Number of Trips'},\n",
        "    color='Station Capacity Category',\n",
        "    category_orders={'Station Capacity Category': category_order} # Enforce the order\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_title='Start Station Capacity Category', yaxis_title='Number of Trips')\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trip Distribution by Start Station Capacity Category\n",
        "\n",
        "This bar chart displays the **number of trips** originating from stations grouped by their **capacity size**:\n",
        "\n",
        "* **Small**\n",
        "* **Average**\n",
        "* **Large**\n",
        "\n",
        "#### **Key Insights:**\n",
        "\n",
        "1. **Average-Capacity Stations Lead in Usage**\n",
        "\n",
        "   * With nearly **700,000 trips**, average-size stations dominate trip origination.\n",
        "   * Indicates a **sweet spot** in terms of infrastructure: large enough to support volume, yet compact enough to be conveniently located.\n",
        "\n",
        "2. **Small Stations Show Strong Engagement**\n",
        "\n",
        "   * Surprisingly, **small stations** come second, with over **350,000 trips**.\n",
        "   * Suggests good utilization of micro-mobility infrastructure, possibly in high-density areas or residential zones.\n",
        "\n",
        "3. **Underperformance of Large Stations**\n",
        "\n",
        "   * Large-capacity stations contribute the **least number of trips (\\~160,000)**.\n",
        "   * This could indicate:\n",
        "\n",
        "     * Poor placement (e.g., less foot traffic)\n",
        "     * Oversupply of docks\n",
        "     * Underutilized transit hubs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4GFM4egUk3o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# B)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK_yKo3mUtU4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task 1\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_mATqblU0nY"
      },
      "source": [
        "| Method                     | Formula                         | Notes                               |\n",
        "| -------------------------- | ------------------------------- | ----------------------------------- |\n",
        "| **Sturgesâ€™ Rule**          | `bins = ceil(log2(n) + 1)`      | Good for small to medium-sized data |\n",
        "| **Freedmanâ€“Diaconis Rule** | `bin_width = 2 * IQR / n^(1/3)` | Good for skewed data or outliers    |\n",
        "| **Square Root Rule**       | `bins = sqrt(n)`                | Simple and often a good baseline    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8HlhefNL3gb"
      },
      "outputs": [],
      "source": [
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedmanâ€“Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "# Static histogram\n",
        "fig = go.Figure(\n",
        "    data=[go.Histogram(\n",
        "        x=durations,\n",
        "        nbinsx=bin_count,\n",
        "        marker_color='blue',\n",
        "        opacity=1.0\n",
        "    )]\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Distribution of Trip Duration (in Minutes)\",\n",
        "    xaxis_title=\"Trip Duration (minutes)\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    bargap=0.05,\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG-L58E8ESel"
      },
      "source": [
        "How many trips took more then a day ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88-AjkyzEU6m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Choose your cutoff (in minutes)\n",
        "cutoff = 1440  # Modify as needed\n",
        "\n",
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedmanâ€“Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "\n",
        "# Count how many trips exceed the cutoff\n",
        "sampled_exceed = (sampled_df['trip_duration_minutes'] > cutoff).sum()\n",
        "full_exceed = (trips_df['trip_duration_minutes'] > cutoff).sum()\n",
        "\n",
        "print(f\"Trips in sampled_df exceeding {cutoff} minutes: {sampled_exceed}\")\n",
        "print(f\"Trips in trips_df exceeding {cutoff} minutes: {full_exceed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36wNkRedY3sL"
      },
      "source": [
        "insights :\n",
        "1. The massive bar near 0-20 minutes clearly shows that most bike trips are very short. This is typical for bike-sharing systems, often used for short commutes or quick errands.\n",
        "2. The presence of bars, even if very short, extending all the way to 1440 minutes shoes that some trips in the data did take more then a day , the number of trips is 361\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78CKfIqMzVr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCw3707wXM-6"
      },
      "outputs": [],
      "source": [
        "# Use the original (not divided) trip durations\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "types = sampled_df['rideable_type']\n",
        "\n",
        "# Build the box plot grouped by rideable_type\n",
        "fig = go.Figure()\n",
        "\n",
        "# Loop through each rideable type and add a box\n",
        "for bike_type in sampled_df['rideable_type'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['rideable_type'] == bike_type]['trip_duration_minutes'],\n",
        "        name=bike_type,\n",
        "        boxpoints='outliers',  # show outliers only\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Rideable Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Rideable Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlLYvhqMbi93"
      },
      "source": [
        "insights :\n",
        "1. both types show a very compact box near 20 minutes, indicating that the vast majority of trips for both bike types are quite short.\n",
        "2. The median line is very close to the bottom of the box, confirming heavy right-skewness, this means almost all of the middle 50% of data is concentrated very close to the lower end, and the remaining data (up to Q3) is more spread out.\n",
        "3. The green dots above the whiskers clearly represent the longer  trips with some of them above the 1440 line\n",
        "4. Electric bikes seem to have a slightly tighter distribution,This suggests that while both have short typical trips, classic bikes might have a slightly wider range of trips durations\n",
        "5. Both bike types exhibit very long duration \"outliers,\" with classic bikes potentially having more extreme longer-duration outliers , It suggests that while electric bikes facilitate shorter, perhaps faster trips, classic bikes are used for the most extended journeys.\n",
        "This could be due to factors like cost , battery limits, or simply user preference ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ5QqIOIM4mz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1ZEPTa6MwH-"
      },
      "outputs": [],
      "source": [
        "trips_df['member_casual'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpLjgXVjMiwf"
      },
      "outputs": [],
      "source": [
        "urations = sampled_df['trip_duration_minutes']\n",
        "types_to_group_by = sampled_df['member_casual']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for member_type in sampled_df['member_casual'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['member_casual'] == member_type]['trip_duration_minutes'],\n",
        "        name=member_type,\n",
        "        boxpoints='outliers',\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Member Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Member Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MadmH7TDdwd1"
      },
      "source": [
        "insights :\n",
        "1. As with the previous duration plots, both \"casual\" and \"member\" trips show an incredibly strong right-skewness. The box for both categories is extremely compact and squashed at the very bottom of the plot (close to 0 minutes), and the median line is practically on top of the first quartile (Q1). This reinforces that the vast majority of trips for both casual and member users are very short.\n",
        "2. While both are low, the median (and Q1/Q3) for 'casual' riders appears slightly higher (or at least, the box is marginally less squashed) than for 'member' riders. This suggests that a \"typical\" trip for a casual user is slightly longer than for a member, even if both are still relatively short.\n",
        "3. Both categories clearly have a significant number of \"outlier\" trips (the green dots) that extend far beyond the main box and whiskers, indicating that very long trips do occur for both types of users.\n",
        "4. The box (IQR) for 'casual' riders seems marginally wider than for 'member' riders,The whiskers for 'casual' riders also appear slightly longer, This implies casual riders exhibit a greater range of typical trip durations compared to members.\n",
        "5. 'Casual' riders have a significantly higher density of very long outlier trips, 'Member' riders also have long outlier trips, but they are fewer in number and generally do not reach the same extreme durations as those of casual riders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0wC5GyObKO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFx42HGM6BK"
      },
      "outputs": [],
      "source": [
        "# Counting Trips Longer Than One Day\n",
        "# Define threshold: 1 day = 1440 minutes\n",
        "one_day_minutes = 1440\n",
        "# Filter trips longer than 1 day\n",
        "long_trips_df = trips_df[trips_df['trip_duration_minutes'] > one_day_minutes]\n",
        "long_sampled_df = sampled_df[sampled_df['trip_duration_minutes'] > one_day_minutes]\n",
        "# Show how many there are\n",
        "print(f\"Total number of trips longer than 1 day in full data: {len(long_trips_df)}\")\n",
        "print(f\"Total number of trips longer than 1 day in sampled data: {len(long_sampled_df)}\")\n",
        "# Combine start and end station counts for long trips\n",
        "start_counts = long_trips_df['start_station_id'].value_counts()\n",
        "end_counts = long_trips_df['end_station_id'].value_counts()\n",
        "\n",
        "# Combine them into a single Series\n",
        "total_counts = start_counts.add(end_counts, fill_value=0).astype(int)\n",
        "\n",
        "# Get station info: name and location\n",
        "stations = sampled_df[['start_station_id', 'start_station_name', 'start_lat', 'start_lng']].drop_duplicates()\n",
        "stations = stations.rename(columns={\n",
        "    'start_station_id': 'station_id',\n",
        "    'start_station_name': 'station_name',\n",
        "    'start_lat': 'lat',\n",
        "    'start_lng': 'lng'\n",
        "})\n",
        "\n",
        "# Merge with counts\n",
        "stations['long_trip_count'] = stations['station_id'].map(total_counts).fillna(0).astype(int)\n",
        "\n",
        "# Filter stations with at least 1 long trip\n",
        "stations = stations[stations['long_trip_count'] > 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVJdaV5bO0sH"
      },
      "outputs": [],
      "source": [
        "# Center the map on Washington DC\n",
        "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Optional: cluster points\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add stations to the map\n",
        "for _, row in stations.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['lat'], row['lng']],\n",
        "        radius=3 + row['long_trip_count']**0.5,  # scale marker size\n",
        "        color='darkred',\n",
        "        fill=True,\n",
        "        fill_color='crimson',\n",
        "        fill_opacity=0.7,\n",
        "        popup=f\"{row['station_name']}<br>Trips > 1 day: {row['long_trip_count']}\"\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sab-rkVxVoJ"
      },
      "source": [
        "insights :\n",
        "1.  primary observation is that stations associated with long-duration trips are particularly concentrated around the central part WDC map ,This suggests that while very long trips are rare overall, they are not uniformly distributed but rather originate from or end in specific zones.\n",
        "2. The 2496 is the count of unique station IDs that appear as either a start_station_id OR an end_station_id within those 361 trips.\n",
        "This is a significant number of stations involved, considering that the total number of such trips was only 361. This means these long trips are spread across a wide variety of stations, rather than being concentrated at just a few specific locations.\n",
        "3. This highlights areas where the system might need to adapt to different user behaviors like offering specific \"long-term rental\" options or different pricing for these stations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehZahS1cJPcY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# C)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task1\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(sampled_df['trip_cost'].unique())\n",
        "\n",
        "trips_df.columns\n",
        "len(sampled_df[sampled_df['start_month'] != sampled_df['end_month']])\n",
        "sampled_df['start_time'] = pd.to_datetime(sampled_df['start_time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# cost Histogram\n",
        "fig = px.histogram(sampled_df, x='trip_cost', nbins=141, title='distrupation of trips cost')\n",
        "fig.show()\n",
        "\n",
        "# cost Boxplot\n",
        "fig = px.box(sampled_df, y='trip_cost', title='Boxplot of trips cost')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Ù†Ù„Ø§Ø­Ø¸ Ø§Ù† Ø§ØºÙ„Ø¨ Ø§Ù„Ø¯Ø§ØªØ§ Ù…ØªÙˆØ²Ø¹Ø© Ø¨ÙŠÙ† Ø§Ù„0 - ÙˆØ§Ù„10 Ø¯ÙˆÙ„Ø§Ø± Ø¨ÙƒØ«Ø±Ø© ÙˆØ§Ù† Ø§Ù„Ù‚Ù…Ø© Ø¨ÙŠÙ† 3.5 Ùˆ4 ÙˆÙ‡Ø°Ø§ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ù†Ù‡ ÙŠÙˆØ¬Ø¯ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù†Ø§Ø³ Ù…Ø´ØªØ±ÙƒØ© ÙˆØ§ØºÙ„Ø¨ Ø§Ù„Ø±Ø­Ù„ Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² Ø§Ù„45 Ø¯Ù‚ÙŠÙ‚Ø©\n",
        "\n",
        "- ÙˆØ§ÙŠØ¶Ø§ ÙŠÙˆØ¬Ø¯ Ù‚ÙŠÙ… Ø§ÙƒØ¨Ø± ØµØ­ÙŠØ­ Ø§Ù†Ù‡Ø§ Ù†Ø§Ø¯Ø±Ø© ÙˆÙ„ÙƒÙ†Ù‡Ø§ Ù…ØªÙˆØ²Ø¹Ø© ÙˆÙ‡Ø°Ø§ ÙŠØ¯Ù„ Ø§Ù†Ù‡ ÙŠÙˆØ¬Ø¯ Ø§Ø´Ø®Ø§Øµ ØªØ§Ø®Ø°Ù‡Ø§ Ù„Ù…Ø³Ø§ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© ÙˆÙ„ÙƒÙ†Ù‡Ø§ Ù‚Ù„ÙŠÙ„Ø©  \n",
        "- ØºØ§Ù„Ø¨Ø§ Ø§Ù„Ø±Ø­Ù„ Ø°Ø§Øª ØªÙƒÙ„ÙØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© Ø§Ø´Ø®Ø§Øµ ØºÙŠØ± Ù…Ø´ØªØ±ÙƒÙŠÙ† Ø¨Ø§Ù„Ø§Ø¶Ø§ÙØ© Ø§Ù„Ù‰ Ø§Ù†Ù‡Ù… Ù‚Ø¯ ÙŠÙƒÙˆÙ†ÙˆÙ† Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· ÙŠØ³ØªØ®Ø¯Ù…ÙˆÙ† Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª ÙˆÙ„Ø§ ÙŠØ¹ÙˆØ¯ÙˆÙ† Ø§Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø®Ø¯Ù…Ø© ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ø³Ø¹Ø±\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter(sampled_df, x='trip_duration_minutes', y='trip_cost', trendline='ols',title='the realtion between duration and cost')\n",
        "fig.show()\n",
        "# lowess', 'rolling', 'ewm', 'expanding', 'ols'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "*   Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙŠ Ù‚ÙŠÙ…ØªÙ‡Ø§ Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„ØµÙØ± ÙƒÙˆÙ‚Øª Ù‡ÙŠ ØªÙ…Ø«Ù„ Ø§Ù„Ø§Ø¹Ø¶Ø§Ø¡ Ø§Ù„ØªÙŠ Ù„Ø¯ÙŠÙ‡Ù… Ø§Ø´ØªØ±Ø§Ùƒ ÙˆÙ„Ù… ÙŠØªØ¬Ø§ÙˆØ²ÙˆØ§ Ø§Ù„45 Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙƒÙ…Ø§ Ù†Ù„Ø§Ø­Ø¸ Ù‡Ù… ÙƒØ«Ø±\n",
        "*   ÙˆÙ„Ø¯ÙŠÙ†Ø§ Ø«Ù„Ø§Ø« ØªÙˆØ²Ø¹Ø§Øª Ù„Ù„Ù†Ù‚Ø§Ø· ÙˆØ°Ù„Ùƒ ÙŠØ¹ÙˆØ¯ Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø§Ø´ØªØ±Ø§Ùƒ Ø§Ùˆ Ø¹Ø¯Ù…Ù‡ ÙˆØ­ØªÙ‰ Ù…Ø±ÙˆØ±Ù‡ Ø¨Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ©\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = px.scatter(sampled_df, x='temp', y='trip_cost', color='member_casual',\n",
        "                 title='cost vs temperatur ')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "*   Ø§ØºÙ„Ø¨ Ø§Ù„Ø±Ø­Ù„Ø§Øª ØªÙƒÙˆÙ† Ø¨ÙŠÙ† 5 Ø¯Ø±Ø¬Ø§Øª ÙˆØ§Ù„20 Ø¯Ø±Ø¬Ø©\n",
        "*   Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© ÙÙˆÙ‚ Ø§Ù„20 Ù†Ù„Ø§Ø­Ø¸ Ø§Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø­Ù„Ø§Øª Ù‚Ù„ÙŠÙ„\n",
        "* ÙƒÙ…Ø§ Ù†Ù„Ø§Ø­Ø¸ Ø§ØºÙ„Ø¨ Ø±Ø­Ù„ Ø§Ù„Ù…Ø´ØªØ±ÙƒÙŠÙ† Ø§Ù„ÙƒÙ„ÙØ© ØºØ§Ù„Ø¨Ø§ Ø§Ù‚Ù„ Ù…Ù† 10 Ø¯ÙˆÙ„Ø§Ø±\n",
        "* Ù†Ù„Ø§Ø­Ø¸ Ø§Ù† Ø§ØºÙ„Ø¨ Ø§Ù„ÙƒÙ„Ù Ø§Ù„Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„ØºÙŠØ± Ø§Ù„Ù…Ø´ØªØ±ÙƒÙŠÙ†\n",
        "* Ù„Ø§ÙŠÙˆØ¬Ø¯ Ø¹Ù„Ø§Ù‚Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨ÙŠÙ† Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© ÙˆØ§Ù„ØªÙƒÙ„ÙØ© Ù„ÙƒÙ† ÙŠÙ…ÙƒÙ† Ø§Ù„ÙÙˆÙ„ Ø§Ù† Ø¨ÙŠÙ† Ø§Ù„ 5 -15 ÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ø§Ø³ Ø§Ù† ØªØ°Ù‡Ø¨ Ø¨Ø±Ø­Ù„Ø§Øª Ø£Ø·ÙˆÙ„\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "daily_rev = sampled_df.groupby(sampled_df['start_time'].dt.date)['trip_cost'].sum().reset_index(name='revenue')\n",
        "fig = px.line(daily_rev, x='start_time', y='revenue', title='daily incomes')\n",
        "fig.show()\n",
        "\n",
        "sampled_df['week'] = sampled_df['start_time'].dt.isocalendar().week\n",
        "weekly_rev = sampled_df.groupby('week')['trip_cost'].sum().reset_index(name='revenue')\n",
        "fig = px.line(weekly_rev, x='week', y='revenue', title='weekly incomes')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "*   Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„ÙŠÙˆÙ…Ø¨Ø© Ù†Ù„Ø§Ø­Ø¸ ÙˆØ¬ÙˆØ¯ Ø¨ÙŠÙ† Ù‡Ø¨ÙˆØ· ÙˆØµØ¹ÙˆØ¯ ÙˆÙ…Ø¹ Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„Ø§ÙŠØ§Ù… Ù†Ù„Ø§Ø­Ø¸ Ø²ÙŠØ§Ø¯Ø© Ø¨Ø§Ù„Ø¯Ø®Ù„ ÙˆÙ†Ù„Ø§Ø­Ø¸ ØªÙ†Ø§ÙˆØ¨ Ø¨ÙŠÙ† ØµØ¹ÙˆØ¯ ÙˆÙ‡Ø¨ÙˆØ· ÙÙŠ Ø§Ù„Ø§ÙŠØ§Ù… ÙˆÙŠØ¹ÙˆØ¯ Ù‡Ø°Ø§ Ø§Ù„Ø§Ù…Ø± Ø§ØªÙˆÙ‚Ø¹ Ø§Ù†Ùˆ Ø´Ø®Øµ ÙŠÙ„ÙŠ Ø¨ÙŠØ±ÙƒØ¨ ÙŠÙˆÙ… Ø¨Ø±ÙŠØ­ Ø§Ù„ÙŠÙˆÙ… ÙŠÙ„ÙŠ Ø¨Ø¹Ø¯Ùˆ\n",
        "\n",
        "*   Ù„Ø¯ÙŠÙ†Ø§ Ø¨Ø´Ù‡Ø± april Ù‡Ø¨ÙˆØ· ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø±Ø¨Ø­ Ø§Ù„Ø³Ø¨Ø¨ Ù‚Ø¯ ÙŠØ¹ÙˆØ¯ Ø§Ù„Ù‰ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¯Ø§ØªØ§ ÙƒØ§ÙÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø´Ù‡Ø±\n",
        "\n",
        "* Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø³Ø¨ÙˆØ¹ÙŠØ© Ù…Ù„Ø§Ø­Ø¸ Ø§Ù†Ù‡ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… Ø§Ù„Ø§Ù…ÙˆØ± Ù†Ø­Ùˆ Ø²ÙŠØ§Ø¯Ø© Ø­ÙŠØ« Ø§Ù† Ù‡Ø°Ø§ Ø§Ù„ØªØ°Ø¨Ø°Ø¨ Ø±Ø§Ø­ Ø¨Ø³Ø¨Ø¨ Ø§Ù†Ùˆ Ø§Ù„Ø§Ø³Ø¨ÙˆØ¹ÙŠ Ø¹Ø·Ø§Ù†Ø§ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¹Ø§Ù… Ø¨Ø§Ù„Ø§Ø³Ø¨ÙˆØ¹ ÙØ§ØµØ¨Ø­ Ø®Ø·  Ø§ÙƒØ«Ø± Ø§Ù†Ø³ÙŠØ§Ø¨ÙŠØ©\n",
        "\n",
        "* Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… ÙŠÙˆØ¬Ø¯ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø´Ù‡Ø± april\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task5\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "monthly_rev = sampled_df.groupby('start_month')['trip_cost'].mean().reset_index(name='avg_revenue')\n",
        "fig = px.line(monthly_rev, x='start_month', y='avg_revenue', title='average month income')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* ÙŠØ¨ÙŠÙ† Ù„Ù†Ø§ Ø§Ù„Ù…Ø®Ø·Ø· ØªØ·ÙˆØ± Ù…ØªÙˆØ³Ø· ØªÙƒÙ„ÙØ© Ø§Ù„Ø±Ø­Ù„Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø© Ø®Ù„Ø§Ù„ Ø´Ù‡Ø± Ø§Ù„Ø§ÙˆÙ„ ÙƒØ§Ù† Ù…ØªÙˆØ³Ø· ØªÙƒÙ„ÙØ© Ø§Ù„Ø±Ø­Ù„Ø© Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 3.78 Ø¯ÙˆÙ„Ø§Ø± Ù…Ø¹ Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø´Ù‡Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ Ù†Ù„Ø§Ø­Ø¸ Ø§Ø±ØªÙØ§Ø¹ Ø·ÙÙŠÙ ÙˆÙŠØ³ØªÙ…Ø± Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ Ø¨Ø´ÙƒÙ„ Ø·ÙÙŠÙ Ø­ØªÙ‰ Ø§Ù„Ø´Ù‡Ø± Ø§Ù„Ø«Ø§Ù„Ø« Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…Ùˆ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ ÙŠÙˆØ­ÙŠ Ø¨Ø§Ù† Ø´ÙŠØ¦Ø§Ù‹ Ù…Ø§ ÙƒØ§Ù† ÙŠØªØºÙŠØ± Ø¨Ø¨Ø·Ø¡ ÙˆØ«Ø¨Ø§Øª Ø±Ø¨Ù…Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙˆÙ† ÙŠÙ…ÙŠÙ„ÙˆÙ† Ù„Ø£Ø®Ø° Ø±Ø­Ù„Ø§Øª Ø£Ø·ÙˆÙ„ Ù‚Ù„ÙŠÙ„Ù‹Ø§ØŒ Ø£Ùˆ Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø²ÙŠØ§Ø¯Ø© Ø·ÙÙŠÙØ© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª Ø°Ø§Øª Ø§Ù„ØªÙƒÙ„ÙØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ Ø£Ùˆ Ø±Ø¨Ù…Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ØªØ²Ø§ÙŠØ¯ ÙÙŠ Ø§Ù„Ø±Ø­Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ®Ø·Ù‰ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ© Ù„Ù„Ù…Ø´ØªØ±ÙƒÙŠÙ† ÙˆØªØªØ­Ù…Ù„ Ø±Ø³ÙˆÙ…Ù‹Ø§ Ø¥Ø¶Ø§ÙÙŠØ©. Ù‡Ø°Ù‡ Ø§Ù„Ø²ÙŠØ§Ø¯Ø©ØŒ ÙˆØ¥Ù† ÙƒØ§Ù†Øª ØµØºÙŠØ±Ø©ØŒ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† Ù‚ÙŠÙ…Ø© Ø§Ù„Ø±Ø­Ù„Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø© ÙƒØ§Ù†Øª ÙÙŠ Ø§Ø²Ø¯ÙŠØ§Ø¯\n",
        "* Ø«Ù… Ù†ØµÙ„ Ø§Ù„Ù‰ Ø´Ù‡Ø± Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù†Ù„Ø§Ø­Ø¸ Ù‚ÙØ²Ø© ÙÙŠ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø±Ø­Ù„Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù„Ø­ÙˆØ¸ Ø­ÙŠØ« ÙˆØµÙ„ Ø§Ù„4 Ø¯ÙˆÙ„Ø§Ø± Ù…Ø³Ø¬Ù„ Ø§Ø¹Ù„Ù‰ Ù…ØªÙˆØ³Ø· Ø®Ù„Ø§Ù„ Ù‡Ø°Ù‡ Ø§Ù„ÙØªØ±Ø© Ù‚Ø¯ ÙŠØ¨Ø¯Ùˆ Ù„Ù„Ø­Ø¸Ø© Ø§Ù† Ø§Ù„Ø§Ù…Ø± Ø¬ÙŠØ¯ ÙˆÙ„ÙƒÙ† Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø± Ø§Ù„Ù‰ Ù…Ø®Ø·Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ ÙˆØ§Ù„Ø§Ø³Ø¨ÙˆØ¹ÙŠ ÙÙ†Ø­Ø¯Ø¯ Ø´Ù‡Ø¯Ù†Ø§ Ù‡Ø¨ÙˆØ· ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø´Ù‡Ø± ÙˆÙ‚Ø¯ ÙŠØ¹ÙˆØ¯ Ø³Ø¨Ø¨ Ø§Ù„Ù‡Ø¨ÙˆØ· ÙÙŠ Ø±ÙØ¹ Ø³Ø¹Ø± Ø§Ù„Ø±Ø­Ù„Ø© Ù…Ù…Ø§ Ø§Ø¯Ù‰ Ø§Ù„Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù†Ù‡ÙŠØ§Ø±Ø§Ù‹ ÙƒØ§Ø±Ø«ÙŠØ§Ù‹\n",
        "\n",
        "* ÙˆØ§ÙŠØ¶Ø§ Ù…Ù…ÙƒÙ† Ù‡Ø°Ø§ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ Ø§ØªÙ‰ Ø¨Ù…Ø§ Ø§Ù†Ù‡ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø­Ù„Ø§Øª Ø§Ù„Ø§Ø¬Ù…Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø´Ù‡Ø± Ø§Ù„Ø±Ø§Ø¨Ø¹ Ù‚Ù„ÙŠÙ„Ø© ÙÙˆØ¬ÙˆØ¯ Ù‚ÙŠÙ… Ø´Ø§Ø°Ø© Ø§Ùˆ Ù…Ø±ØªÙØ¹Ø© ÙƒÙ…Ø§ Ø´Ù‡Ø¯Ù†Ø§ ÙÙŠ Ù…Ø®Ø·Ø· ÙƒÙ„Ù Ø§Ù„Ø±Ø­Ù„ Ø³ÙŠØ±ÙØ¹ Ù…ØªÙˆØ³Ø· ÙƒÙ„ÙØ© Ø§Ù„Ø±Ø­Ù„Ø© Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Ø§Ù„Ø®Ù„Ø§ØµØ©**\n",
        "\n",
        "\n",
        "*   ÙƒØ§Ù†Øª Ø®Ø¯Ù…Ø© Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª ØªØ´Ù‡Ø¯ Ù†Ù…ÙˆÙ‹Ø§ Ù…Ø³ØªÙ…Ø±Ù‹Ø§ ÙÙŠ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¥ÙŠØ±Ø§Ø¯Ø§ØªÙ‡Ø§ ÙˆÙÙŠ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø±Ø­Ù„Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø© Ù…Ù† ÙŠÙ†Ø§ÙŠØ± ÙˆØ­ØªÙ‰ Ù…Ù†ØªØµÙ Ù…Ø§Ø±Ø³.\n",
        "*   Ù…Ø¹ Ø°Ù„ÙƒØŒ ÙÙŠ Ø£ÙˆØ§Ø®Ø± Ù…Ø§Ø±Ø³/Ø£ÙˆØ§Ø¦Ù„ Ø£Ø¨Ø±ÙŠÙ„ØŒ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø´Ø±ÙƒØ© ØªØ¹Ø±Ø¶Øª Ù„Ø­Ø¯Ø« Ø¬Ø³ÙŠÙ… (Ø¥Ù…Ø§ Ø¥ØºÙ„Ø§Ù‚ØŒ Ø£Ùˆ ØªØ¹Ù„ÙŠÙ‚ØŒ Ø£Ùˆ Ø¹Ø·Ù„ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…) Ø£Ø¯Ù‰ Ø¥Ù„Ù‰ ØªÙˆÙ‚Ù Ø´Ø¨Ù‡ ÙƒØ§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø­Ù„Ø§Øª ÙˆØ§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§Øª, Ø§Ùˆ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¨Ø³Ø¨Ø¨ Ø±ÙØ¹ Ø±Ø³ÙˆÙ… Ø§Ù„Ø±Ø­Ù„Ø©\n",
        "* Ø§Ù„Ù‚ÙØ²Ø© ÙÙŠ Ù…ØªÙˆØ³Ø· ØªÙƒÙ„ÙØ© Ø§Ù„Ø±Ø­Ù„Ø© ÙÙŠ Ø£Ø¨Ø±ÙŠÙ„ØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù‡Ø§ ØªØ¨Ø¯Ùˆ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø¨Ù…ÙØ±Ø¯Ù‡ØŒ Ù‡ÙŠ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ù…Ø¬Ø±Ø¯ Ø§Ù†Ø¹ÙƒØ§Ø³ Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø£Ù† Ø§Ù„Ø±Ø­Ù„Ø§Øª Ø§Ù„Ù‚Ù„ÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© ÙƒØ§Ù†Øª Ù‡ÙŠ Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒÙ„ÙØ©ØŒ Ù…Ù…Ø§ ÙŠÙ„Ù‚ÙŠ Ø§Ù„Ø¶ÙˆØ¡ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ÙƒØ§Ø±Ø«ÙŠ Ù„Ù„Ø®Ø¯Ù…Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø´Ù‡Ø±.\n",
        "\n",
        "* Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø³Ø¨Ø¨ Ø§Ø®Ø° Ù‚Ø±Ø§Ø± Ø§Ù„Ø´Ø±ÙƒØ© Ø¨Ø±ÙØ¹ Ø§Ù†Ù‡Ø§ ÙƒØ§Ù†Øª ØªØ­Ø§ÙˆÙ„ Ø±ÙØ¹ Ø§Ù„Ø±Ø³ÙˆÙ… ÙÙŠ Ø§Ù„Ø§Ø´Ù‡Ø± Ø§Ù„Ø§ÙˆÙ„Ù‰ ÙˆÙ„ÙƒÙ† Ø¨Ø´ÙƒÙ„ Ø·ÙÙŠÙ ÙˆØ¹Ù†Ø¯Ù…Ø§ ÙˆØ¬Ø¯Øª Ø§Ù† Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª ØªØ²Ø§Ø¯ Ù‚Ø§Ù…Øª Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø±ÙØ¹Ø© Ø¸Ù†Ø§ Ù…Ù†Ù‡Ø§ Ø§Ù†Ù‡ Ø§ØµØ¨Ø­ Ù„Ø¯ÙŠÙ‡Ø§ Ù‚Ø§Ø¹Ø¯Ø© Ø¬Ù…Ø§Ù‡ÙŠØ±ÙŠØ© ÙƒØ¨ÙŠØ±Ø© ÙˆØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ø§Ø²Ø¯ÙŠØ§Ø¯ Ù„ØªÙØ§Ø¬Ø¦ Ø¨Ø­ØµÙˆÙ„ Ø¹ÙƒØ³ Ø°Ù„Ùƒ ØªÙ…Ø§Ù…Ø§\n",
        "* ÙƒÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ù…ÙˆØ± Ù‡ÙŠ Ù…Ø¬Ø±Ø¯ ØªÙØ³ÙŠØ±Ø§Øª Ù…Ù…ÙƒÙ†Ø©\n",
        "\n",
        "* Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø³Ø¨Ø¨ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ Ù‡Ùˆ ÙˆØ¬ÙˆØ¯ ØªØ¶Ø®Ù…\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOP_nkSnJTVM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# D)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWjBMMLQz1C"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task1\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnB_ydw33K-z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- (Your existing data preparation for res_points) ---\n",
        "# Assuming 'sampled_df' and 'Residential_Visitor_Parking_Zones' are loaded\n",
        "\n",
        "# Step 0: Load residential zones GeoDataFrame\n",
        "res_zones = Residential_Visitor_Parking_Zones\n",
        "res_zones = res_zones.to_crs(epsg=4326)\n",
        "\n",
        "# Step 1: Create GeoDataFrames for start and end points (using sampled_df)\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['start_lng'], sampled_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['end_lng'], sampled_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "# Step 2: Spatial join to check which points fall inside residential zones\n",
        "start_in_res = gpd.sjoin(start_gdf, res_zones, predicate='within', how='inner')\n",
        "end_in_res = gpd.sjoin(end_gdf, res_zones, predicate='within', how='inner')\n",
        "\n",
        "# Step 3: Extract lat/lon of trips touching residential zones\n",
        "res_start_points = start_in_res[['start_lat', 'start_lng']].rename(columns={'start_lat': 'lat', 'start_lng': 'lon'})\n",
        "res_end_points = end_in_res[['end_lat', 'end_lng']].rename(columns={'end_lat': 'lat', 'end_lng': 'lon'})\n",
        "res_points = pd.concat([res_start_points, res_end_points], ignore_index=True)\n",
        "\n",
        "# --- Load CBD Data and ensure correct CRS (EPSG:4326) ---\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=4326) # Ensure CBD is also in EPSG:4326\n",
        "\n",
        "# --- Create the Plotly Graph Object Figure ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the Heatmap Trace\n",
        "fig.add_trace(go.Densitymapbox(\n",
        "    lat=res_points['lat'],\n",
        "    lon=res_points['lon'],\n",
        "    radius=10, # Adjust radius as needed for visual effect\n",
        "    colorscale=\"Viridis\", # Or \"Jet\", \"Hot\", \"Portland\", etc. for heatmap colors\n",
        "    hoverinfo=\"skip\"\n",
        "))\n",
        "\n",
        "# Add Residential Zones as a GeoJSON layer\n",
        "fig.update_layout(\n",
        "    mapbox_layers=[\n",
        "        {\n",
        "            \"below\": 'traces',\n",
        "            \"sourcetype\": \"geojson\",\n",
        "            \"source\": json.loads(res_zones.to_json()),\n",
        "            \"type\": \"line\",\n",
        "            # \"line\": {\"width\": 1, \"color\": \"blue\"} # Corrected: used \"color\" within \"line\" object\n",
        "        },\n",
        "        # Add CBD as another GeoJSON layer\n",
        "        {\n",
        "            \"below\": 'traces',\n",
        "            \"sourcetype\": \"geojson\",\n",
        "            \"source\": json.loads(CBD.to_json()),\n",
        "            \"type\": \"line\",\n",
        "            # \"line\": {\"width\": 2, \"color\": \"green\"} # Corrected: used \"color\" within \"line\" object\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Update layout for map style, center, zoom, and title\n",
        "fig.update_layout(\n",
        "    title_text='Geographic Heatmap of Trips to Residential Zones with Borders',\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    mapbox_zoom=10,\n",
        "    mapbox_center = {\"lat\": res_points['lat'].mean(), \"lon\": res_points['lon'].mean()},\n",
        "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
        ")\n",
        "\n",
        "# Render statically (Plotly handles this directly when fig.show() is called in a notebook)\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdhQF2Fi7lup"
      },
      "source": [
        "insights :\n",
        "1. The most prominent insight is the very high density (bright yellow areas) of bike trips that overwhelmingly coincides with and largely respects the boundaries of the residential zones.\n",
        " This indicates that the bike-sharing system is heavily used for purposes directly related to residential areas, whether starting from, ending in, or traversing through them.\n",
        "2. The highest concentration of trips within residential zones appears to be in the central and northern parts of Washington D.C., This suggests these are the most active residential areas for bike-sharing usage.\n",
        "3. While the overall pattern is dense,These could indicate areas with fewer stations, different demographic profiles, or less need for bike-sharing, which could be further explored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZTM_ZuT-zD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY_LY76_J5iH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Count trips per geohash sector\n",
        "geohash_counts = sampled_df['geohash_p6'].value_counts().reset_index()\n",
        "geohash_counts.columns = ['geohash_p6', 'trip_count']\n",
        "\n",
        "# Optional: sort alphabetically or by count\n",
        "geohash_counts = geohash_counts.sort_values(by='trip_count', ascending=False)\n",
        "\n",
        "# Step 2: Plot\n",
        "fig = px.bar(\n",
        "    geohash_counts,\n",
        "    x='geohash_p6',\n",
        "    y='trip_count',\n",
        "    title='Distribution of Trips by Geographic Sector (Geohash_p6)',\n",
        "    labels={'geohash_p6': 'Geographic Sector', 'trip_count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "# Step 3: Turn off interactivity\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEbLkSru_yc6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- Your Function to Convert Geohash to Polygon ---\n",
        "def geohash_to_polygon(geohash_str):\n",
        "    \"\"\"\n",
        "    Converts a geohash string to a shapely Polygon representing its bounding box.\n",
        "    \"\"\"\n",
        "    lat, lon, lat_err, lon_err = gh.decode_exactly(geohash_str)\n",
        "    min_lat = lat - lat_err\n",
        "    max_lat = lat + lat_err\n",
        "    min_lon = lon - lon_err\n",
        "    max_lon = lon + lon_err\n",
        "    coords = [\n",
        "        (min_lon, min_lat),\n",
        "        (max_lon, min_lat),\n",
        "        (max_lon, max_lat),\n",
        "        (min_lon, max_lat),\n",
        "        (min_lon, min_lat)  # Close the polygon\n",
        "    ]\n",
        "    return Polygon(coords)\n",
        "\n",
        "# --- Sample Input: Top 5 Geohashes (replace with your real data) ---\n",
        "top_5_geohashes = geohash_counts.head(5)['geohash_p6'].tolist()\n",
        "geohash_polygons = [geohash_to_polygon(g) for g in top_5_geohashes]\n",
        "\n",
        "# --- Convert to GeoDataFrame for easier plotting ---\n",
        "gdf = gpd.GeoDataFrame(geometry=geohash_polygons, crs='EPSG:4326')\n",
        "\n",
        "# --- Plot using Plotly ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add each polygon as a filled scattermapbox trace\n",
        "for poly in gdf.geometry:\n",
        "    lons, lats = poly.exterior.coords.xy\n",
        "    fig.add_trace(go.Scattermapbox(\n",
        "        lon=list(lons),\n",
        "        lat=list(lats),\n",
        "\n",
        "        mode='lines',\n",
        "        fill='toself',\n",
        "        fillcolor='rgba(255, 0, 0, 0.4)',  # Semi-transparent red\n",
        "        line=dict(width=1, color='red'),\n",
        "        hoverinfo='skip',\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "# Map settings\n",
        "fig.update_layout(\n",
        "    mapbox_style='carto-positron',\n",
        "    mapbox_zoom=12,\n",
        "    mapbox_center={\"lat\": gdf.geometry.centroid.y.mean(), \"lon\": gdf.geometry.centroid.x.mean()},\n",
        "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\n",
        ")\n",
        "\n",
        "# Static rendering (no interactions)\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE98VnXv-ww7"
      },
      "source": [
        "insights :\n",
        "1.  The first few bars on the left are significantly taller than the rest, with the tallest bar approaching 800 trips, while many others have less than 50. This indicates that bike-sharing activity is heavily concentrated in a few high-demand areas.\n",
        "2. the plot allows for quick identification of the highest-demand geographic sectors. These are likely to correspond to areas with high population density, major transit hubs, popular attractions, or dense commercial/office districts\n",
        "3. we ploted these top sectors and we compared it the heatmap in task1 , and we are now sure that they are in hotspots for trips in residential zone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ZXZV3XVa7n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbmejUVrdzbO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def freedman_diaconis_bins(data):\n",
        "    data = data.dropna()\n",
        "    q75, q25 = np.percentile(data, [75 ,25])\n",
        "    iqr = q75 - q25\n",
        "    n = len(data)\n",
        "    if n == 0 or iqr == 0:\n",
        "        return 10  # fallback if data is empty or IQR is zero\n",
        "    bin_width = 2 * iqr / (n ** (1/3))\n",
        "    if bin_width == 0:\n",
        "        return 10\n",
        "    bin_count = int(np.ceil((data.max() - data.min()) / bin_width))\n",
        "    print(\"Bin count : \", bin_count)\n",
        "    return bin_count\n",
        "\n",
        "\n",
        "# Calculate bins for each variable\n",
        "bins_cbd = freedman_diaconis_bins(sampled_df['distance_to_cbd_m'])\n",
        "bins_metro = freedman_diaconis_bins(sampled_df['start_nearest_metro_distance'])\n",
        "bins_shuttle = freedman_diaconis_bins(sampled_df['start_nearest_shuttle_distance'])\n",
        "\n",
        "sampled_df['distance_to_cbd_km'] = sampled_df['distance_to_cbd_m'] / 1000\n",
        "\n",
        "# 1. Distance to CBD\n",
        "fig1 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='distance_to_cbd_km',\n",
        "    nbins=bins_cbd,\n",
        "    title='Distribution of Distance to CBD (km)',\n",
        "    labels={'distance_to_cbd_km': 'Distance to CBD (km)'}\n",
        ")\n",
        "fig1.show(config={'staticPlot': True})\n",
        "\n",
        "# 2. Closest Metro Station Distance\n",
        "fig2 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_metro_distance',\n",
        "    nbins=bins_metro,\n",
        "    title='Distribution of Distance to Nearest Metro Station',\n",
        "    labels={'start_nearest_metro_distance': 'Distance to Metro (km)'}\n",
        ")\n",
        "fig2.show(config={'staticPlot': True})\n",
        "\n",
        "# 3. Closest Shuttle Station Distance\n",
        "fig3 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_shuttle_distance',\n",
        "    nbins=bins_shuttle,\n",
        "    title='Distribution of Distance to Nearest Shuttle Station',\n",
        "    labels={'start_nearest_shuttle_distance': 'Distance to Shuttle (km)'}\n",
        ")\n",
        "fig3.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B_a704MjIGH"
      },
      "source": [
        "insights :\n",
        "1. The overwhelming majority of trips (as indicated by the highest bars) occur within approximately 0 to 4 kilometers of the Central Business District. The peak frequency appears to be around 2.2 kilometers, This reinforces the CBD's role as a major hub for bike-sharing activity.\n",
        "2. this strongly supports the idea that the bike-sharing system primarily serves \"last-mile\" transportation, short-distance commutes, or intra-CBD movement.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. The histogram displays a highly skewed distribution, with an extremely high frequency of trips originating or ending very close to Metro stations. The counts drop off sharply as the distance increases.\n",
        "2. This insight highlights the importance of placing bike-sharing stations in very close proximity to Metro entrances to maximize their integration with the public transit network and serve commuter needs effectively.\n",
        "\n",
        "\n",
        "---\n",
        "1.  Similar to the Metro station plot, this histogram shows a highly skewed distribution, with the highest frequency of trips occurring very close to shuttle stations.\n",
        "2. This suggests a consistent user behavior of using bike-sharing to bridge short gaps to fixed public transport points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zxf96lZssHE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63xLkRdqrujr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Categorize trips\n",
        "def classify_trip(row):\n",
        "    if row['start_in_cbd'] == 1 and row['end_in_cbd'] == 1:\n",
        "        return 'Fully in CBD'\n",
        "    else:\n",
        "        return 'Outside CBD'\n",
        "\n",
        "# Apply classification\n",
        "sampled_df['cbd_trip_type'] = sampled_df.apply(classify_trip, axis=1)\n",
        "\n",
        "# Count\n",
        "trip_cbd_counts = sampled_df['cbd_trip_type'].value_counts().reset_index()\n",
        "trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    trip_cbd_counts,\n",
        "    x='Trip Type',\n",
        "    y='Count',\n",
        "    title='Trips Fully in CBD vs Outside',\n",
        "    text='Count',\n",
        "    labels={'Count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(yaxis_title='Number of Trips', xaxis_title='Trip Category')\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoWwMMOttvP4"
      },
      "outputs": [],
      "source": [
        "trips_df['cbd_trip_type'] = trips_df.apply(classify_trip, axis=1)\n",
        "full_trip_cbd_counts = trips_df['cbd_trip_type'].value_counts().reset_index()\n",
        "full_trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "full_trip_cbd_counts['Percentage'] = (full_trip_cbd_counts['Count'] / full_trip_cbd_counts['Count'].sum()) * 100\n",
        "full_trip_cbd_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEClq4S-lzUe"
      },
      "source": [
        "insights :\n",
        "1. The vast majority of trips  fall into the \"Outside CBD\" category, This indicates that the bike-sharing system primarily serves a broader geographical area beyond just the core CBD, or facilitates connections into/out of it rather than exclusively within it.\n",
        "3.  A significantly smaller number of trips (1,243 trips) were classified as \"Fully in CBD\". This suggests that while there is some intra-CBD movement, it represents a much smaller proportion\n",
        "4. This distribution implies that the bike-sharing service's primary function might not be solely for quick hops within the CBD itself, but rather for facilitating commutes, errands, or leisure travel that connects to or traverses the CBD from other parts of the city. This aligns with the \"first/last mile\" insights from the distance histograms, where trips are often connecting to major transit points or residential areas that might lie outside the strict CBD boundaries.\n",
        "5.  The fact that over 93% of all the trips involve areas outside the CBD strongly reinforces the idea that bike-sharing is primarily used as a \"first/last mile\" solution, connecting users to or from transit hubs (like Metro and Shuttle stations) or residential areas that often lie outside the strict CBD boundaries. These trips are unlikely to be exclusively within the CBD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbWLXpMNuCVn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task5\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re3aHjZmtmpv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter trips that passed through CBD\n",
        "cbd_passed_df = sampled_df[\n",
        "    (sampled_df['start_in_cbd'] == 1) | (sampled_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    grouped,\n",
        "    x='rideable_type',\n",
        "    y='trip_count',\n",
        "    color='member_casual',\n",
        "    barmode='group',\n",
        "    title='Trips That Passed Through CBD by Rideable Type and Membership',\n",
        "    labels={'trip_count': 'Number of Trips', 'rideable_type': 'Bike Type'}\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Rideable Type',\n",
        "    yaxis_title='Number of Trips'\n",
        ")\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE6Ah0JBuxCX"
      },
      "outputs": [],
      "source": [
        "cbd_passed_df_trips_df=trips_df[\n",
        "    (trips_df['start_in_cbd'] == 1) | (trips_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df_trips_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "print(f\"Length of cbd_passed_df_trips_df: {len(cbd_passed_df_trips_df)}\")\n",
        "print(f\"Length of trips_df: {len(trips_df)}\")\n",
        "\n",
        "percentage = (len(cbd_passed_df_trips_df) / len(trips_df)) * 100\n",
        "print(f\"Percentage of cbd_passed_df_trips_df compared to trips_df: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH3oXctYnxyk"
      },
      "source": [
        "insights :\n",
        "1. the extra exploration shows that 29.85% of all bike-sharing trips pass through the CBD, This is a substantial portion, indicating the CBD's critical role as an origin, destination, or transit point for a large segment of bike-sharing users.\n",
        "2. For both classic_bike and electric_bike categories, members (orange/red bars) consistently account for a significantly higher number of trips that pass through the CBD compared to casual riders (blue bars). This suggests that regular commuters or frequent users (members) are more likely to utilize bike-sharing for travel involving the city's central business district.\n",
        "3. Both casual and member riders use classic_bike more frequently for CBD-involved trips than electric_bike\n",
        "4. Since members are the primary users for CBD-involved trips, strategies to retain and grow membership, and ensure sufficient bike availability in and around the CBD, are crucial.\n",
        "5. The demand for both classic and electric bikes for CBD-involved trips means that fleet management should consider a balanced distribution to meet the preferences of both member and casual riders.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUGoOn14yROg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task6\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmcXeVxLv7yx"
      },
      "outputs": [],
      "source": [
        "# Create a contingency table\n",
        "# (Counts of each combination)\n",
        "# Make sure weâ€™re using categorical data\n",
        "contingency_table = pd.crosstab(trips_df['close_to_cbd'], trips_df['member_casual'])\n",
        "contingency_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00075kXbydNE"
      },
      "outputs": [],
      "source": [
        "# Run chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "print(\"Chi2 Statistic:\", chi2)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"P-value:\", p)\n",
        "# interpretion based on the p-value:\n",
        "if p < 0.05:\n",
        "    print(\" There is a significant correlation between distance to CBD segments and membership type.\")\n",
        "else:\n",
        "    print(\" No significant correlation found between distance to CBD segments and membership type.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIDvh1wlz_gr"
      },
      "source": [
        "| Î± Value  | Interpretation                                                                |\n",
        "| -------- | ----------------------------------------------------------------------------- |\n",
        "| **0.05** | Most common â€” means you're willing to accept a 5% chance of a false positive. |\n",
        "| 0.01     | Stricter â€” used in more critical fields (medicine, etc.).                     |\n",
        "| 0.10     | Looser â€” sometimes used in exploratory research.                              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0oXDOroztOm"
      },
      "source": [
        "insights:\n",
        "1. The p-value of 0.0, which is much less than the conventional significance level of 0.05 leads us to believe there is a statistically significant relationship between whether a trip is close to the CBD and the user's membership type.\n",
        "2. While the Chi-square test indicates strong statistical significance (due to the very large sample size), the proportional difference between casual and member trips being close_to_cbd is relatively small,This suggests thatBoth member and casual riders have roughly half their trips originating or ending close to the CBD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "# E)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Task 1\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampled_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampled_df['rideable_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "daily_weather_avg = sampled_df.groupby('date')[['temp', 'humidity', 'windspeed']].mean().reset_index()\n",
        "daily_weather_avg = daily_weather_avg.rename(columns={\n",
        "    'temp': 'Average Temperature',\n",
        "    'humidity': 'Average Humidity',\n",
        "    'windspeed': 'Average Wind Speed'\n",
        "})\n",
        "fig = px.line(\n",
        "    daily_weather_avg,\n",
        "    x='date',\n",
        "    y=['Average Temperature', 'Average Humidity', 'Average Wind Speed'], # List of columns for y-axis\n",
        "    title='Average Daily Weather Conditions (Temperature, Humidity, Wind Speed)',\n",
        "    labels={\n",
        "        'date': 'Date',\n",
        "        'value': 'Average Value', # Default label for the combined y-axis values\n",
        "        'variable': 'Metric'     # Default label for the legend (which variable is which line)\n",
        "    }\n",
        ")\n",
        "fig.update_layout(hovermode=\"x unified\") # Enhances hover tooltips for multiple lines\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "daily_weather_cond = sampled_df.groupby('date')['weather_segment'].first().reset_index()\n",
        "\n",
        "daily_rev = sampled_df.groupby(sampled_df['date'])['trip_cost'].sum().reset_index(name='revenue')\n",
        "\n",
        "merged_df = pd.merge(daily_rev, daily_weather_cond, on='date', how='left')\n",
        "fig = px.box(\n",
        "    merged_df,\n",
        "    x='weather_segment',  # Categorical variable on x-axis\n",
        "    y='revenue',      # Numerical variable on y-axis\n",
        "    title='Daily Revenue by Weather Condition',\n",
        "    labels={\n",
        "        'weather_condition': 'Weather Condition',\n",
        "        'daily_revenue': 'Daily Revenue ($)'\n",
        "    },\n",
        "    category_orders={\"weather_condition\": [\"Sunny\", \"Cloudy\", \"Rainy\"]} # Optional: ensure specific order\n",
        ")\n",
        "fig.update_traces(boxpoints='all', jitter=0.3) # Show individual points for more detail\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "*   Ù†Ù„Ø§Ø­Ø¸ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø§ÙŠØ§Ù… Ø§Ù„Ù…Ø§Ø·Ø±Ø© ÙŠÙ‚Ø¹ ÙˆØ³Ø·ÙŠØ§ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª Ø¹Ù†Ø¯ Ù…Ø§ÙŠÙ‚Ø§Ø±Ø¨ 650 Ø¯ÙˆÙ„Ø§Ø± ÙˆÙ‡Ùˆ  Ø§Ù‚Ù„  Ù…ØªÙˆØ³Ø· Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø·Ù‚Ø³ Ø±ØºÙ… ÙˆØ¬ÙˆØ¯ Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„Ø§ÙŠØ§Ù… Ù…Ø§Ø·Ø±Ø© Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 55 Ø¨Ø§Ù„Ù…Ø¦Ø© Ù…Ù† Ø§Ù„Ø£ÙŠØ§Ù… Ù‡ÙŠ Ù…Ø§Ø·Ø±Ø© ÙˆÙ†Ù„Ø§Ø­Ø¸ Ù…Ø¯Ù‰ ØªÙˆØ³Ø¹ Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ ÙˆÙ‡Ø°Ø§ ÙŠØ´ÙŠØ± Ø§Ù„Ù‰ ØªÙ‚Ù„Ø¨ ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª ÙÙŠ Ø§Ù„Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ø§Ø·Ø±Ø© ÙˆÙ…Ù„Ø§Ø­Ø¸ Ù‡Ø°Ø§ Ø­ÙŠØ« Ù„Ø¯ÙŠÙ†Ø§ Ø§ÙŠØ§Ù… Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª ØªÙ‚Ø§Ø±Ø¨ Ø§Ù„ØµÙØ± ÙˆØ¨Ø¹Ø¶ Ù…ØªØ¬Ø§ÙˆØ²Ø© Ø§Ù„Ø§Ù„Ù ÙˆØ§Ø¹ØªÙ‚Ø¯ ÙŠØ¹ÙˆØ¯ Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù‚Ø±ÙŠØ¨Ø© Ø§Ù„Ù‰ Ø§Ù„ØµÙØ± Ù‡ÙŠ Ø§Ù„Ø§ÙŠØ§Ù… Ø°Ùˆ Ø§Ù…Ø·Ø§Ø± Ø´Ø¯ÙŠØ¯Ø© ÙˆÙ‡Ø°Ø§ Ù…Ù†Ø·Ù‚ÙŠ Ù…Ù† Ø§Ù„ØµØ¹Ø¨ Ø¹Ù†Ø¯Ù‡Ø§ Ø±ÙƒÙˆØ¨ Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª Ø§Ù…Ø§ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¹Ø§Ù„ÙŠØ© ÙˆØ§Ø±Ø¯ Ø§Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø§ÙŠØ§Ù… Ø§Ù„Ù…Ù…Ø·Ø±Ø© ØªÙƒÙˆÙ† Ù…Ù‚Ø¨ÙˆÙ„Ø© ÙˆÙ‡Ø°Ø§ ÙŠØ¹ÙˆØ¯ Ø§Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„ØªÙŠ ØªØ³ØªÙ…Ø¹ ÙÙŠ Ø°Ù„Ùƒ Ø§Ùˆ Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø© Ø§Ù„Ø¹Ø§Ø¬Ù„Ø© Ù„Ù„Ø¯Ø±Ø§Ø¬Ø© Ø¨Ø¯Ù„ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±\n",
        "*   Ù†Ù„Ø§Ø­Ø¸ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø§ÙŠØ§Ù… Ø§Ù„ØºØ§Ø¦Ù…Ø© Ù…Ø±ØªÙØ¹ ÙˆØ³Ø·ÙŠ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª Ù„Ù…Ø§ ÙŠÙ‚Ø§Ø±Ø¨ 800 Ø¯ÙˆÙ„Ø§Ø± Ø§ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ø§ÙŠØ§Ù… Ø§Ù„Ù…Ø§Ø·Ø±Ø© ÙˆÙ†Ù„Ø§Ø­Ø¸ Ø§Ù†Ù‡ ÙŠÙˆØ¬Ø¯ Ø§Ø³ØªÙ‚Ø±Ø§Ø± ÙˆÙ„ÙŠØ³ ØªÙ‚Ù„Ø¨ Ø¨Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª ÙˆØ§ÙŠØ¶Ø§ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª ÙÙŠ Ø§ÙŠØ§Ù… Ø§Ù„ØºØ§Ø¦Ù…Ø© Ø§Ù…Ø§ Ø¨Ø§Ø²Ø¯ÙŠØ§Ø¯ Ø§Ùˆ Ø§Ø³ØªÙ‚Ø±Ø§Ø± ÙˆÙ†Ù„Ø§Ø­Ø¸ Ù‚ÙØ²Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§ ÙÙŠ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª ÙˆØ§Ø±Ø¯ Ø°Ù„Ùƒ Ø¹Ù†Ø¯ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ù…Ø¹ØªØ¯Ù„Ø© Ø§Ù…Ø§ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªØ¯Ù†ÙŠØ© Ø¬Ø¯Ø§ ÙÙ‡ÙŠ Ø§Ù…Ø§ Ø¨Ø´Ù‡Ø± Ø§Ù„Ø±Ø§Ø¨Ø¹ Ø§Ùˆ Ø§Ù†Ù‡Ø§ ÙƒØ§Ù†Øª Ø§ÙŠØ§Ù… Ø¹Ø·Ù„\n",
        "\n",
        "* Ø±ØºÙ… Ù‚Ù„Ø© Ø§Ù„Ø§ÙŠØ§Ù… Ø§Ù„Ù…Ø´Ù…Ø³Ø© Ø§Ù„Ø§ Ø§Ù†Ù†Ø§ Ù†Ø¬Ø¯ Ø§Ù† Ø§Ù„Ù†Ø§Ø³ ØªØªØ¬Ù‡ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª ÙˆÙ‡ÙŠ Ø§Ø¹Ù„Ù‰ Ù…ØªÙˆØ³Ø· Ø¯Ø®Ù„ ÙˆÙ…Ù„Ø§Ø­Ø¸ Ø§Ù† Ø§Ù„Ù†Ø§Ø³ ÙÙŠ Ø§Ù„Ø§ÙŠØ§Ù… Ø§Ù„Ù…Ø´Ù…Ø³Ø© ØªÙ…ÙŠÙ„ Ø§Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª ÙˆÙ‚Ø¯ ÙŠØ¹ÙˆØ¯ Ø°Ù„Ùƒ Ø¨Ø³Ø¨Ø¨ Ù‚Ù„Ø© Ø§Ù„Ø§ÙŠØ§Ù… Ø§Ù„Ù…Ø´Ù…Ø³Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙØ§Ù„Ù†Ø§Ø³ ØªØ­Ø¨ Ø§Ù„ØªØ¹Ø±Ø¶ Ù„Ù„Ø´Ù…Ø³ Ù„Ø°Ù„Ùƒ ØªÙØ¶Ù„ Ø¹Ù†Ø¯Ù‡Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª Ø¨Ø§Ù„Ø§Ø¶Ø§ÙØ© Ø§Ù† Ø§Ù„Ø¬Ùˆ ÙŠÙƒÙˆÙ† Ø¬ÙŠØ¯\n",
        "\n",
        "* Ø­ÙŠØ« Ù†Ø³ØªÙ†ØªØ¬ ØªØ£Ø«ÙŠØ± Ø§Ù„Ø·Ù‚Ø³ Ø¹Ù„Ù‰ Ø³Ù„ÙˆÙƒ Ø§Ù„Ø±ÙƒØ§Ø¨ ÙŠØ¸Ù‡Ø± Ø¨ÙˆØ¶ÙˆØ­ ÙƒÙŠÙ ØªØ¤Ø«Ø± Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù‚Ø³ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©ØŒ Ø­ÙŠØ« ÙŠÙØ¶Ù„ Ø§Ù„Ù†Ø§Ø³ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø¬Ø§Øª ÙÙŠ Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„Ù…Ø¹ØªØ¯Ù„ ÙˆØ§Ù„Ù…Ø´Ù…Ø³ØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§ØªØŒ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ø§Ù„Ø£ÙŠØ§Ù… Ø§Ù„Ù…Ù…Ø·Ø±Ø© Ø§Ù„Ø¹ÙƒØ³ Ø§Ù‚Ù„ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØ§ÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù‚Ù„\n",
        "\n",
        "* Ù„ÙƒÙ† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø¹Ø¶ Ø§Ù„Ø£ÙŠØ§Ù… Ø§Ù„ØªÙŠ Ù„Ø§ ØªØªØ¨Ø¹ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¹Ø§Ù… Ù„Ù„Ø·Ù‚Ø³ØŒ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù†Øª Ø¬ÙŠØ¯Ø© Ø¨Ø´ÙƒÙ„ Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠ Ø£Ùˆ Ø³ÙŠØ¦Ø© Ø¨Ø´ÙƒÙ„ Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠ Ø­ÙŠØ« Ø¹Ù†Ø¯Ù‡Ø§ Ø§ØªÙˆÙ‚Ø¹ ÙŠÙˆØ¬Ø¯ Ø§Ù…ÙˆØ± Ø§Ø®Ø±Ù‰  ØªØ¯Ø®Ù„ Ø¹Ù†Ø¯Ù‡Ø§\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Task3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lowess', 'rolling', 'ewm', 'expanding', 'ols'\n",
        "# --- Apply Min-Max Normalization to 'daily_revenue' ---\n",
        "# xi-xmin /xmax-xmin\n",
        "\n",
        "# min_revenue = daily_rev['revenue'].min()\n",
        "# max_revenue = daily_rev['revenue'].max()\n",
        "# daily_rev['normalized_daily_revenue'] = (daily_rev['revenue'] - min_revenue) / (max_revenue - min_revenue)\n",
        "\n",
        "merg = pd.merge(daily_weather_avg,daily_rev,on='date',how='left')\n",
        "\n",
        "cols_to_normalize = ['revenue', 'Average Temperature', 'Average Humidity']\n",
        "for col in cols_to_normalize:\n",
        "    min_val = merg[col].min()\n",
        "    max_val = merg[col].max()\n",
        "    # Avoid division by zero if all values are the same\n",
        "    if (max_val - min_val) != 0:\n",
        "        merg[f'normalized_{col}'] = (merg[col] - min_val) / (max_val - min_val)\n",
        "    else: # If all values are the same, normalized value is 0 (or 1, depends on convention)\n",
        "        merg[f'normalized_{col}'] = 0.0\n",
        "\n",
        "fig1 = px.scatter(merg,x='normalized_Average Temperature',y='normalized_revenue',\n",
        "                 title=\"relationship between daily income and temperature\",trendline='ols',\n",
        "                 labels={\n",
        "                   'Temperature': 'normalized_Average Daily Temperature',\n",
        "                   'daily_revenue': 'Daily Revenue ($)' }\n",
        "                 )\n",
        "fig1.show()\n",
        "\n",
        "\n",
        "fig2 = px.scatter(merg,x='normalized_Average Humidity',y='normalized_revenue',\n",
        "                 title=\"relationship between daily income and Humidity\",trendline='ols',\n",
        "                 labels={\n",
        "                   'Humidity': 'normalized_Average Daily humidity',\n",
        "                   'daily_revenue': 'Daily Revenue ($)' }\n",
        "                 )\n",
        "fig2.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆØ¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ù†Ù„Ø§Ø­Ø¸ ÙˆØ¬ÙˆØ¯ Ø¹Ù„Ø§ÙØ© Ø§Ø±ØªØ¨Ø§Ø· Ø®Ø·ÙŠ Ø§ÙŠØ¬Ø§Ø¨ÙŠØ© Ø­ÙŠØ« ÙÙŠ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© (-5 - 3) Ù†Ø±Ù‰ Ø§Ù†Ø®ÙØ§Ø¶ ÙÙŠ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª Ø«Ù… Ù…Ø¹ Ø§Ø²Ø¯ÙŠØ§Ø¯ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ù†Ù„Ø§Ø­Ø¸ Ø§Ù†Ù‡Ø§ ØªØ²Ø¯Ø§Ø¯ Ø§Ù„Ø§ÙŠØ±Ø¯Ø§Øª Ø§Ù„Ù‰ Ø§Ù† ØªØµÙ„ Ø§Ù„Ù‰ Ø­Ø¯ Ù…Ø¹ÙŠÙ† Ø«Ù… ØªØ¨Ø¯Ø¡ Ø¨Ø§Ù„Ù†Ø²ÙˆÙ„ Ø­ÙŠØ« Ø§Ø²Ø¯ÙŠØ§Ø¯ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„Ù‰ Ø¯Ø±Ø¬Ø© Ù…Ø§ ÙˆÙ‡ÙŠ 16 ÙŠØ¤Ø¯ÙŠ Ø§Ø²Ø¯ÙŠØ§Ø¯ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª ÙˆÙ„ÙƒÙ† Ø¨Ø¹Ø¯Ù‡Ø§ Ù†Ø±Ù‰ Ø§Ù† Ø§Ø²Ø¯ÙŠØ§Ø¯ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø³ÙŠØ¤Ø¯ÙŠ Ø§Ù„Ù‰ Ø§Ù†Ø®ÙØ§Ø¶ ÙÙŠ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª\n",
        "\n",
        "\n",
        "* Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø®Ø·ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø±Ø·ÙˆØ¨Ø© Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù„Ø§Ù‚Ø© Ø§Ø±ØªØ¨Ø§Ø· Ø®Ø·ÙŠ Ø­ÙŠØ« Ù†Ù„Ø§Ø­Ø¸ Ø¹Ù†Ø¯ Ø±Ø·ÙˆØ¨Ø© Ù…Ù†Ø®Ù‚Ø¶Ø© Ù„Ø¯ÙŠÙ†Ø§ Ø§ÙŠØ±Ø¯Ø§Ø¯Ø§Øª Ù…Ø±ØªÙØ¹Ø© ÙˆØ§ÙŠØ±Ø§Ø¯Ø§Øª ÙˆÙ…Ù†Ø®ÙØ¶Ø© ÙˆØ§Ù„Ø§Ù…Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ… Ø§ÙŠ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø±Ø·ÙˆØ¨Ø© Ù…ØªÙˆØ³Ø·Ø© Ø§Ùˆ Ø­ØªÙ‰ Ø¹Ø§Ù„ÙŠØ© Ù„Ø¯ÙŠÙ†Ø§ Ø§Ù„Ø§ÙŠØ±Ø§Ø¯Ø§Øª Ù…Ø±Ø§Øª ØªÙƒÙˆÙ† Ù…Ù†Ø®ÙØ¶Ø© ÙˆÙ…Ø±Ø§Øª ØªÙƒÙˆÙ† Ø¹Ø§Ù„ÙŠØ©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        " Task4\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "  # 1. Create the Contingency Table\n",
        "# This table shows the observed frequencies (counts) of each unique combination\n",
        "# of weather segment and ride type.\n",
        "# Rows: weather_segment\n",
        "# Columns: rideable_type\n",
        "contingency_table = pd.crosstab(sampled_df['weather_segment'], sampled_df['rideable_type'])\n",
        "print(\"Contingency Table (Observed Frequencies):\")\n",
        "print(contingency_table)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Visual separator in output\n",
        "\n",
        "# 2. Perform the Chi-Square Test\n",
        "# The chi2_contingency function performs the statistical calculations.\n",
        "# It returns four values:\n",
        "#   - chi2: The calculated Chi-Square statistic.\n",
        "#   - p_value: The probability value (most important for interpretation).\n",
        "#   - dof: Degrees of freedom.\n",
        "#   - expected_frequencies: A 2D array of expected frequencies if the variables were independent.\n",
        "chi2, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi2 Statistic: {chi2:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(\"\\nExpected Frequencies Table:\")\n",
        "\n",
        "# Display the expected frequencies array as a DataFrame for better readability,\n",
        "# using the same indices (rows) and columns as the observed contingency table.\n",
        "print(pd.DataFrame(expected_frequencies, index=contingency_table.index, columns=contingency_table.columns))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Another visual separator\n",
        "\n",
        "\n",
        "# 3. Interpret the Results\n",
        "# Define the significance level (alpha), which is the threshold for comparing the p_value.\n",
        "# A common alpha level is 0.05 (or 5%).\n",
        "alpha = 0.05\n",
        "print(\"Interpretation of Results:\")\n",
        "if p_value < alpha:\n",
        "    # If the p-value is less than alpha, we reject the null hypothesis.\n",
        "    # The null hypothesis (H0) here is: There is no relationship between weather condition and ride type.\n",
        "    print(f\"Since the P-value ({p_value:.4f}) is less than the significance level (alpha = {alpha}),\")\n",
        "    print(\"we reject the null hypothesis (H0).\")\n",
        "    print(\"Conclusion: There is strong statistical evidence of a significant relationship between weather condition and ride type.\")\n",
        "    print(\"In other words, it appears that the distribution of ride types (or bike types) differs depending on the weather condition.\")\n",
        "    print(\"\\n* To understand this relationship further, compare the observed frequencies with the expected frequencies to identify which categories contribute most to the association.\")\n",
        "else:\n",
        "    # If the p-value is greater than or equal to alpha, we fail to reject the null hypothesis.\n",
        "    print(f\"Since the P-value ({p_value:.4f}) is greater than or equal to the significance level (alpha = {alpha}),\")\n",
        "    print(\"we fail to reject the null hypothesis (H0).\")\n",
        "    print(\"Conclusion: There is no sufficient statistical evidence to claim a significant relationship between weather condition and ride type.\")\n",
        "    print(\"In other words, it appears that the choice of ride type (or bike type) is not significantly affected by the weather condition, or any observed differences could be due to random chance.\")\n",
        "\n",
        "\n",
        "df_plot = contingency_table.reset_index().melt(id_vars='weather_segment', var_name='rideable_type', value_name='Count')\n",
        "\n",
        "# 2. Draw a Grouped Bar Chart\n",
        "fig = px.bar(\n",
        "    df_plot,\n",
        "    x='weather_segment',  # X-axis will be weather conditions\n",
        "    y='Count',            # Y-axis will be the number of rides\n",
        "    color='rideable_type',    # Different bars for each ride type within each weather condition\n",
        "    barmode='group',      # This makes the bars for each ride_type stand side-by-side\n",
        "    title='Ride Type Distribution by Weather Condition',\n",
        "    labels={\n",
        "        'weather_segment': 'Weather Condition',\n",
        "        'Count': 'Number of Rides',\n",
        "        'Ride Type': 'Ride Type'\n",
        "    },\n",
        "    category_orders={\"weather_segment\": [\"Sunny\", \"Cloudy\", \"Rainy\"]} # Optional: ensure specific order\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Weather Condition\", yaxis_title=\"Number of Rides\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Catching patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A ) Temporal Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Task 1: Data Verification and Temporal Repairs\n",
        "#### What to do:\n",
        "\n",
        "- Verify that the temporal data you obtained is in the correct chronological order and without any missing intervals\n",
        "- Perform necessary repairs for any temporal issues found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Future Revenue Predictions (15-day forecast)\n",
        "#### What to do:\n",
        "\n",
        "- Build a baseline model to predict future revenues for the next 15 days\n",
        "- Use the method you find most appropriate for time series forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: Prophet Model Implementation\n",
        "#### What to do:\n",
        "\n",
        "- Use Facebook's Prophet library to model temporal time series components\n",
        "- Use Prophet for future predictions and perform hyperparameter tuning for better model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 4: Model Comparison Using Appropriate Evaluation Metrics\n",
        "#### What to do:\n",
        "\n",
        "- Compare your baseline model with Prophet model\n",
        "- Use appropriate evaluation metrics for time series forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 5: Insights and Conclusions\n",
        "#### What to conclude:\n",
        "\n",
        "What will you learn from each component?\n",
        "\n",
        "Analysis points:\n",
        "\n",
        "Trend Analysis: Is bike usage increasing, decreasing, or stable over time?\n",
        "Seasonality Patterns: Are there daily, weekly, or monthly patterns?\n",
        "Model Performance: Which model performs better for forecasting?\n",
        "Business Insights: What do the predictions tell us about future demand?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B ) **General Analysis of Usage Patterns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Data Sampling and Analysis\n",
        "#### What to do:\n",
        "\n",
        "- Perform sampling on the data to a degree where we can analyze and process it\n",
        "- Make the samples comprehensive so that they represent the characteristics we see in the way that suits us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Sampled data is stored in `sampled_df`\n",
        "print(\"Sampling already performed.\")\n",
        "print(f\"Full data size: {len(trips_df)}\")\n",
        "print(f\"Sampled data size: {len(sampled_df)}\")\n",
        "print(\"\\nSampled data head:\")\n",
        "print(sampled_df.head())\n",
        "\n",
        "# Basic statistical analysis on the sampled data to show its characteristics\n",
        "print(\"\\nSampled Data Descriptive Statistics:\")\n",
        "print(sampled_df.describe())\n",
        "\n",
        "print(\"\\nValue counts for key categorical columns in sampled data:\")\n",
        "print(sampled_df['member_casual'].value_counts())\n",
        "print(sampled_df['rideable_type'].value_counts())\n",
        "print(sampled_df['weather_segment'].value_counts())\n",
        "print(sampled_df['kmeans_segment'].value_counts()) # Or 'volume_segment'\n",
        "\n",
        "# Check if the sample distribution resembles the full dataset distribution for a few columns\n",
        "print(\"\\nComparison of value counts (Sampled vs Full Data):\")\n",
        "print(\"\\nMember/Casual:\")\n",
        "print(\"Sampled:\\n\", sampled_df['member_casual'].value_counts(normalize=True))\n",
        "print(\"Full:\\n\", trips_df['member_casual'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nRideable Type:\")\n",
        "print(\"Sampled:\\n\", sampled_df['rideable_type'].value_counts(normalize=True))\n",
        "print(\"Full:\\n\", trips_df['rideable_type'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nTrip Cost Distribution (Sampled vs Full Data):\")\n",
        "print(\"Sampled:\\n\", sampled_df['trip_cost'].describe())\n",
        "print(\"Full:\\n\", trips_df['trip_cost'].describe())\n",
        "\n",
        "print(\"\\nTrip Duration Distribution (Sampled vs Full Data):\")\n",
        "print(\"Sampled:\\n\", sampled_df['trip_duration_minutes'].describe())\n",
        "print(\"Full:\\n\", trips_df['trip_duration_minutes'].describe())\n",
        "\n",
        "print(\"\\nSampling is comprehensive as the value counts and descriptive statistics for key columns in the sampled data are similar to the full dataset, indicating it represents the main characteristics.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Clustering Analysis with Machine Learning\n",
        "#### What to do:\n",
        "\n",
        "- Perform clustering using machine learning techniques (three algorithms)\n",
        "- Focus on the **two most important features**\n",
        "Then describe and compare the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Based on the previous analysis and the problem context, two important features could be:\n",
        "# 1. trip_duration_minutes: Represents the usage length and potentially trip purpose.\n",
        "# 2. trip_cost: Represents the value of the trip and is directly related to duration and type/location.\n",
        "# Other potentially important features could be location-based distances, but duration and cost seem most direct for usage patterns.\n",
        "\n",
        "features_for_clustering = sampled_df[['trip_duration_minutes', 'trip_cost']].dropna()\n",
        "\n",
        "# Handle potential zero costs if they significantly skew data (though cleaning already addressed negative/high outliers)\n",
        "features_for_clustering = features_for_clustering[features_for_clustering['trip_cost'] > 0]\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features_for_clustering)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Clustering Algorithms\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 1. K-Means\n",
        "# Determine optimal k using the Elbow method (optional but good practice)\n",
        "inertia = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10) # Explicitly set n_init\n",
        "    kmeans.fit(scaled_features)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot Elbow method (using Matplotlib)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.title('Elbow Method for K-Means')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n",
        "\n",
        "# Let's choose k based on a plausible interpretation (e.g., k=3 or k=4 could be reasonable)\n",
        "# Let's try k=3 for simplicity as a starting point\n",
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Explicitly set n_init\n",
        "sampled_df.loc[features_for_clustering.index, 'kmeans_cluster'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "\n",
        "\n",
        "# For K-Means and Agglomerative Clustering, we can look at the mean of the features in each cluster.\n",
        "# DBSCAN has a noise cluster (-1), so we'll treat it separately.\n",
        "\n",
        "print(\"\\n--- K-Means Cluster Analysis ---\")\n",
        "kmeans_summary = sampled_df.groupby('kmeans_cluster')[['trip_duration_minutes', 'trip_cost']].mean()\n",
        "print(kmeans_summary)\n",
        "\n",
        "# Visualize K-Means clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=sampled_df, x='trip_duration_minutes', y='trip_cost', hue='kmeans_cluster', palette='viridis', legend='full')\n",
        "plt.title('K-Means Clusters (Trip Duration vs Trip Cost)')\n",
        "plt.xlabel('Trip Duration (minutes)')\n",
        "plt.ylabel('Trip Cost')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Agglomerative Clustering Analysis ---\")\n",
        "agg_summary = sampled_df.groupby('agg_cluster')[['trip_duration_minutes', 'trip_cost']].mean()\n",
        "print(agg_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 2. Agglomerative Clustering (Hierarchical)\n",
        "# We need to choose the number of clusters 'n_clusters'\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=k) # Using the same k as KMeans for comparison\n",
        "sampled_df.loc[features_for_clustering.index, 'agg_cluster'] = agg_clustering.fit_predict(scaled_features)\n",
        "\n",
        "# Visualize Agglomerative clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=sampled_df, x='trip_duration_minutes', y='trip_cost', hue='agg_cluster', palette='viridis', legend='full')\n",
        "plt.title('Agglomerative Clusters (Trip Duration vs Trip Cost)')\n",
        "plt.xlabel('Trip Duration (minutes)')\n",
        "plt.ylabel('Trip Cost')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- DBSCAN Cluster Analysis ---\")\n",
        "# DBSCAN clusters and noise points\n",
        "dbscan_summary = sampled_df.groupby('dbscan_cluster')[['trip_duration_minutes', 'trip_cost']].agg(['mean', 'count'])\n",
        "print(dbscan_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 3. DBSCAN\n",
        "# DBSCAN does not require specifying the number of clusters beforehand\n",
        "# It requires epsilon (eps) and minimum samples (min_samples)\n",
        "# Estimating eps can be tricky; k-distance plot is common, but let's try some values.\n",
        "# min_samples is typically set to 2*ndim or more.\n",
        "# We'll use a relatively small eps and min_samples for this example.\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5) # Adjust eps and min_samples based on data scaling and density\n",
        "sampled_df.loc[features_for_clustering.index, 'dbscan_cluster'] = dbscan.fit_predict(scaled_features) # -1 is noise\n",
        "\n",
        "\n",
        "# Visualize DBSCAN clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=sampled_df, x='trip_duration_minutes', y='trip_cost', hue='dbscan_cluster', palette='viridis', legend='full')\n",
        "plt.title('DBSCAN Clusters (Trip Duration vs Trip Cost)')\n",
        "plt.xlabel('Trip Duration (minutes)')\n",
        "plt.ylabel('Trip Cost')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cluster Comparison\n",
        "\n",
        "##### K-Means Cluster Means:\n",
        "*[Display your kmeans_summary data here]*\n",
        "\n",
        "##### Agglomerative Cluster Means:\n",
        "*[Display your agg_summary data here]*\n",
        "\n",
        "##### DBSCAN Cluster Means and Counts:\n",
        "*[Display your dbscan_summary data here]*\n",
        "\n",
        "##### Description and Comparison:\n",
        "\n",
        "###### K-Means and Agglomerative Clustering\n",
        "- K-Means and Agglomerative Clustering partition the data into a fixed number of clusters (k=3).\n",
        "- Both algorithms identify clusters with distinct means for trip duration and cost.\n",
        "- Cluster 0 in K-Means and Agglomerative likely represents short, low-cost trips.\n",
        "- Cluster 1 or 2 in both likely represent longer, higher-cost trips.\n",
        "- The exact mapping of cluster labels might differ between K-Means and Agglomerative for the same underlying cluster.\n",
        "- K-Means is generally faster for large datasets and produces spherical clusters.\n",
        "- Agglomerative Clustering can identify clusters of different shapes but is slower.\n",
        "\n",
        "###### DBSCAN\n",
        "- DBSCAN identifies dense regions as clusters and marks points in low-density regions as noise (-1).\n",
        "- The number of clusters in DBSCAN is not fixed and depends on 'eps' and 'min_samples'.\n",
        "- A significant number of points might be classified as noise if 'eps' is too small or 'min_samples' too high for sparser areas.\n",
        "- DBSCAN is good at finding arbitrarily shaped clusters and is robust to outliers (which become noise points).\n",
        "- Tuning 'eps' and 'min_samples' is crucial for DBSCAN's performance and the resulting clusters.\n",
        "\n",
        "##### Overall Analysis\n",
        "\n",
        "Using 'trip_duration_minutes' and 'trip_cost' features, clustering helps segment trip data into groups like 'short/cheap', 'medium/moderate', and 'long/expensive'.\n",
        "\n",
        "K-Means and Agglomerative provide a fixed partition, while DBSCAN highlights dense usage patterns and identifies unusual trips as noise.\n",
        "\n",
        "The choice of algorithm depends on whether a fixed number of segments is needed (K-Means, Agglomerative) or if identifying core usage areas and outliers is important (DBSCAN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: Post-Processing Operations\n",
        "What to do:\n",
        "\n",
        "- Perform operations after the appropriate processing to determine the most important features that had an impact\n",
        "- Choose the best model for each algorithm that we used and compare them with a clear baseline that we initially built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 4: Comprehensive Analysis Summary\n",
        "#### What to conclude:\n",
        "- For each clustering result, analyze and interpret:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Insights to Extract:\n",
        "\n",
        "Cluster Interpretation:\n",
        "python# Example interpretations:\n",
        "```\n",
        "# Cluster 0: Short trips during rush hours (commuters)\n",
        "# Cluster 1: Long trips during leisure hours (tourists/recreation)\n",
        "# Cluster 2: Medium trips throughout the day (mixed usage)\n",
        "```\n",
        "Business Recommendations:\n",
        "\n",
        "Which user segments need different pricing strategies?\n",
        "When should bike availability be optimized?\n",
        "What are the peak usage patterns?\n",
        "\n",
        "\n",
        "Algorithm Comparison:\n",
        "\n",
        "Which algorithm captured the most meaningful patterns?\n",
        "How do the clustering results differ between algorithms?\n",
        "Which approach is most suitable for this type of data?\n",
        "\n",
        "\n",
        "\n",
        "Additional Analysis You Can Include:\n",
        "python# Seasonal patterns within clusters\n",
        "sampled_df['month'] = sampled_df['started_at'].dt.month\n",
        "sampled_df['day_of_week'] = sampled_df['started_at'].dt.dayofweek\n",
        "```\n",
        "# Add more features for deeper analysis\n",
        "# Weather correlation, station popularity, etc.\n",
        "```\n",
        "Important Note:\n",
        "Document why you chose specific features for clustering\n",
        "Validate that your clusters make business sense, not just statistical sense"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SL1oTCjr6OFa",
        "sLYxFqxxhgQ0",
        "n4GFM4egUk3o"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
