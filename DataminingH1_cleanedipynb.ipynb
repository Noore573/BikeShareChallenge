{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF3CEiZb4GtS"
      },
      "source": [
        "# **Loading libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xs-YTsgO8QA"
      },
      "outputs": [],
      "source": [
        "%pip install gdown\n",
        "%pip install tqdm scikit-learn\n",
        "%pip install geopandas\n",
        "%pip install geohash2\n",
        "%pip install folium\n",
        "%pip install python-geohash\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import gdown\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "from google.colab import drive\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "from tqdm import tqdm\n",
        "import geohash2\n",
        "from sklearn.cluster import KMeans\n",
        "import json\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "from scipy.stats import chi2_contingency\n",
        "import geohash as gh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I05pgLlnO8QA"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTfRsiq6CQa"
      },
      "source": [
        "# **Loading the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9lConBSO8QB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "downloading the dataset\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkCADdlJzQGc"
      },
      "outputs": [],
      "source": [
        "folder_id = '1O3w5OKnS__hzlL8kTSfGCUc_iX8XNjEN'\n",
        "output_dir = 'Homework'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "print(f\"Attempting to download content from folder ID: {folder_id} into {output_dir}\")\n",
        "try:\n",
        "    gdown.download_folder(id=folder_id, output=output_dir, quiet=False, use_cookies=False)\n",
        "    print(f\"\\nSuccessfully downloaded content to: /content/{output_dir}\")\n",
        "    print(\"You can now find the downloaded content in the 'downloaded_external_folder' directory in your Colab files browser.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during download: {e}\")\n",
        "    print(\"Please ensure the Google Drive folder is publicly accessible or shared with 'Anyone with the link can view'.\")\n",
        "\n",
        "stations_info=pd.read_csv(\"Homework/data/Capital_Bikeshare_Locations.csv\")\n",
        "#\n",
        "# Load tabular data\n",
        "weather_df = pd.read_csv(\"Homework/data/Washington,DC,USA 2024-01-01 to 2024-12-31.csv\")\n",
        "trips_df = pd.read_parquet('Homework/data/daily-rent.parquet')\n",
        "\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "\n",
        "stations_df = pd.read_csv(\"Homework/data/Capital_Bikeshare_Locations.csv\")\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "Shuttle_Bus_Stops=pd.read_csv(\"Homework/data/Shuttle_Bus_Stops.csv\")\n",
        "Metro_Bus_Stops =pd.read_csv(\"Homework/data/Metro_Bus_Stops.csv\")\n",
        "#Loading Residential and Visitor Parking Zones\n",
        "Residential_Visitor_Parking_Zones  = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjq-QmE5b9K"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Downloading the combined and modified dataset (for ease of use )\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI0R9q-L5l8o"
      },
      "outputs": [],
      "source": [
        "file_id = \"114g7JYuZ00i864przAIJQYymib_5h6Qa\"  # Replace with your actual file ID\n",
        "output_file = \"trips_df.csv\"  # You can change the output file name\n",
        "\n",
        "gdown.download(id=file_id, output=output_file, quiet=False)\n",
        "trips_df = pd.read_csv(output_file)\n",
        "print(f\"File downloaded to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL1oTCjr6OFa"
      },
      "source": [
        "# **Preprocessing , Cleaning & inspecting the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gt_DuEv4w9P"
      },
      "source": [
        "\n",
        "There is a problem with missing start/id , almost 20% of the data are nulls so we must find a way to fill these up\n",
        "\n",
        "**spatial join**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "using lang and lati we can match it to the nearest station and then assign this id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h20Hhlhg1IFk"
      },
      "outputs": [],
      "source": [
        "trips_df = trips_df.dropna(subset=['end_lat', 'end_lng'])\n",
        "\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw3RvCyR1T9h"
      },
      "outputs": [],
      "source": [
        "# EPSG:4326 = lat/lon\n",
        "trips_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "stations_gdf = gpd.GeoDataFrame(\n",
        "    stations_df,\n",
        "    geometry=gpd.points_from_xy(stations_df['LONGITUDE'], stations_df['LATITUDE']),\n",
        "    crs='EPSG:4326'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA47hcEu1ldC"
      },
      "outputs": [],
      "source": [
        "# Find nearest station to each ride\n",
        "trips_with_nearest_station = gpd.sjoin_nearest(\n",
        "    trips_gdf, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"distance\"\n",
        ")\n",
        "\n",
        "# Now we fill missing station_id with nearest one\n",
        "trips_df['start_station_id'] = trips_df['start_station_id'].fillna(\n",
        "    trips_with_nearest_station['STATION_ID']\n",
        ")\n",
        "# Creating a mapping from STATION_ID to STATION_NAME\n",
        "id_to_name = stations_df.set_index('STATION_ID')['NAME'].to_dict()\n",
        "\n",
        "# Fill in missing start_station_name using start_station_id\n",
        "trips_df['start_station_name'] = trips_df['start_station_name'].fillna(\n",
        "    trips_df['start_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8isYZzA4yfR"
      },
      "source": [
        "Repeating the process to end id and name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIcXJiy95mU9"
      },
      "outputs": [],
      "source": [
        "trips_gdf_end = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "trips_with_nearest_end_station = gpd.sjoin_nearest(\n",
        "    trips_gdf_end, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"end_distance\"\n",
        ")\n",
        "\n",
        "trips_df['end_station_id'] = trips_df['end_station_id'].fillna(\n",
        "    trips_with_nearest_end_station['STATION_ID']\n",
        ")\n",
        "trips_df['end_station_name'] = trips_df['end_station_name'].fillna(\n",
        "    trips_df['end_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df=trips_df.drop_duplicates()\n",
        "trips_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkda9AV_6sc4"
      },
      "source": [
        "we will continue inspecting the rest of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo2_a0EG6qbm"
      },
      "outputs": [],
      "source": [
        "stations_df=stations_df.drop_duplicates()\n",
        "stations_df.isna().sum()  # we dont need to drop null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4BwUW136uQH"
      },
      "outputs": [],
      "source": [
        "weather_df=weather_df.drop_duplicates()\n",
        "weather_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQGps8Rt62eU"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrMSX_Av6vdY"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhqxpEU267b0"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf = parking_zones_gdf.drop(columns=['CREATOR', 'CREATED','EDITOR','EDITED'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_WPYmtO8iLt"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Laqgf-vPBIl"
      },
      "source": [
        "---\n",
        "**The outside WDC problem :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgmtRPGpPKqE"
      },
      "outputs": [],
      "source": [
        "# The bounding box method\n",
        "DC_LAT_MIN = 38.7916\n",
        "DC_LAT_MAX = 38.9955\n",
        "DC_LNG_MIN = -77.1198\n",
        "DC_LNG_MAX = -76.9094\n",
        "# Filtering  Points Outside the Bounding Box\n",
        "out_of_bounds_start = ~(\n",
        "    (trips_df['start_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &\n",
        "    (trips_df['start_lng'].between(DC_LNG_MIN, DC_LNG_MAX))\n",
        ")\n",
        "\n",
        "out_of_bounds_end = ~(\n",
        "    (trips_df['end_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &\n",
        "    (trips_df['end_lng'].between(DC_LNG_MIN, DC_LNG_MAX))\n",
        ")\n",
        "\n",
        "# Combine both to detect any trip with at least one bad coordinate\n",
        "outlier_mask = out_of_bounds_start | out_of_bounds_end\n",
        "outliers = trips_df[outlier_mask]\n",
        "\n",
        "# Inspect the Outliers\n",
        "print(f\"Number of outlier trips: {len(outliers)}\")\n",
        "outliers[['ride_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng']].head()\n",
        "# Total number of trips in the dataset\n",
        "total_trips = len(trips_df)\n",
        "\n",
        "# Number of outliers detected\n",
        "num_outliers = len(outliers)\n",
        "\n",
        "# Calculate the percentage of outliers\n",
        "percentage_outliers = (num_outliers / total_trips) * 100\n",
        "\n",
        "print(f\"Percentage of outlier trips: {percentage_outliers:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6sn_C7ASFUj"
      },
      "outputs": [],
      "source": [
        "# we will drop them\n",
        "trips_df = trips_df[~outlier_mask].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcTb9jx8S9KT"
      },
      "outputs": [],
      "source": [
        "# checking ride_id\n",
        "print(\"Duplicate ride_ids:\", trips_df['ride_id'].duplicated().sum())\n",
        "print(\"Missing ride_ids:\", trips_df['ride_id'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIRvlXxGVCng"
      },
      "source": [
        "*there an issue with dublicated ride_id so we will only keep the first occurrence*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-Dv6GvgUzWk"
      },
      "outputs": [],
      "source": [
        "# Keep first occurrence or drop based on your context:\n",
        "trips_df = trips_df.drop_duplicates(subset='ride_id', keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4RYU-8_TK-i"
      },
      "outputs": [],
      "source": [
        "print(\"Null times:\", trips_df[['started_at', 'ended_at']].isna().sum())\n",
        "print(\"Negative durations:\", (trips_df['ended_at'] < trips_df['started_at']).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY510PKnTOV9"
      },
      "outputs": [],
      "source": [
        "print(\"Missing start station:\", trips_df['start_station_id'].isna().sum())\n",
        "print(\"Missing end station:\", trips_df['end_station_id'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E87_lOcUTZ5P"
      },
      "outputs": [],
      "source": [
        "zero_coords = trips_df[(trips_df['start_lat'] == 0) | (trips_df['start_lng'] == 0)]\n",
        "print(\"Zero coordinates:\", len(zero_coords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcjBcf4NTcqH"
      },
      "outputs": [],
      "source": [
        "# checkign if rideable_type and member_casual has weird values\n",
        "print(\"Ride types:\", trips_df['rideable_type'].unique())\n",
        "print(\"Member types:\", trips_df['member_casual'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhYVVGgM9mwY"
      },
      "outputs": [],
      "source": [
        "weather_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHgrh-3o-F1Y"
      },
      "outputs": [],
      "source": [
        "# first we make sure all the dates are in the same format (by checking the length)\n",
        "datetime_lengths = weather_df[\"datetime\"].astype(str).apply(len)\n",
        "print(datetime_lengths.value_counts())\n",
        "weather_df[\"date\"] = pd.to_datetime(weather_df[\"datetime\"])\n",
        "print(weather_df[\"date\"].dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU_XzGfm-3st"
      },
      "outputs": [],
      "source": [
        "trips_df[\"start_time\"] = pd.to_datetime(trips_df[\"started_at\"])\n",
        "trips_df[\"end_time\"] = pd.to_datetime(trips_df[\"ended_at\"])\n",
        "# ensuring that CRS is EPSG:4326\n",
        "if parking_zones_gdf.crs != \"EPSG:4326\":\n",
        "    parking_zones_gdf = parking_zones_gdf.to_crs(\"EPSG:4326\")\n",
        "# Spatial Join to Map Stations to Parking Zones\n",
        "# Spatial join: add zone info to each station\n",
        "stations_with_zone = gpd.sjoin(\n",
        "    stations_gdf,\n",
        "    parking_zones_gdf[[\"NAME\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"\n",
        ")\n",
        "# Rename column for clarity\n",
        "stations_with_zone = stations_with_zone.rename(columns={\"zone_name\": \"residential_zone\"})\n",
        "# Joining Weather Data\n",
        "# Extract date from start_time for weather join\n",
        "trips_df[\"date\"] = trips_df[\"start_time\"].dt.date\n",
        "weather_df[\"date\"] = weather_df[\"date\"].dt.date\n",
        "\n",
        "# Join weather by date\n",
        "trips_df = trips_df.merge(weather_df, on=\"date\", how=\"left\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MHwqZPyBL_t"
      },
      "outputs": [],
      "source": [
        "trips_df[['start_station_id', 'end_station_id', 'start_station_name', 'end_station_name']].isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnhDD_fQBZvY"
      },
      "outputs": [],
      "source": [
        "trips_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLYxFqxxhgQ0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Feature engineering**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRUlFc1tO8QN"
      },
      "source": [
        "\n",
        "---\n",
        "B1\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48-Qa2HGpClZ"
      },
      "outputs": [],
      "source": [
        "# B1\n",
        "\n",
        "# From started_at\n",
        "trips_df['start_year'] = trips_df['started_at'].dt.year\n",
        "trips_df['start_month'] = trips_df['started_at'].dt.month\n",
        "trips_df['start_day_num'] = trips_df['started_at'].dt.day\n",
        "trips_df['start_day_name'] = trips_df['started_at'].dt.day_name()\n",
        "\n",
        "# From ended_at\n",
        "trips_df['end_year'] = trips_df['ended_at'].dt.year\n",
        "trips_df['end_month'] = trips_df['ended_at'].dt.month\n",
        "trips_df['end_day_num'] = trips_df['ended_at'].dt.day\n",
        "trips_df['end_day_name'] = trips_df['ended_at'].dt.day_name()\n",
        "trips_df.head(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6HWUwZfBbaR"
      },
      "source": [
        "\n",
        "---\n",
        "B2\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7eipZArwwpS"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_duration_minutes'] = (trips_df['end_time'] - trips_df['start_time']).dt.total_seconds() / 60\n",
        "trips_df['trip_duration_minutes']=trips_df['trip_duration_minutes'].round(2)\n",
        "trips_df['trip_duration_minutes'].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN-VHZhttU30"
      },
      "source": [
        "**The trip_duration_minutes problem**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ0TSaictSZr"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_duration_minutes'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dugc51rytbJp"
      },
      "source": [
        "*we can clearly see that there is a problem with the tripd_durations, the min is a negative value and that is not right*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltwKRGertgvi"
      },
      "outputs": [],
      "source": [
        "# Show trips with negative or 0 duration\n",
        "invalid_durations = trips_df[trips_df['trip_duration_minutes'] <= 0]\n",
        "print(f\"Invalid rows: {len(invalid_durations)}\")\n",
        "invalid_durations[['ride_id', 'started_at', 'ended_at', 'trip_duration_minutes']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-wTEW_ZtjcF"
      },
      "outputs": [],
      "source": [
        "# Filter only valid trips\n",
        "trips_df = trips_df[trips_df['trip_duration_minutes'] > 0]\n",
        "trips_df['trip_duration_minutes'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MC8yVG5pmu6"
      },
      "source": [
        "---\n",
        "B3\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iMUivT3_god"
      },
      "outputs": [],
      "source": [
        "trips_df['member_casual'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy1RkWaJodJF"
      },
      "outputs": [],
      "source": [
        "trips_df['rideable_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Q2d88_r7xx"
      },
      "outputs": [],
      "source": [
        "# Initialize base cost\n",
        "# Start with 0 cost\n",
        "trips_df['trip_cost'] = 0.0\n",
        "\n",
        "# Define fixed costs\n",
        "trips_df.loc[trips_df['member_casual'] == 'member', 'trip_cost'] = 3.95\n",
        "trips_df.loc[trips_df['member_casual'] == 'casual', 'trip_cost'] = 1.00\n",
        "\n",
        "# Add extra cost for duration\n",
        "# for members :\n",
        "# Create condition for member rides longer than 45 mins\n",
        "cond_member_extra = (trips_df['member_casual'] == 'member') & (trips_df['trip_duration_minutes'] > 45)\n",
        "\n",
        "# Electric bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'electric_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.10\n",
        "\n",
        "# Classic bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'classic_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.05\n",
        "# Electric bike for casuals\n",
        "cond_casual_electric = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'electric_bike')\n",
        "trips_df.loc[cond_casual_electric, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.15\n",
        "\n",
        "# Classic bike for casuals\n",
        "cond_casual_classic = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'classic_bike')\n",
        "trips_df.loc[cond_casual_classic, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.05\n",
        "# Add Central Business District (CBD) fee\n",
        "# Preparaing your geometry points\n",
        "# Create GeoDataFrame of start points\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "# #  Load CBD Polygon\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=4326)  # Ensures it's in WGS 84\n",
        "\n",
        "\n",
        "# Convert to GeoDataFrames with correct CRS\n",
        "start_gdf = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "end_gdf = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "\n",
        "# Load CBD polygon and project to EPSG:6933\n",
        "# CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "# CBD = CBD.to_crs(epsg=6933)\n",
        "# cbd_polygon = CBD.geometry.unary_union  # Get full boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph9GeYKu2WJl"
      },
      "outputs": [],
      "source": [
        "# Load CBD polygon and project to EPSG:6933\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.unary_union  # Get full boundary\n",
        "CBD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "306f9C9Nuh0x"
      },
      "outputs": [],
      "source": [
        "# Check spatial containment in EPSG:6933\n",
        "trips_df['start_in_cbd'] = start_gdf['start_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "trips_df['end_in_cbd'] = end_gdf['end_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "\n",
        "# Final condition and cost update\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df.loc[trips_df['in_cbd'], 'trip_cost'] += 0.5\n",
        "trips_df['trip_cost'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYgY5Pp2jR5W"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_cost'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck8sSCqNzuXo"
      },
      "source": [
        "*we can see a clear issue in the data ,  and super high values (4.3 mil in the max ) and std is very high (4837.62) , so we must identify this outliers and deal with them*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-WAd13voMS"
      },
      "outputs": [],
      "source": [
        "# High-cost trips\n",
        "high_cost = trips_df[trips_df['trip_cost'] > 1000].copy()\n",
        "print(high_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n",
        "\n",
        "# Negative-cost trips\n",
        "neg_cost = trips_df[trips_df['trip_cost'] < 0].copy()\n",
        "# try to only print the len\n",
        "print(neg_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRq1RuQcwwZT"
      },
      "outputs": [],
      "source": [
        "# Total rows\n",
        "total_rows = len(trips_df)\n",
        "# Define thresholds\n",
        "high_cost_threshold = 10000\n",
        "negative_cost_threshold = 0\n",
        "\n",
        "# Find outliers\n",
        "high_cost_outliers = trips_df[trips_df['trip_cost'] > high_cost_threshold]\n",
        "negative_cost_outliers = trips_df[trips_df['trip_cost'] < negative_cost_threshold]\n",
        "\n",
        "# Count\n",
        "num_high_cost = len(high_cost_outliers)\n",
        "num_negative_cost = len(negative_cost_outliers)\n",
        "total_outliers = num_high_cost + num_negative_cost\n",
        "\n",
        "# Percentages\n",
        "percent_high_cost = (num_high_cost / total_rows) * 100\n",
        "percent_negative_cost = (num_negative_cost / total_rows) * 100\n",
        "percent_total_outliers = (total_outliers / total_rows) * 100\n",
        "\n",
        "print(f\"High cost outliers: {num_high_cost} ({percent_high_cost:.2f}%)\")\n",
        "print(f\"Negative cost outliers: {num_negative_cost} ({percent_negative_cost:.2f}%)\")\n",
        "print(f\"Total outliers: {total_outliers} ({percent_total_outliers:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkFhEsEC0YA0"
      },
      "source": [
        "since they make a very small amount of the data (0.0%) they can be classifed as false data and we can drop them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzmRagLeyj4C"
      },
      "outputs": [],
      "source": [
        "# Drop outliers by reassigning the filtered DataFrame back to df\n",
        "trips_df = trips_df[(trips_df['trip_cost'] <= high_cost_threshold) & (trips_df['trip_cost'] >= negative_cost_threshold)]\n",
        "trips_df['trip_cost'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-inlm6X1UtR"
      },
      "source": [
        "---\n",
        "B4\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb4fPoBD1PaZ"
      },
      "outputs": [],
      "source": [
        "stations_df['CAPACITY'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THTyaahb2BmM"
      },
      "outputs": [],
      "source": [
        "# Basic histogram using Plotly\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Distribution of Station Capacity')\n",
        "fig.update_layout(xaxis_title='Capacity', yaxis_title='Count', bargap=0.1)\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYG6CLLl1UHj"
      },
      "source": [
        "*Choosing the right threshold*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBzKGS5A4g2K"
      },
      "outputs": [],
      "source": [
        "# Drop NaNs\n",
        "capacity_data = stations_df['CAPACITY'].dropna()\n",
        "# Histogram\n",
        "hist_data = go.Histogram(x=capacity_data, nbinsx=30, name='Histogram', opacity=0.6)\n",
        "# Density Curve\n",
        "kde = gaussian_kde(capacity_data)\n",
        "x_vals = np.linspace(capacity_data.min(), capacity_data.max(), 1000)\n",
        "kde_data = go.Scatter(x=x_vals, y=kde(x_vals) * len(capacity_data) * (x_vals[1] - x_vals[0]),\n",
        "                      mode='lines', name='KDE Curve')\n",
        "\n",
        "# Plot both\n",
        "fig = go.Figure(data=[hist_data, kde_data])\n",
        "fig.update_layout(title='Capacity Distribution with KDE',\n",
        "                  xaxis_title='Capacity', yaxis_title='Count')\n",
        "# Example thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.30)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "print(low_thresh,high_thresh)\n",
        "fig.add_vline(x=low_thresh, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=high_thresh, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8xY7YVb1hxP"
      },
      "source": [
        "method 1 : using quantiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULXZ605f4sqp"
      },
      "outputs": [],
      "source": [
        "# Calculate the thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.33)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "\n",
        "def classify_capacity(cap):\n",
        "    if cap <= low_thresh:\n",
        "        return 'Small'\n",
        "    elif cap <= high_thresh:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "stations_df['STATION_SIZE'].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtJ_U9Is1qdi"
      },
      "source": [
        "method 2 : based on domain knowledge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LssiAiXc6shB"
      },
      "outputs": [],
      "source": [
        "def classify_capacity(cap):\n",
        "    if cap <= 15:\n",
        "        return 'Small'\n",
        "    elif cap <= 25:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "stations_df['STATION_SIZE'].value_counts()\n",
        "print(stations_df['STATION_SIZE'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsYLrpKY1vqS"
      },
      "source": [
        "combine with trips df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7vWPiUD19OW"
      },
      "source": [
        "try1 : using the station_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vouMGhoaqdxB"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create a simplified DataFrame for merging\n",
        "station_size_map = stations_df[['STATION_ID', 'STATION_SIZE']].copy()\n",
        "# Step 3: Merge for start_station_size\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_map.rename(columns={\n",
        "        'STATION_ID': 'start_station_id',\n",
        "        'STATION_SIZE': 'start_station_size'\n",
        "    }),\n",
        "    on='start_station_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Step 4: Merge for end_station_size\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_map.rename(columns={\n",
        "        'STATION_ID': 'end_station_id',\n",
        "        'STATION_SIZE': 'end_station_size'\n",
        "    }),\n",
        "    on='end_station_id',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjJnWcdNt128"
      },
      "outputs": [],
      "source": [
        "print(\"Missing start_station_size:\", trips_df['start_station_size'].isna().sum())\n",
        "print(\"Missing end_station_size:\", trips_df['end_station_size'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAxOiIeqyVaE"
      },
      "outputs": [],
      "source": [
        "# What kind of start_station_id had no match?\n",
        "print(trips_df[trips_df['start_station_size'].isna()][['start_station_id']].drop_duplicates().head(10))\n",
        "\n",
        "# Same for end_station\n",
        "print(trips_df[trips_df['end_station_size'].isna()][['end_station_id']].drop_duplicates().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lK7oC-15RR"
      },
      "source": [
        "try2 : with names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuNfrlpwyuIk"
      },
      "outputs": [],
      "source": [
        "stations_df['NAME'] = stations_df['NAME'].str.strip().str.lower()\n",
        "trips_df['start_station_name'] = trips_df['start_station_name'].str.strip().str.lower()\n",
        "trips_df['end_station_name'] = trips_df['end_station_name'].str.strip().str.lower()\n",
        "\n",
        "# Map start station size using name\n",
        "station_size_name_map = stations_df[['NAME', 'STATION_SIZE']]\n",
        "\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_name_map.rename(columns={'NAME': 'start_station_name', 'STATION_SIZE': 'start_station_size_name'}),\n",
        "    on='start_station_name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Same for end station\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_name_map.rename(columns={'NAME': 'end_station_name', 'STATION_SIZE': 'end_station_size_name'}),\n",
        "    on='end_station_name',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t904hbWnty9o"
      },
      "outputs": [],
      "source": [
        "trips_df['end_station_size'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyd6ovXSzhG7"
      },
      "outputs": [],
      "source": [
        "trips_df['start_station_size'] = trips_df['start_station_size_name']\n",
        "trips_df['end_station_size'] = trips_df['end_station_size_name']\n",
        "\n",
        "# Then drop the temp columns\n",
        "trips_df.drop(columns=['start_station_size_name', 'end_station_size_name'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbDfN8QEubCh"
      },
      "outputs": [],
      "source": [
        "print(trips_df[['start_station_size', 'end_station_size']].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7RoQ7P92IHU"
      },
      "source": [
        "using names was better but we still have some null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5_IiBmU7Npw"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Station Capacity Distribution')\n",
        "fig.add_vline(x=15, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=25, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMBlw701c9vN"
      },
      "source": [
        "---\n",
        "B5\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fylKxgKMecgw"
      },
      "outputs": [],
      "source": [
        "Shuttle_Bus_Stops.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIUA0TNYe2RO"
      },
      "outputs": [],
      "source": [
        "Metro_Bus_Stops['BSTP_LAT'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_buZ1x_CAVFQ"
      },
      "source": [
        "\n",
        "Approaches\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Approach                    | Time Complexity | Vectorized | Fast    |\n",
        "| --------------------------- | --------------- | ---------- | ------- |\n",
        "| Brute Force (Your original) | O(N × M)        | ❌ No       | 🐌 Slow |\n",
        "| BallTree (New)              | O(N log M)      | ✅ Yes      | ⚡ Fast  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_i2yvv2O8QX"
      },
      "source": [
        "Project all your coordinates to EPSG:6933\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEGPg81slsNc"
      },
      "outputs": [],
      "source": [
        "# Create start and end point geometries\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "\n",
        "# Create GeoDataFrames\n",
        "gdf_start = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "gdf_end = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "\n",
        "# Add x/y columns\n",
        "trips_df['start_x'] = gdf_start.geometry.x\n",
        "trips_df['start_y'] = gdf_start.geometry.y\n",
        "trips_df['end_x'] = gdf_end.geometry.x\n",
        "trips_df['end_y'] = gdf_end.geometry.y\n",
        "\n",
        "\n",
        "# projecting   metro and shuttle station coordinates:\n",
        "\n",
        "# Convert station lat/lng to projected coordinates\n",
        "def project_coords(coords_list):\n",
        "    gdf = gpd.GeoDataFrame(geometry=[Point(lon, lat) for lat, lon in coords_list], crs='EPSG:4326')\n",
        "    gdf = gdf.to_crs(epsg=6933)\n",
        "    return np.array([(geom.x, geom.y) for geom in gdf.geometry])\n",
        "# coords\n",
        "# Metro stop coordinates\n",
        "metro_coords = Metro_Bus_Stops[['BSTP_LAT', 'BSTP_LON']].dropna().values\n",
        "\n",
        "# Shuttle stop coordinates\n",
        "shuttle_coords = Shuttle_Bus_Stops[['LATITUDE', 'LONGITUDE']].dropna().values\n",
        "\n",
        "metro_coords_projected = project_coords(metro_coords)\n",
        "shuttle_coords_projected = project_coords(shuttle_coords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN48kQYPl5A0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def euclidean_tree_batch(source_df, stop_coords, x_col, y_col, batch_size=10000):\n",
        "    tree = BallTree(stop_coords, metric='euclidean')\n",
        "\n",
        "    distances = []\n",
        "    n = len(source_df)\n",
        "    tqdm.pandas(desc=f\"Computing distances for {x_col}\")\n",
        "\n",
        "    for i in tqdm(range(0, n, batch_size), desc=\"Batch processing\", unit=\"batch\"):\n",
        "        batch = source_df.iloc[i:i+batch_size]\n",
        "        batch_points = batch[[x_col, y_col]].values\n",
        "\n",
        "        dists, _ = tree.query(batch_points, k=1)\n",
        "        distances.extend(dists.flatten().tolist())\n",
        "\n",
        "    return distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvjyWh64l7Qd"
      },
      "outputs": [],
      "source": [
        "# Start → Metro\n",
        "trips_df['start_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End → Metro\n",
        "trips_df['end_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'end_x', 'end_y'\n",
        ")\n",
        "\n",
        "# Start → Shuttle\n",
        "trips_df['start_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End → Shuttle\n",
        "trips_df['end_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'end_x', 'end_y'\n",
        ")\n",
        "\n",
        "# converting to from meters to km (our choice)\n",
        "trips_df['start_nearest_metro_distance'] = trips_df['start_nearest_metro_distance'] / 1000\n",
        "trips_df['end_nearest_metro_distance'] = trips_df['end_nearest_metro_distance'] / 1000\n",
        "trips_df['start_nearest_shuttle_distance'] = trips_df['start_nearest_shuttle_distance'] / 1000\n",
        "trips_df['end_nearest_shuttle_distance'] = trips_df['end_nearest_shuttle_distance'] / 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4bgJlGMAlPE"
      },
      "outputs": [],
      "source": [
        "trips_df[\n",
        "    ['start_nearest_metro_distance',\n",
        "     'end_nearest_metro_distance',\n",
        "     'start_nearest_shuttle_distance',\n",
        "     'end_nearest_shuttle_distance']\n",
        "].describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3_w8Db_YRhh"
      },
      "outputs": [],
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "cols = ['start_nearest_metro_distance', 'end_nearest_metro_distance',\n",
        "        'start_nearest_shuttle_distance', 'end_nearest_shuttle_distance']\n",
        "for col in cols:\n",
        "    fig = go.Figure(\n",
        "        data=[go.Histogram(\n",
        "            x=sampled_df[col],\n",
        "            nbinsx=100,\n",
        "            marker=dict(color='skyblue'),\n",
        "            opacity=0.75\n",
        "        )]\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=col,\n",
        "        xaxis_title=col,\n",
        "        yaxis_title='Count (Log Scale)',\n",
        "        yaxis_type='log',\n",
        "        bargap=0.1,\n",
        "        width=800,\n",
        "        height=400\n",
        "    )\n",
        "    fig.show(config={'staticPlot':True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAFaQhYwnyJ5"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import pandas as pd # Assuming trips_df is a pandas DataFrame\n",
        "\n",
        "# Re-define thresholds for clarity\n",
        "start_nearest_metro_distance_thr = 1 # meters\n",
        "end_nearest_metro_distance_thr = 1   # meters\n",
        "start_nearest_shuttle_distance_thr = 9 # meters\n",
        "end_nearest_shuttle_distance_thr = 9 # meters\n",
        "\n",
        "# --- Step 1: Identify \"far from\" trips based on current thresholds ---\n",
        "# Using the corrected end_nearest_shuttle_distance_thr for the end shuttle distance\n",
        "far_metro_start_trips = trips_df[trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr]\n",
        "far_metro_end_trips = trips_df[trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr]\n",
        "far_shuttle_start_trips = trips_df[trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr]\n",
        "far_shuttle_end_trips = trips_df[trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr]\n",
        "\n",
        "# Combine all \"far from transit\" trips for a general map (for demonstration)\n",
        "# Using a logical OR to get any trip that is far from ANY of these points\n",
        "far_from_transit_trips = trips_df[\n",
        "    (trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr) |\n",
        "    (trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr) |\n",
        "    (trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr) |\n",
        "    (trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr)\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "print(f\"Number of trips far from metro (start): {len(far_metro_start_trips)}\")\n",
        "print(f\"Number of trips far from metro (end): {len(far_metro_end_trips)}\")\n",
        "print(f\"Number of trips far from shuttle (start): {len(far_shuttle_start_trips)}\")\n",
        "print(f\"Number of trips far from shuttle (end): {len(far_shuttle_end_trips)}\")\n",
        "print(f\"Total unique trips identified as 'far from transit': {len(far_from_transit_trips)}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Create a Folium Map ---\n",
        "\n",
        "# Get the approximate center of Washington D.C. for the map's initial view\n",
        "# You can use the mean of your start/end lat/lngs or a known DC coordinate\n",
        "dc_center_lat = trips_df['start_lat'].mean() # Or a more precise known center for DC\n",
        "dc_center_lng = trips_df['start_lng'].mean() # Or a more precise known center for DC\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[dc_center_lat, dc_center_lng], zoom_start=12)\n",
        "\n",
        "# Add markers for start points of trips identified as \"far from transit\"\n",
        "# Due to the large number of potential points (40k), plotting individual markers for all\n",
        "# might be slow or make the map unreadable.\n",
        "# We'll plot a sample or use a MarkerCluster for better performance.\n",
        "# Let's start by plotting a *sample* of these points if far_from_transit_trips is very large,\n",
        "# or use MarkerCluster. For a first look, a small sample is good.\n",
        "\n",
        "# If you have too many points, consider sampling for initial visualization\n",
        "# Or, even better for density visualization, use MarkerCluster or HeatMap (if allowed for density, check project rules)\n",
        "# Since you're using Plotly for charts and Folium for maps, heatmap should be fine.\n",
        "\n",
        "# Let's just add a few to see the logic work, or use MarkerCluster for all:\n",
        "\n",
        "# OPTION A: Plotting a limited sample (good for a quick check if map gets cluttered)\n",
        "# sample_size = 1000 # Adjust as needed\n",
        "# if len(far_from_transit_trips) > sample_size:\n",
        "#     sample_to_plot = far_from_transit_trips.sample(sample_size, random_state=42)\n",
        "# else:\n",
        "#     sample_to_plot = far_from_transit_trips\n",
        "\n",
        "# for idx, row in sample_to_plot.iterrows():\n",
        "#     folium.CircleMarker(\n",
        "#         location=[row['start_lat'], row['start_lng']],\n",
        "#         radius=2, # Small radius\n",
        "#         color='red',\n",
        "#         fill=True,\n",
        "#         fill_color='red',\n",
        "#         fill_opacity=0.6,\n",
        "#         tooltip=f\"Start: {row['start_station_name']} (Far from Transit)\"\n",
        "#     ).add_to(m)\n",
        "\n",
        "# OPTION B: Using MarkerCluster for better visualization of many points\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "marker_cluster_start = MarkerCluster().add_to(m)\n",
        "marker_cluster_end = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add start points\n",
        "for idx, row in far_from_transit_trips.iterrows():\n",
        "    if pd.notnull(row['start_lat']) and pd.notnull(row['start_lng']):\n",
        "        folium.CircleMarker(\n",
        "            location=[row['start_lat'], row['start_lng']],\n",
        "            radius=2,\n",
        "            color='red', # Color for start points\n",
        "            fill=True,\n",
        "            fill_color='red',\n",
        "            fill_opacity=0.6,\n",
        "            tooltip=f\"Start: {row['start_station_name']} (Far from Transit)\"\n",
        "        ).add_to(marker_cluster_start)\n",
        "\n",
        "# Add end points (optional, you might want separate layers or colors if combining)\n",
        "# For now, let's just show start points to avoid overwhelming the map.\n",
        "# If you want to see end points, you could use a different color or a separate MarkerCluster\n",
        "# for idx, row in far_from_transit_trips.iterrows():\n",
        "#     if pd.notnull(row['end_lat']) and pd.notnull(row['end_lng']):\n",
        "#         folium.CircleMarker(\n",
        "#             location=[row['end_lat'], row['end_lng']],\n",
        "#             radius=2,\n",
        "#             color='blue', # Color for end points\n",
        "#             fill=True,\n",
        "#             fill_color='blue',\n",
        "#             fill_opacity=0.6,\n",
        "#             tooltip=f\"End: {row['end_station_name']} (Far from Transit)\"\n",
        "#         ).add_to(marker_cluster_end)\n",
        "\n",
        "\n",
        "# Save the map to an HTML file or display in a Jupyter Notebook\n",
        "# m.save(\"far_from_transit_trips_map.html\")\n",
        "# In a Jupyter/IPython environment, you can also just display `m` directly\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgaKHoAnoORK"
      },
      "outputs": [],
      "source": [
        "# --- Define your new thresholds based on your histogram observations ---\n",
        "# Example new thresholds (YOU WILL REPLACE THESE WITH YOUR OWN INSIGHTS)\n",
        "new_metro_start_thr = 1  # meters (e.g., if you see a clear drop after 1.2km)\n",
        "new_metro_end_thr = 1    # meters\n",
        "new_shuttle_start_thr = 9 # meters (e.g., if you see a drop after 20km)\n",
        "new_shuttle_end_thr = 9 # meters\n",
        "\n",
        "# --- Create the new boolean features ---\n",
        "trips_df['is_far_from_metro_start'] = trips_df['start_nearest_metro_distance'] > new_metro_start_thr\n",
        "trips_df['is_far_from_metro_end'] = trips_df['end_nearest_metro_distance'] > new_metro_end_thr\n",
        "trips_df['is_far_from_shuttle_start'] = trips_df['start_nearest_shuttle_distance'] > new_shuttle_start_thr\n",
        "trips_df['is_far_from_shuttle_end'] = trips_df['end_nearest_shuttle_distance'] > new_shuttle_end_thr\n",
        "\n",
        "# You can also create a combined flag for any \"far from transit\"\n",
        "trips_df['is_far_from_any_transit'] = (\n",
        "    trips_df['is_far_from_metro_start'] |\n",
        "    trips_df['is_far_from_metro_end'] |\n",
        "    trips_df['is_far_from_shuttle_start'] |\n",
        "    trips_df['is_far_from_shuttle_end']\n",
        ")\n",
        "\n",
        "# Verify the counts of the new features\n",
        "print(\"\\nCounts for new 'far from' features:\")\n",
        "print(trips_df[['is_far_from_metro_start', 'is_far_from_metro_end',\n",
        "                'is_far_from_shuttle_start', 'is_far_from_shuttle_end',\n",
        "                'is_far_from_any_transit']].sum())\n",
        "\n",
        "# Display the first few rows with the new columns to confirm\n",
        "print(\"\\nTrips DataFrame with new features:\")\n",
        "print(trips_df[['ride_id', 'start_nearest_metro_distance', 'is_far_from_metro_start',\n",
        "                'start_nearest_shuttle_distance', 'is_far_from_shuttle_start',\n",
        "                'is_far_from_any_transit']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw0kmmiTpELo"
      },
      "outputs": [],
      "source": [
        "trips_df['is_far_from_metro_start'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smMf9ipl0yav"
      },
      "source": [
        "---\n",
        "B6\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lXEKzGY2Lhs"
      },
      "outputs": [],
      "source": [
        "print(trips_df['start_point'].iloc[0], type(trips_df['start_point'].iloc[0]))\n",
        "print(trips_df['end_point'].iloc[0], type(trips_df['end_point'].iloc[0]))\n",
        "print(type(cbd_polygon))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOAsnPfBp6Py"
      },
      "outputs": [],
      "source": [
        "# STEP 0: Make sure the CBD polygon is projected correctly\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.iloc[0]  # assuming a single polygon\n",
        "# STEP 1: Create a GeoDataFrame from the trip points (start and end)\n",
        "# start_gdf = gpd.GeoDataFrame(trips_df, geometry=trips_df['start_point'], crs=\"EPSG:4326\")\n",
        "# end_gdf   = gpd.GeoDataFrame(trips_df, geometry=trips_df['end_point'], crs=\"EPSG:4326\")\n",
        "\n",
        "# Rebuild the point geometries from lat/lng in EPSG:4326\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "\n",
        "# Project everything to EPSG:6933\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "start_gdf = start_gdf.to_crs(epsg=6933)\n",
        "end_gdf = end_gdf.to_crs(epsg=6933)\n",
        "\n",
        "# CBD polygon (in same projection)\n",
        "cbd_polygon = CBD.geometry.unary_union\n",
        "# Check containment\n",
        "trips_df['start_in_cbd'] = start_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "trips_df['end_in_cbd']   = end_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "\n",
        "# Final result\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df['in_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M_uabQt6Bza"
      },
      "source": [
        "---\n",
        "B7\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oxDKxPlsm3r"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Compute the CBD centroid (already in EPSG:6933)\n",
        "cbd_centroid = cbd_polygon.centroid  # geometry in meters (EPSG:6933)\n",
        "\n",
        "# --- Step 2: Recreate end point GeoDataFrame and project to EPSG:6933\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=6933)\n",
        "\n",
        "# --- Step 3: Compute Euclidean distance in meters\n",
        "trips_df['distance_to_cbd_m'] = end_gdf.geometry.distance(cbd_centroid)\n",
        "\n",
        "# --- Step 4: Set distance to None where start AND end are in the CBD\n",
        "mask = trips_df['start_in_cbd'] & trips_df['end_in_cbd']\n",
        "trips_df.loc[mask, 'distance_to_cbd_m'] = None\n",
        "\n",
        "# --- Step 5: Inspect result\n",
        "trips_df['distance_to_cbd_m'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szoai97X63Qj"
      },
      "source": [
        "\n",
        "\n",
        "**Threasholding strategies**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wnOOXiY7I-C"
      },
      "source": [
        "kinda of an elbow method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPKO880k6_k8"
      },
      "outputs": [],
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "# Extract the data\n",
        "data = sampled_df['distance_to_cbd_m'].dropna()\n",
        "\n",
        "# Create histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=100,\n",
        "    name='Histogram',\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75\n",
        ")\n",
        "\n",
        "# Create KDE line (manual since Plotly doesn’t support KDE directly)\n",
        "kde = gaussian_kde(data)\n",
        "x_vals = np.linspace(data.min(), data.max(), 1000)\n",
        "kde_vals = kde(x_vals) * len(data) * (x_vals[1] - x_vals[0])  # scale to match histogram\n",
        "\n",
        "kde_trace = go.Scatter(\n",
        "    x=x_vals,\n",
        "    y=kde_vals,\n",
        "    mode='lines',\n",
        "    name='KDE',\n",
        "    line=dict(color='darkblue')\n",
        ")\n",
        "\n",
        "# Vertical reference lines\n",
        "vline1 = go.Scatter(\n",
        "    x=[2000, 2000],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='2km Threshold',\n",
        "    line=dict(color='red', dash='dash')\n",
        ")\n",
        "\n",
        "vline2 = go.Scatter(\n",
        "    x=[2764, 2764],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='Median',\n",
        "    line=dict(color='green', dash='dash')\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=[hist, kde_trace, vline1, vline2])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distance to CBD at End of Trip',\n",
        "    xaxis_title='distance_to_cbd_m',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    legend=dict(x=0.7, y=0.95)\n",
        ")\n",
        "\n",
        "fig.show( config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvBXxgrW7IIf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "i will choose the median beacause looking at the histogram we can see the counts drops\n",
        "\"\"\"\n",
        "threshold = 2764\n",
        "# Apply binary classification\n",
        "trips_df['close_to_cbd'] = trips_df['distance_to_cbd_m'].apply(\n",
        "    lambda d: None if pd.isna(d) else d <= threshold\n",
        ")\n",
        "trips_df['close_to_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68oomfV78m6-"
      },
      "outputs": [],
      "source": [
        "print(trips_df['close_to_cbd'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ellq2QJVzAe3"
      },
      "source": [
        "---\n",
        "B8\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZVkChHv0YkX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Washington, D.C. is roughly:\n",
        "\n",
        "~16 km (north-south)\n",
        "\n",
        "~13 km (east-west)\n",
        "\n",
        "So, a geohash precision of 5–8 is appropriate.\n",
        "\"\"\"\n",
        "def encode_geohashes(df, lat_col, lon_col, precisions):\n",
        "    for p in precisions:\n",
        "        col_name = f'geohash_p{p}'\n",
        "        df[col_name] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lon_col], p), axis=1)\n",
        "    return df\n",
        "\n",
        "# Try precisions from 5 to 8\n",
        "precisions_to_test = [5, 6, 7, 8]\n",
        "trips_df = encode_geohashes(trips_df, 'start_lat', 'start_lng', precisions_to_test)\n",
        "for p in precisions_to_test:\n",
        "    print(f\"Precision {p}: {trips_df[f'geohash_p{p}'].nunique()} unique regions\")\n",
        "\"\"\"\n",
        "If the number is too small → you're over-aggregating.\n",
        "\n",
        "If it's too big (e.g. thousands) → too fine → hard to summarize meaningfully.\n",
        "\"\"\"\n",
        "\n",
        "for p in precisions_to_test:\n",
        "    counts = trips_df[f'geohash_p{p}'].value_counts()\n",
        "    print(f\"Precision {p} → median trips per geohash: {counts.median()}\")\n",
        "\"\"\"\n",
        "This tells us how balanced the spatial bins are.\n",
        "\n",
        "we ideally want 50–500 trips per cell.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShObmS0z4FJ1"
      },
      "source": [
        "| Precision | Median Trips per Geohash | Interpretation                                                     |\n",
        "| --------- | ------------------------ | ------------------------------------------------------------------ |\n",
        "| **5**     | 1761                     | ⚠️ Too coarse — merges many neighborhoods into one.                |\n",
        "| **6**     | 196                      | ✅ Good balance — each area has enough trips for reliable analysis. |\n",
        "| **7**     | 7                        | ⚠️ Very fine — may be too sparse for most practical summaries.     |\n",
        "| **8**     | 2                        | 🚫 Too sparse — most areas will be noise or empty.                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3yKdEI63oaL"
      },
      "outputs": [],
      "source": [
        "# we will choose 6t\n",
        "trips_df['geohash_sector'] = trips_df['geohash_p6']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRZhifmX5KRC"
      },
      "source": [
        "---\n",
        "\n",
        "B9\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpEGkPhx4RQ5"
      },
      "outputs": [],
      "source": [
        "# Group by Sector and Date\n",
        "# Assume you have a 'date' column (convert if needed)\n",
        "trips_df['date'] = pd.to_datetime(trips_df['date'])\n",
        "\n",
        "# Count trips per day per sector\n",
        "daily_counts = trips_df.groupby(['geohash_p6', 'date']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Now compute average daily trips per geohash sector\n",
        "avg_daily_trips = daily_counts.groupby('geohash_p6')['trip_count'].mean().reset_index()\n",
        "avg_daily_trips.rename(columns={'trip_count': 'avg_daily_trips'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctAtFgF5rwM"
      },
      "source": [
        "Choose Segmentation Method (for Red / Yellow / Gray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfhDqYSj5nJs"
      },
      "source": [
        "\n",
        "| Method                         | Description                          | Pros             | Use Case             |\n",
        "| ------------------------------ | ------------------------------------ | ---------------- | -------------------- |\n",
        "| **Quantiles** (e.g., tertiles) | Divide into 3 equal-sized groups     | Simple, fair     | Balanced datasets    |\n",
        "| **Natural Breaks (Jenks)**     | Optimize separation between clusters | Data-aware       | Uneven distributions |\n",
        "| **KMeans Clustering (k=3)**    | Machine learning-based segmentation  | Optimal grouping | Large datasets       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g__UfD5W5a6n"
      },
      "outputs": [],
      "source": [
        "# quantiles :\n",
        "# Assign labels based on quantiles\n",
        "quantiles = avg_daily_trips['avg_daily_trips'].quantile([1/3, 2/3])\n",
        "low_thresh = quantiles.iloc[0]\n",
        "high_thresh = quantiles.iloc[1]\n",
        "\n",
        "def classify_volume(val):\n",
        "    if val < low_thresh:\n",
        "        return 'gray'   # Low volume\n",
        "    elif val < high_thresh:\n",
        "        return 'yellow' # Medium volume\n",
        "    else:\n",
        "        return 'red'    # High volume\n",
        "\n",
        "avg_daily_trips['volume_segment'] = avg_daily_trips['avg_daily_trips'].apply(classify_volume)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAYD3Ls1-Rpi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract the data\n",
        "data = avg_daily_trips['avg_daily_trips'].dropna()\n",
        "\n",
        "# Histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=30,\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75,\n",
        "    name='Avg Daily Trips'\n",
        ")\n",
        "\n",
        "# Vertical threshold lines\n",
        "vline_low = go.Scatter(\n",
        "    x=[low_thresh, low_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='Low Threshold',\n",
        "    line=dict(color='gray', dash='dash')\n",
        ")\n",
        "\n",
        "vline_high = go.Scatter(\n",
        "    x=[high_thresh, high_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='High Threshold',\n",
        "    line=dict(color='orange', dash='dash')\n",
        ")\n",
        "\n",
        "# Combine into figure\n",
        "fig = go.Figure(data=[hist, vline_low, vline_high])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distribution of Avg Daily Trips per Geohash Sector',\n",
        "    xaxis_title='Avg Daily Trips',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    bargap=0.1\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYWaSyEE569w"
      },
      "outputs": [],
      "source": [
        "X = avg_daily_trips[['avg_daily_trips']].values\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42  , n_init=10).fit(X)\n",
        "avg_daily_trips['kmeans_label'] = kmeans.labels_\n",
        "\n",
        "# Map to red/yellow/gray using sorted cluster means\n",
        "label_map = dict(zip(\n",
        "    np.argsort(kmeans.cluster_centers_.flatten()),\n",
        "    ['gray', 'yellow', 'red']\n",
        "))\n",
        "avg_daily_trips['kmeans_segment'] = avg_daily_trips['kmeans_label'].map(label_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaBBJvFnZ81X"
      },
      "outputs": [],
      "source": [
        "avg_daily_trips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2epWDpMGakGT"
      },
      "outputs": [],
      "source": [
        "trips_df['geohash_p6'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONedOBCPcTl-"
      },
      "outputs": [],
      "source": [
        "# Merge segments into trips_df\n",
        "trips_df = trips_df.merge(\n",
        "    avg_daily_trips[['geohash_p6','volume_segment','kmeans_segment']],\n",
        "    on='geohash_p6',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eK2NYQkgpue"
      },
      "outputs": [],
      "source": [
        "trips_df['kmeans_segment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WSRw0iZhLjR"
      },
      "outputs": [],
      "source": [
        "trips_df['volume_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1Nby-lsEf9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "B10\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKSxW3W-dWCx"
      },
      "outputs": [],
      "source": [
        "trips_df['conditions'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh1m4kfBfY8I"
      },
      "outputs": [],
      "source": [
        "def classify_weather(condition):\n",
        "    condition = condition.lower()  # lowercase for safety\n",
        "    if 'rain' in condition or 'snow' in condition:\n",
        "        return 'rainy'\n",
        "    elif 'overcast' in condition or 'cloudy' in condition:\n",
        "        return 'cloudy'\n",
        "    elif 'clear' in condition:\n",
        "        return 'sunny'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "# Apply binning\n",
        "trips_df['weather_segment'] = trips_df['conditions'].apply(classify_weather)\n",
        "trips_df['weather_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep9rr_N9s9v_"
      },
      "source": [
        "---\n",
        "\n",
        "B11\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cd81R4R4PWY"
      },
      "source": [
        "quick inspection of the data dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_f0gqxJNym1"
      },
      "outputs": [],
      "source": [
        "sorted_ended_at_df = trips_df[['ended_at']].sort_values(by='ended_at')\n",
        "print(\"--- Sorted 'ended_at' DataFrame (first 5 rows) ---\")\n",
        "print(sorted_ended_at_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Find the earliest and latest dates ---\n",
        "earliest_date = sorted_ended_at_df['ended_at'].min()\n",
        "latest_date = sorted_ended_at_df['ended_at'].max()\n",
        "\n",
        "print(f\"The earliest date in 'ended_at' is: {earliest_date}\")\n",
        "print(f\"The latest date in 'ended_at' is: {latest_date}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0A2F38oTZ5x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Step 2: Sort the DataFrame by 'started_at' ---\n",
        "sorted_started_at_df = trips_df[['started_at']].sort_values(by='started_at')\n",
        "print(\"--- Sorted 'started_at' DataFrame (first 5 rows) ---\")\n",
        "print(sorted_started_at_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Find the earliest and latest dates using 'started_at' ---\n",
        "earliest_date_started = sorted_started_at_df['started_at'].min()\n",
        "latest_date_started = sorted_started_at_df['started_at'].max()\n",
        "\n",
        "print(f\"The earliest date in 'started_at' is: {earliest_date_started}\")\n",
        "print(f\"The latest date in 'started_at' is: {latest_date_started}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37IY3SeYr1C1"
      },
      "outputs": [],
      "source": [
        "# Make sure 'ended_at' is datetime\n",
        "# trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'])\n",
        "trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'], format='mixed', errors='coerce')\n",
        "\n",
        "\n",
        "# Extract just the date (without time)\n",
        "trips_df['end_date'] = trips_df['ended_at'].dt.date\n",
        "daily_income_weather = trips_df.groupby(['end_date', 'weather_segment'])['trip_cost'].sum().reset_index()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxfbTLjmtBDz"
      },
      "outputs": [],
      "source": [
        "# convert\n",
        "# Make sure end_date is datetime\n",
        "daily_income_weather['end_date'] = pd.to_datetime(daily_income_weather['end_date'])\n",
        "\n",
        "fig_long = px.line(\n",
        "    daily_income_weather,\n",
        "    x='end_date',\n",
        "    y='trip_cost',\n",
        "    color='weather_segment',\n",
        "    title='Daily Total Trip Cost by Weather Condition (Long Format)',\n",
        "    labels={'end_date': 'Date', 'trip_cost': 'Total Income', 'weather_segment': 'Weather'}\n",
        ")\n",
        "\n",
        "fig_long.update_layout(xaxis_title='Date', yaxis_title='Trip Cost', hovermode='x unified')\n",
        "fig_long.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6WfXRDRtHwQ"
      },
      "outputs": [],
      "source": [
        "# Pivot to wide format\n",
        "wide_df = daily_income_weather.pivot(index='end_date', columns='weather_segment', values='trip_cost').fillna(0)\n",
        "wide_df = wide_df.sort_index()\n",
        "\n",
        "# Build traces\n",
        "fig_wide = go.Figure()\n",
        "\n",
        "for condition in wide_df.columns:\n",
        "    fig_wide.add_trace(go.Scatter(\n",
        "        x=wide_df.index,\n",
        "        y=wide_df[condition],\n",
        "        mode='lines',\n",
        "        name=condition\n",
        "    ))\n",
        "\n",
        "fig_wide.update_layout(\n",
        "    title='Daily Total Trip Cost by Weather Condition (Wide Format)',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Trip Cost',\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white',\n",
        "    legend_title='Weather'\n",
        ")\n",
        "\n",
        "fig_wide.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMDgJxs8OXx_"
      },
      "source": [
        "Which one is better for our problem  ?<br>\n",
        "the Long Format is the most suitable and effective,This is because it allows for direct visual comparison of revenue trends across different weather types over time on a single graph, making it easier to spot patterns and seasonal impacts. The long format is also considered more intuitive for time-series visualization. Conversely, the \"wide format\" is deemed less clear due to potential visual clutter when many categories are present.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBXs3HzQO-wt"
      },
      "source": [
        "---\n",
        "B12\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruD09zYz5E68"
      },
      "source": [
        "Feature 1 : rush_hour\n",
        "<br> Indicates if the ride occurred during typical commuting hours (7–10 AM or 4–7 PM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN5x_9uTt2RV"
      },
      "outputs": [],
      "source": [
        "\n",
        "trips_df['start_time'] = pd.to_datetime(trips_df['start_time'], errors='coerce')\n",
        "\n",
        "trips_df['rush_hour'] = (\n",
        "    trips_df['start_time'].dt.hour.between(7, 10) |\n",
        "    trips_df['start_time'].dt.hour.between(16, 19)\n",
        ").astype(int)\n",
        "trips_df['rush_hour'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCxcP4_M5MK-"
      },
      "source": [
        "Feature 2 : hour_segment <br>\n",
        "Categorize ride start times into broader buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQtxwZZnPxnT"
      },
      "outputs": [],
      "source": [
        "def get_hour_segment(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Midday'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "trips_df['hour_segment'] = trips_df['start_time'].dt.hour.apply(get_hour_segment)\n",
        "trips_df['hour_segment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGwHHc2n5ThH"
      },
      "source": [
        "Feature 3 : is_weekend<br>\n",
        "Helps spot usage patterns on weekends vs weekdays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiVY5Od0QKAe"
      },
      "outputs": [],
      "source": [
        "trips_df['is_weekend'] = trips_df['start_time'].dt.dayofweek >= 5\n",
        "trips_df['is_weekend'] = trips_df['is_weekend'].astype(int)\n",
        "trips_df['is_weekend'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49oBoftHUUaG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**EDA**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOhRmjguH-c"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Sampling the data\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABgwYinmuLkR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sampled data stats\n",
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCLOvee7p5Ii"
      },
      "outputs": [],
      "source": [
        "sampled_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxL-4ceXUYhn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# A )\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PB5IAGID92O"
      },
      "source": [
        "## Task 1: Top 5 Starting Stations Analysis\n",
        "\n",
        "### Objective\n",
        "Identify the top 5 stations with the highest number of trip departures (starting stations) and create a bar chart showing statistical information for these top 5 stations.\n",
        "\n",
        "### Requirements\n",
        "- Identify the top 5 stations with the highest number of trip departures (starting stations)\n",
        "- Create a bar chart showing statistical information for these top 5 stations\n",
        "- Display the count of trips starting from each station\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cwZZ30VTEp_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Identify the top 5 starting stations\n",
        "start_station_counts = sampled_df['start_station_name'].value_counts()\n",
        "top_5_start_stations = start_station_counts.head(5)\n",
        "\n",
        "print(\"Top 5 Starting Stations:\")\n",
        "print(top_5_start_stations)\n",
        "\n",
        "# Create a bar chart for the top 5 starting stations\n",
        "fig = px.bar(\n",
        "    top_5_start_stations,\n",
        "    x=top_5_start_stations.index,\n",
        "    y=top_5_start_stations.values,\n",
        "    title='Top 5 Starting Stations by Trip Count',\n",
        "    labels={'x': 'Station Name', 'y': 'Number of Trips'},\n",
        "    color=top_5_start_stations.values,  # Color bars by count\n",
        "    color_continuous_scale=px.colors.sequential.Viridis # Optional: choose a color scale\n",
        ")\n",
        "\n",
        "# Update layout for better readability\n",
        "fig.update_layout(\n",
        "    xaxis={'categoryorder':'total descending'}, # Ensure bars are ordered by count\n",
        "    xaxis_title='Station Name',\n",
        "    yaxis_title='Number of Trips',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlBQVMJyD92O"
      },
      "source": [
        "### Top 5 Starting Stations by Trip Count\n",
        "\n",
        "**Key Insight:**\n",
        "\n",
        "* The station **\"Park Rd & Holmead Pl NW\"** is by far the **most popular starting point**, with approximately **300 trips**, significantly ahead of the next stations.\n",
        "\n",
        "**Detailed Breakdown:**\n",
        "\n",
        "* The remaining top 4 stations:\n",
        "\n",
        "  * **\"14th & Belmont St NW\"**\n",
        "  * **\"18th St & Wyoming Ave NW\"**\n",
        "  * **\"Columbus Circle / Union Station\"**\n",
        "  * **\"Lamont & Mt Pleasant NW\"**\n",
        "\n",
        "  Each of these falls in the **190–210 trip range**, indicating moderate popularity and relatively close usage levels among them.\n",
        "\n",
        "**Implication:**\n",
        "\n",
        "* The high volume at **Park Rd & Holmead Pl NW** could reflect factors such as:\n",
        "\n",
        "  * Proximity to residential or commercial zones\n",
        "  * Availability of bike lanes or connectivity\n",
        "  * Strategic location near transit hubs or universities\n",
        "\n",
        "This station likely plays a **central role in trip generation**, which may warrant priority in terms of maintenance, expansions, or targeted promotions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM2ER_1XD92O"
      },
      "source": [
        "## Task 2: Trip Distribution by Bike Type and Membership\n",
        "\n",
        "### Objective\n",
        "Calculate the distribution of trips by bike type (classic vs electric) and membership type (member vs casual), then create a single bar chart showing these distributions.\n",
        "\n",
        "### Requirements\n",
        "- Calculate the distribution of trips by:\n",
        "  - Bike type (classic vs electric)\n",
        "  - Membership type (member vs casual)\n",
        "- Create one bar chart showing these distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdLp9fZaD92O"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the distribution\n",
        "distribution = sampled_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='count')\n",
        "\n",
        "# Create the bar chart\n",
        "fig = px.bar(\n",
        "    distribution,\n",
        "    x='rideable_type',\n",
        "    y='count',\n",
        "    color='member_casual',\n",
        "    barmode='group',\n",
        "    title='Trip Distribution by Bike Type and Membership',\n",
        "    labels={'rideable_type': 'Bike Type', 'count': 'Number of Trips', 'member_casual': 'Membership Type'}\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_title='Bike Type', yaxis_title='Number of Trips')\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OkpnsnDD92O"
      },
      "source": [
        "### Trip Distribution by Bike Type and Membership\n",
        "\n",
        "**Key Insights:**\n",
        "\n",
        "#### 1. **Electric bikes are the dominant preference:**\n",
        "\n",
        "* **Members** show a strong preference for **electric bikes** (≈7,500 trips), much higher than classic bikes.\n",
        "* Even **casual users** prefer electric bikes (≈3,700 trips) over classic ones.\n",
        "\n",
        "#### 2. **Membership significantly boosts usage:**\n",
        "\n",
        "* Both bike types have **higher trip counts among members** than casual users.\n",
        "* Members took nearly **double the trips** compared to casuals, suggesting:\n",
        "\n",
        "  * Regular commuting behavior\n",
        "  * Higher cost-effectiveness for frequent riders\n",
        "  * Greater engagement with the system\n",
        "\n",
        "#### 3. **Classic bikes are less used overall:**\n",
        "\n",
        "* Classic bikes saw fewer total trips for both user types, particularly from casual riders, possibly due to:\n",
        "\n",
        "  * Higher effort required\n",
        "  * Less appeal in convenience and speed\n",
        "\n",
        "**Implication:**\n",
        "\n",
        "* The system should consider **prioritizing electric bike availability**, especially in high-traffic stations and for members.\n",
        "* **Member-focused incentives** or **electric bike maintenance** should be top priorities to support usage trends.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdTxnG-fD92O"
      },
      "source": [
        "## Task 3: Sunburst Chart for Top 5 Starting Stations\n",
        "\n",
        "### Objective\n",
        "Create a sunburst chart showing the breakdown of trips for the top 5 starting stations with the hierarchy: Station → Bike Type → Membership Type. This will show how trips are distributed across bike types and membership types for each top station.\n",
        "\n",
        "### Requirements\n",
        "- Create a sunburst chart showing the breakdown of trips for the top 5 starting stations\n",
        "- Show the hierarchy: Station → Bike Type → Membership Type\n",
        "- Display how trips are distributed across bike types and membership types for each top station\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6g3nHt_D92P"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter data for only the top 5 starting stations\n",
        "trips_top5_stations = sampled_df[sampled_df['start_station_name'].isin(top_5_start_stations.index)].copy()\n",
        "\n",
        "# Group data for the sunburst chart\n",
        "sunburst_data = trips_top5_stations.groupby(['start_station_name', 'rideable_type', 'member_casual']).size().reset_index(name='count')\n",
        "\n",
        "# Create the sunburst chart\n",
        "fig_sunburst = px.sunburst(\n",
        "    sunburst_data,\n",
        "    path=['start_station_name', 'rideable_type', 'member_casual'],  # Hierarchy\n",
        "    values='count',\n",
        "    title='Trip Breakdown for Top 5 Starting Stations by Bike and Membership Type'\n",
        ")\n",
        "\n",
        "# Update layout for better appearance\n",
        "fig_sunburst.update_layout(\n",
        "    margin=dict(t=0, l=0, r=0, b=0),\n",
        "    title_text='Trip Breakdown for Top 5 Starting Stations by Bike and Membership Type',\n",
        "    title_x=0.5 # Center the title\n",
        ")\n",
        "\n",
        "fig_sunburst.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi0T9C-MD92P"
      },
      "source": [
        "### Trip Breakdown by Start Station, Bike Type, and Membership\n",
        "\n",
        "The sunburst chart provides a **multidimensional view** of trip distribution across three variables:\n",
        "\n",
        "1. **Starting Station**\n",
        "2. **Bike Type (electric or classic)**\n",
        "3. **Membership Type (casual or member)**\n",
        "\n",
        "#### **Key Observations:**\n",
        "\n",
        "1. **Electric Bikes Dominate at Most Stations**\n",
        "\n",
        "   * At nearly every station, **electric bikes** occupy the largest segment.\n",
        "   * Most **member** trips originate using electric bikes, indicating a clear usage preference.\n",
        "\n",
        "2. **Park Rd & Holmead Pl NW Leads in Total Trips**\n",
        "\n",
        "   * This station has the **widest outer ring**, especially in electric bike usage by members.\n",
        "   * Indicates it is a **strategic hub**—possibly due to location, connectivity, or accessibility.\n",
        "\n",
        "3. **Membership-Driven Usage**\n",
        "\n",
        "   * Across all stations, **member users consistently take more trips** than casual users.\n",
        "   * Reflects a higher engagement and potential for loyalty-driven programs.\n",
        "\n",
        "4. **Small Classic Bike Usage by Casuals**\n",
        "\n",
        "   * In many stations (e.g., 14th & Belmont St NW), classic bikes used by casuals represent a **very thin slice**.\n",
        "   * Suggests low attractiveness of classic bikes to non-regular riders.\n",
        "\n",
        "5. **Columbus Circle / Union Station: Balanced Mix**\n",
        "\n",
        "   * This station shows a relatively **balanced use** of both bike types across membership types.\n",
        "   * Indicates it serves diverse rider profiles—possibly due to being a major transit point.\n",
        "   \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl7OzBX1D92P"
      },
      "source": [
        "## Task 4: Station Capacity Analysis\n",
        "\n",
        "### Objective\n",
        "Create a histogram showing the distribution of station capacities, then create a bar chart showing trip distribution by station capacity categories (small, medium, large). Stations need to be categorized into capacity groups first.\n",
        "\n",
        "### Requirements\n",
        "- Create a histogram showing the distribution of station capacities\n",
        "- Create a bar chart showing trip distribution by station capacity categories (small, medium, large)\n",
        "- Categorize stations into capacity groups first\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toycNkIBD92P"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Merge station size back to trips_df\n",
        "# Ensure station_id types match\n",
        "stations_df['STATION_ID'] = stations_df['STATION_ID'].astype(sampled_df['start_station_id'].dtype)\n",
        "\n",
        "# Merge station size to trips_df based on start station\n",
        "trips_with_station_size = sampled_df.merge(\n",
        "    stations_df[['STATION_ID', 'STATION_SIZE']],\n",
        "    left_on='start_station_id',\n",
        "    right_on='STATION_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Rename column for clarity\n",
        "trips_with_station_size.rename(columns={'STATION_SIZE': 'start_station_size'}, inplace=True)\n",
        "\n",
        "# Merge station size based on end station as well\n",
        "trips_with_station_size = trips_with_station_size.merge(\n",
        "    stations_df[['STATION_ID', 'STATION_SIZE']],\n",
        "    left_on='end_station_id',\n",
        "    right_on='STATION_ID',\n",
        "    how='left',\n",
        "    suffixes=('', '_end_station') # Add suffix for the end station size column\n",
        ")\n",
        "trips_with_station_size.rename(columns={'STATION_SIZE': 'end_station_size'}, inplace=True)\n",
        "\n",
        "# Drop the redundant STATION_ID columns from the merges\n",
        "trips_with_station_size.drop(columns=['STATION_ID', 'STATION_ID_end_station'], errors='ignore', inplace=True)\n",
        "\n",
        "\n",
        "# Count trips by start station capacity category\n",
        "trip_distribution_by_capacity = trips_with_station_size['start_station_size'].value_counts().reset_index()\n",
        "trip_distribution_by_capacity.columns = ['Station Capacity Category', 'Number of Trips']\n",
        "\n",
        "# Define category order\n",
        "category_order = ['Small', 'Average', 'Large']\n",
        "trip_distribution_by_capacity['Station Capacity Category'] = pd.Categorical(\n",
        "    trip_distribution_by_capacity['Station Capacity Category'], categories=category_order, ordered=True\n",
        ")\n",
        "trip_distribution_by_capacity = trip_distribution_by_capacity.sort_values('Station Capacity Category')\n",
        "\n",
        "\n",
        "# Create bar chart\n",
        "fig = px.bar(\n",
        "    trip_distribution_by_capacity,\n",
        "    x='Station Capacity Category',\n",
        "    y='Number of Trips',\n",
        "    title='Trip Distribution by Start Station Capacity Category',\n",
        "    labels={'Station Capacity Category': 'Start Station Size', 'Number of Trips': 'Number of Trips'},\n",
        "    color='Station Capacity Category',\n",
        "    category_orders={'Station Capacity Category': category_order} # Enforce the order\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_title='Start Station Capacity Category', yaxis_title='Number of Trips')\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n00YsWw3D92P"
      },
      "source": [
        "### Trip Distribution by Start Station Capacity Category\n",
        "\n",
        "This bar chart displays the **number of trips** originating from stations grouped by their **capacity size**:\n",
        "\n",
        "* **Small**\n",
        "* **Average**\n",
        "* **Large**\n",
        "\n",
        "#### **Key Insights:**\n",
        "\n",
        "1. **Average-Capacity Stations Lead in Usage**\n",
        "\n",
        "   * With nearly **700,000 trips**, average-size stations dominate trip origination.\n",
        "   * Indicates a **sweet spot** in terms of infrastructure: large enough to support volume, yet compact enough to be conveniently located.\n",
        "\n",
        "2. **Small Stations Show Strong Engagement**\n",
        "\n",
        "   * Surprisingly, **small stations** come second, with over **350,000 trips**.\n",
        "   * Suggests good utilization of micro-mobility infrastructure, possibly in high-density areas or residential zones.\n",
        "\n",
        "3. **Underperformance of Large Stations**\n",
        "\n",
        "   * Large-capacity stations contribute the **least number of trips (\\~160,000)**.\n",
        "   * This could indicate:\n",
        "\n",
        "     * Poor placement (e.g., less foot traffic)\n",
        "     * Oversupply of docks\n",
        "     * Underutilized transit hubs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4GFM4egUk3o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# B)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK_yKo3mUtU4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task 1\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_mATqblU0nY"
      },
      "source": [
        "| Method                     | Formula                         | Notes                               |\n",
        "| -------------------------- | ------------------------------- | ----------------------------------- |\n",
        "| **Sturges’ Rule**          | `bins = ceil(log2(n) + 1)`      | Good for small to medium-sized data |\n",
        "| **Freedman–Diaconis Rule** | `bin_width = 2 * IQR / n^(1/3)` | Good for skewed data or outliers    |\n",
        "| **Square Root Rule**       | `bins = sqrt(n)`                | Simple and often a good baseline    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8HlhefNL3gb"
      },
      "outputs": [],
      "source": [
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedman–Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "# Static histogram\n",
        "fig = go.Figure(\n",
        "    data=[go.Histogram(\n",
        "        x=durations,\n",
        "        nbinsx=bin_count,\n",
        "        marker_color='blue',\n",
        "        opacity=1.0\n",
        "    )]\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Distribution of Trip Duration (in Minutes)\",\n",
        "    xaxis_title=\"Trip Duration (minutes)\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    bargap=0.05,\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG-L58E8ESel"
      },
      "source": [
        "How many trips took more then a day ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88-AjkyzEU6m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Choose your cutoff (in minutes)\n",
        "cutoff = 1440  # Modify as needed\n",
        "\n",
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedman–Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "\n",
        "# Count how many trips exceed the cutoff\n",
        "sampled_exceed = (sampled_df['trip_duration_minutes'] > cutoff).sum()\n",
        "full_exceed = (trips_df['trip_duration_minutes'] > cutoff).sum()\n",
        "\n",
        "print(f\"Trips in sampled_df exceeding {cutoff} minutes: {sampled_exceed}\")\n",
        "print(f\"Trips in trips_df exceeding {cutoff} minutes: {full_exceed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36wNkRedY3sL"
      },
      "source": [
        "insights :\n",
        "1. The massive bar near 0-20 minutes clearly shows that most bike trips are very short. This is typical for bike-sharing systems, often used for short commutes or quick errands.\n",
        "2. The presence of bars, even if very short, extending all the way to 1440 minutes shoes that some trips in the data did take more then a day , the number of trips is 361\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78CKfIqMzVr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCw3707wXM-6"
      },
      "outputs": [],
      "source": [
        "# Use the original (not divided) trip durations\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "types = sampled_df['rideable_type']\n",
        "\n",
        "# Build the box plot grouped by rideable_type\n",
        "fig = go.Figure()\n",
        "\n",
        "# Loop through each rideable type and add a box\n",
        "for bike_type in sampled_df['rideable_type'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['rideable_type'] == bike_type]['trip_duration_minutes'],\n",
        "        name=bike_type,\n",
        "        boxpoints='outliers',  # show outliers only\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Rideable Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Rideable Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlLYvhqMbi93"
      },
      "source": [
        "insights :\n",
        "1. both types show a very compact box near 20 minutes, indicating that the vast majority of trips for both bike types are quite short.\n",
        "2. The median line is very close to the bottom of the box, confirming heavy right-skewness, this means almost all of the middle 50% of data is concentrated very close to the lower end, and the remaining data (up to Q3) is more spread out.\n",
        "3. The green dots above the whiskers clearly represent the longer  trips with some of them above the 1440 line\n",
        "4. Electric bikes seem to have a slightly tighter distribution,This suggests that while both have short typical trips, classic bikes might have a slightly wider range of trips durations\n",
        "5. Both bike types exhibit very long duration \"outliers,\" with classic bikes potentially having more extreme longer-duration outliers , It suggests that while electric bikes facilitate shorter, perhaps faster trips, classic bikes are used for the most extended journeys.\n",
        "This could be due to factors like cost , battery limits, or simply user preference ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ5QqIOIM4mz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1ZEPTa6MwH-"
      },
      "outputs": [],
      "source": [
        "trips_df['member_casual'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpLjgXVjMiwf"
      },
      "outputs": [],
      "source": [
        "urations = sampled_df['trip_duration_minutes']\n",
        "types_to_group_by = sampled_df['member_casual']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for member_type in sampled_df['member_casual'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['member_casual'] == member_type]['trip_duration_minutes'],\n",
        "        name=member_type,\n",
        "        boxpoints='outliers',\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Member Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Member Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MadmH7TDdwd1"
      },
      "source": [
        "insights :\n",
        "1. As with the previous duration plots, both \"casual\" and \"member\" trips show an incredibly strong right-skewness. The box for both categories is extremely compact and squashed at the very bottom of the plot (close to 0 minutes), and the median line is practically on top of the first quartile (Q1). This reinforces that the vast majority of trips for both casual and member users are very short.\n",
        "2. While both are low, the median (and Q1/Q3) for 'casual' riders appears slightly higher (or at least, the box is marginally less squashed) than for 'member' riders. This suggests that a \"typical\" trip for a casual user is slightly longer than for a member, even if both are still relatively short.\n",
        "3. Both categories clearly have a significant number of \"outlier\" trips (the green dots) that extend far beyond the main box and whiskers, indicating that very long trips do occur for both types of users.\n",
        "4. The box (IQR) for 'casual' riders seems marginally wider than for 'member' riders,The whiskers for 'casual' riders also appear slightly longer, This implies casual riders exhibit a greater range of typical trip durations compared to members.\n",
        "5. 'Casual' riders have a significantly higher density of very long outlier trips, 'Member' riders also have long outlier trips, but they are fewer in number and generally do not reach the same extreme durations as those of casual riders.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0wC5GyObKO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFx42HGM6BK"
      },
      "outputs": [],
      "source": [
        "# Counting Trips Longer Than One Day\n",
        "# Define threshold: 1 day = 1440 minutes\n",
        "one_day_minutes = 1440\n",
        "# Filter trips longer than 1 day\n",
        "long_trips_df = trips_df[trips_df['trip_duration_minutes'] > one_day_minutes]\n",
        "long_sampled_df = sampled_df[sampled_df['trip_duration_minutes'] > one_day_minutes]\n",
        "# Show how many there are\n",
        "print(f\"Total number of trips longer than 1 day in full data: {len(long_trips_df)}\")\n",
        "print(f\"Total number of trips longer than 1 day in sampled data: {len(long_sampled_df)}\")\n",
        "# Combine start and end station counts for long trips\n",
        "start_counts = long_trips_df['start_station_id'].value_counts()\n",
        "end_counts = long_trips_df['end_station_id'].value_counts()\n",
        "\n",
        "# Combine them into a single Series\n",
        "total_counts = start_counts.add(end_counts, fill_value=0).astype(int)\n",
        "\n",
        "# Get station info: name and location\n",
        "stations = sampled_df[['start_station_id', 'start_station_name', 'start_lat', 'start_lng']].drop_duplicates()\n",
        "stations = stations.rename(columns={\n",
        "    'start_station_id': 'station_id',\n",
        "    'start_station_name': 'station_name',\n",
        "    'start_lat': 'lat',\n",
        "    'start_lng': 'lng'\n",
        "})\n",
        "\n",
        "# Merge with counts\n",
        "stations['long_trip_count'] = stations['station_id'].map(total_counts).fillna(0).astype(int)\n",
        "\n",
        "# Filter stations with at least 1 long trip\n",
        "stations = stations[stations['long_trip_count'] > 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVJdaV5bO0sH"
      },
      "outputs": [],
      "source": [
        "# Center the map on Washington DC\n",
        "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Optional: cluster points\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add stations to the map\n",
        "for _, row in stations.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['lat'], row['lng']],\n",
        "        radius=3 + row['long_trip_count']**0.5,  # scale marker size\n",
        "        color='darkred',\n",
        "        fill=True,\n",
        "        fill_color='crimson',\n",
        "        fill_opacity=0.7,\n",
        "        popup=f\"{row['station_name']}<br>Trips > 1 day: {row['long_trip_count']}\"\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sab-rkVxVoJ"
      },
      "source": [
        "insights :\n",
        "1.  primary observation is that stations associated with long-duration trips are particularly concentrated around the central part WDC map ,This suggests that while very long trips are rare overall, they are not uniformly distributed but rather originate from or end in specific zones.\n",
        "2. The 2496 is the count of unique station IDs that appear as either a start_station_id OR an end_station_id within those 361 trips.\n",
        "This is a significant number of stations involved, considering that the total number of such trips was only 361. This means these long trips are spread across a wide variety of stations, rather than being concentrated at just a few specific locations.\n",
        "3. This highlights areas where the system might need to adapt to different user behaviors like offering specific \"long-term rental\" options or different pricing for these stations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehZahS1cJPcY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# C)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXvAYNEvD92R"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task1\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9QFcuybD92R"
      },
      "outputs": [],
      "source": [
        "len(sampled_df['trip_cost'].unique())\n",
        "\n",
        "trips_df.columns\n",
        "len(sampled_df[sampled_df['start_month'] != sampled_df['end_month']])\n",
        "sampled_df['start_time'] = pd.to_datetime(sampled_df['start_time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARmddmRyD92R"
      },
      "outputs": [],
      "source": [
        "# import plotly.express as px\n",
        "\n",
        "# # cost Histogram\n",
        "# fig = px.histogram(sampled_df, x='trip_cost', nbins=141, title='distrupation of trips cost')\n",
        "# fig.show()\n",
        "\n",
        "# # cost Boxplot\n",
        "# fig = px.box(sampled_df, y='trip_cost', title='Boxplot of trips cost')\n",
        "# fig.show()\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "# Extract the data\n",
        "trip_costs = sampled_df['trip_cost'].dropna()\n",
        "\n",
        "# Freedman–Diaconis rule\n",
        "q75, q25 = np.percentile(trip_costs, [75 ,25])\n",
        "iqr = q75 - q25\n",
        "n = len(trip_costs)\n",
        "\n",
        "# Avoid divide by zero\n",
        "if iqr == 0:\n",
        "    nbins = 50  # fallback value\n",
        "else:\n",
        "    bin_width = 2 * iqr / (n ** (1/3))\n",
        "    data_range = trip_costs.max() - trip_costs.min()\n",
        "    nbins = max(1, int(np.ceil(data_range / bin_width)))\n",
        "\n",
        "# Histogram using Freedman–Diaconis bin count\n",
        "fig = px.histogram(sampled_df, x='trip_cost', nbins=nbins, title='Distribution of Trip Costs (Freedman–Diaconis Rule)')\n",
        "fig.show()\n",
        "\n",
        "# Boxplot (no change needed)\n",
        "fig = px.box(sampled_df, y='trip_cost', title='Boxplot of Trip Costs')\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWYoCZwbD92S"
      },
      "source": [
        "- نلاحظ ان اغلب الداتا متوزعة بين ال0 - وال10 دولار بكثرة وان القمة بين 3.5 و4 وهذا يدل على انه يوجد الكثير من الناس مشتركة واغلب الرحل لا تتجاوز ال45 دقيقة\n",
        "\n",
        "- وايضا يوجد قيم اكبر صحيح انها نادرة ولكنها متوزعة وهذا يدل انه يوجد اشخاص تاخذها لمسافات كبيرة ولكنها قليلة  \n",
        "- غالبا الرحل ذات تكلفة العالية اشخاص غير مشتركين بالاضافة الى انهم قد يكونون مرة واحدة فقط يستخدمون الدراجات ولا يعودون الى استخدامها بعد تجربة الخدمة ورؤية السعر\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKOPBMBAD92S"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTV6PC1PD92S"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(sampled_df, x='trip_duration_minutes', y='trip_cost', trendline='ols',title='the realtion between duration and cost')\n",
        "fig.show()\n",
        "# lowess', 'rolling', 'ewm', 'expanding', 'ols'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBUm5LMkD92S"
      },
      "source": [
        "\n",
        "\n",
        "*   النقاط التي قيمتها قريبة من الصفر كوقت هي تمثل الاعضاء التي لديهم اشتراك ولم يتجاوزوا ال45 دقيقة وكما نلاحظ هم كثر\n",
        "*   ولدينا ثلاث توزعات للنقاط وذلك يعود بسبب الاشتراك او عدمه وحتى مروره بالمنطقة التجارية\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbJS-eEJD92S"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cVuorwdD92S"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(sampled_df, x='temp', y='trip_cost', color='member_casual',\n",
        "                 title='cost vs temperatur ')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8clzEFAhD92S"
      },
      "source": [
        "\n",
        "\n",
        "*   اغلب الرحلات تكون بين 5 درجات وال20 درجة\n",
        "*   عندما تكون درجة الحرارة فوق ال20 نلاحظ ان عدد الرحلات قليل\n",
        "* كما نلاحظ اغلب رحل المشتركين الكلفة غالبا اقل من 10 دولار\n",
        "* نلاحظ ان اغلب الكلف العالية من الغير المشتركين\n",
        "* لايوجد علاقة واضحة بين درجة الحرارة والتكلفة لكن يمكن الفول ان بين ال 5 -15 يمكن للناس ان تذهب برحلات أطول\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORcbeymnD92S"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4my_ddWD92S"
      },
      "outputs": [],
      "source": [
        "daily_rev = sampled_df.groupby(sampled_df['start_time'].dt.date)['trip_cost'].sum().reset_index(name='revenue')\n",
        "fig = px.line(daily_rev, x='start_time', y='revenue', title='daily incomes')\n",
        "fig.show()\n",
        "\n",
        "sampled_df['week'] = sampled_df['start_time'].dt.isocalendar().week\n",
        "weekly_rev = sampled_df.groupby('week')['trip_cost'].sum().reset_index(name='revenue')\n",
        "fig = px.line(weekly_rev, x='week', y='revenue', title='weekly incomes')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB4EhTvaD92S"
      },
      "source": [
        "\n",
        "\n",
        "*   بالنسبة للايرادات اليومبة نلاحظ وجود بين هبوط وصعود ومع استمرار الايام نلاحظ زيادة بالدخل ونلاحظ تناوب بين صعود وهبوط في الايام ويعود هذا الامر اتوقع انو شخص يلي بيركب يوم بريح اليوم يلي بعدو\n",
        "\n",
        "*   لدينا بشهر april هبوط واضح في الربح السبب قد يعود الى عدم وجود داتا كافية في هذا الشهر\n",
        "\n",
        "* بالنسبة للايرادات الاسبوعية ملاحظ انه بشكل عام الامور نحو زيادة حيث ان هذا التذبذب راح بسبب انو الاسبوعي عطانا الشكل العام بالاسبوع فاصبح خط  اكثر انسيابية\n",
        "\n",
        "* بشكل عام يوجد مشكلة في شهر april\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTi-juCSD92T"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task5\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUVr2BsoD92T"
      },
      "outputs": [],
      "source": [
        "monthly_rev = sampled_df.groupby('start_month')['trip_cost'].mean().reset_index(name='avg_revenue')\n",
        "fig = px.line(monthly_rev, x='start_month', y='avg_revenue', title='average month income')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUrGoHTDD92T"
      },
      "source": [
        "* يبين لنا المخطط تطور متوسط تكلفة الرحلة الواحدة خلال شهر الاول كان متوسط تكلفة الرحلة ما يقارب 3.78 دولار مع دخول الشهر الثاني نلاحظ ارتفاع طفيف ويستمر الارتفاع بشكل طفيف حتى الشهر الثالث هذا النمو التدريجي يوحي بان شيئاً ما كان يتغير ببطء وثبات ربما كان المستخدمون يميلون لأخذ رحلات أطول قليلًا، أو أن هناك زيادة طفيفة في استخدام الدراجات ذات التكلفة الأعلى، أو ربما كان هناك تزايد في الرحلات التي تتخطى الحدود الزمنية المجانية للمشتركين وتتحمل رسومًا إضافية. هذه الزيادة، وإن كانت صغيرة، تشير إلى أن قيمة الرحلة الواحدة كانت في ازدياد\n",
        "* ثم نصل الى شهر الرابع نلاحظ قفزة في متوسط الرحلة الواحدة بشكل ملحوظ حيث وصل ال4 دولار مسجل اعلى متوسط خلال هذه الفترة قد يبدو للحظة ان الامر جيد ولكن مع النظر الى مخطط اليومي والاسبوعي فنحدد شهدنا هبوط في هذا الشهر وقد يعود سبب الهبوط في رفع سعر الرحلة مما ادى الانهيار الخدمة انهياراً كارثياً\n",
        "\n",
        "* وايضا ممكن هذا الارتفاع اتى بما انه عدد الرحلات الاجمالية في الشهر الرابع قليلة فوجود قيم شاذة او مرتفعة كما شهدنا في مخطط كلف الرحل سيرفع متوسط كلفة الرحلة بهذا الشكل\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **الخلاصة**\n",
        "\n",
        "\n",
        "*   كانت خدمة مشاركة الدراجات تشهد نموًا مستمرًا في إجمالي إيراداتها وفي قيمة الرحلة الواحدة من يناير وحتى منتصف مارس.\n",
        "*   مع ذلك، في أواخر مارس/أوائل أبريل، قد تكون الشركة تعرضت لحدث جسيم (إما إغلاق، أو تعليق، أو عطل كبير في النظام) أدى إلى توقف شبه كامل لجميع الرحلات والإيرادات, او قد يكون بسبب رفع رسوم الرحلة\n",
        "* القفزة في متوسط تكلفة الرحلة في أبريل، على الرغم من أنها تبدو إيجابية في هذا الرسم البياني بمفرده، هي في الواقع مجرد انعكاس لحقيقة أن الرحلات القليلة جدًا المتبقية كانت هي الأكثر تكلفة، مما يلقي الضوء على الوضع الكارثي للخدمة في هذا الشهر.\n",
        "\n",
        "* قد يكون سبب اخذ قرار الشركة برفع انها كانت تحاول رفع الرسوم في الاشهر الاولى ولكن بشكل طفيف وعندما وجدت ان المبيعات تزاد قامت بهذه الرفعة ظنا منها انه اصبح لديها قاعدة جماهيرية كبيرة وان المستخدمين بازدياد لتفاجئ بحصول عكس ذلك تماما\n",
        "* كل هذه الامور هي مجرد تفسيرات ممكنة\n",
        "\n",
        "* قد يكون سبب الارتفاع هو وجود تضخم\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOP_nkSnJTVM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# D)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWjBMMLQz1C"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task1\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnB_ydw33K-z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- (Your existing data preparation for res_points) ---\n",
        "# Assuming 'sampled_df' and 'Residential_Visitor_Parking_Zones' are loaded\n",
        "\n",
        "# Step 0: Load residential zones GeoDataFrame\n",
        "res_zones = Residential_Visitor_Parking_Zones\n",
        "res_zones = res_zones.to_crs(epsg=4326)\n",
        "\n",
        "# Step 1: Create GeoDataFrames for start and end points (using sampled_df)\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['start_lng'], sampled_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['end_lng'], sampled_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "# Step 2: Spatial join to check which points fall inside residential zones\n",
        "start_in_res = gpd.sjoin(start_gdf, res_zones, predicate='within', how='inner')\n",
        "end_in_res = gpd.sjoin(end_gdf, res_zones, predicate='within', how='inner')\n",
        "\n",
        "# Step 3: Extract lat/lon of trips touching residential zones\n",
        "res_start_points = start_in_res[['start_lat', 'start_lng']].rename(columns={'start_lat': 'lat', 'start_lng': 'lon'})\n",
        "res_end_points = end_in_res[['end_lat', 'end_lng']].rename(columns={'end_lat': 'lat', 'end_lng': 'lon'})\n",
        "res_points = pd.concat([res_start_points, res_end_points], ignore_index=True)\n",
        "\n",
        "# --- Load CBD Data and ensure correct CRS (EPSG:4326) ---\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=4326) # Ensure CBD is also in EPSG:4326\n",
        "\n",
        "# --- Create the Plotly Graph Object Figure ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the Heatmap Trace\n",
        "fig.add_trace(go.Densitymapbox(\n",
        "    lat=res_points['lat'],\n",
        "    lon=res_points['lon'],\n",
        "    radius=10, # Adjust radius as needed for visual effect\n",
        "    colorscale=\"Viridis\", # Or \"Jet\", \"Hot\", \"Portland\", etc. for heatmap colors\n",
        "    hoverinfo=\"skip\"\n",
        "))\n",
        "\n",
        "# Add Residential Zones as a GeoJSON layer\n",
        "fig.update_layout(\n",
        "    mapbox_layers=[\n",
        "        {\n",
        "            \"below\": 'traces',\n",
        "            \"sourcetype\": \"geojson\",\n",
        "            \"source\": json.loads(res_zones.to_json()),\n",
        "            \"type\": \"line\",\n",
        "            # \"line\": {\"width\": 1, \"color\": \"blue\"} # Corrected: used \"color\" within \"line\" object\n",
        "        },\n",
        "        # Add CBD as another GeoJSON layer\n",
        "        {\n",
        "            \"below\": 'traces',\n",
        "            \"sourcetype\": \"geojson\",\n",
        "            \"source\": json.loads(CBD.to_json()),\n",
        "            \"type\": \"line\",\n",
        "            # \"line\": {\"width\": 2, \"color\": \"green\"} # Corrected: used \"color\" within \"line\" object\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Update layout for map style, center, zoom, and title\n",
        "fig.update_layout(\n",
        "    title_text='Geographic Heatmap of Trips to Residential Zones with Borders',\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    mapbox_zoom=10,\n",
        "    mapbox_center = {\"lat\": res_points['lat'].mean(), \"lon\": res_points['lon'].mean()},\n",
        "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
        ")\n",
        "\n",
        "# Render statically (Plotly handles this directly when fig.show() is called in a notebook)\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdhQF2Fi7lup"
      },
      "source": [
        "insights :\n",
        "1. The most prominent insight is the very high density (bright yellow areas) of bike trips that overwhelmingly coincides with and largely respects the boundaries of the residential zones.\n",
        " This indicates that the bike-sharing system is heavily used for purposes directly related to residential areas, whether starting from, ending in, or traversing through them.\n",
        "2. The highest concentration of trips within residential zones appears to be in the central and northern parts of Washington D.C., This suggests these are the most active residential areas for bike-sharing usage.\n",
        "3. While the overall pattern is dense,These could indicate areas with fewer stations, different demographic profiles, or less need for bike-sharing, which could be further explored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZTM_ZuT-zD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY_LY76_J5iH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Count trips per geohash sector\n",
        "geohash_counts = sampled_df['geohash_p6'].value_counts().reset_index()\n",
        "geohash_counts.columns = ['geohash_p6', 'trip_count']\n",
        "\n",
        "# Optional: sort alphabetically or by count\n",
        "geohash_counts = geohash_counts.sort_values(by='trip_count', ascending=False)\n",
        "\n",
        "# Step 2: Plot\n",
        "fig = px.bar(\n",
        "    geohash_counts,\n",
        "    x='geohash_p6',\n",
        "    y='trip_count',\n",
        "    title='Distribution of Trips by Geographic Sector (Geohash_p6)',\n",
        "    labels={'geohash_p6': 'Geographic Sector', 'trip_count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "# Step 3: Turn off interactivity\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEbLkSru_yc6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- Your Function to Convert Geohash to Polygon ---\n",
        "def geohash_to_polygon(geohash_str):\n",
        "    \"\"\"\n",
        "    Converts a geohash string to a shapely Polygon representing its bounding box.\n",
        "    \"\"\"\n",
        "    lat, lon, lat_err, lon_err = gh.decode_exactly(geohash_str)\n",
        "    min_lat = lat - lat_err\n",
        "    max_lat = lat + lat_err\n",
        "    min_lon = lon - lon_err\n",
        "    max_lon = lon + lon_err\n",
        "    coords = [\n",
        "        (min_lon, min_lat),\n",
        "        (max_lon, min_lat),\n",
        "        (max_lon, max_lat),\n",
        "        (min_lon, max_lat),\n",
        "        (min_lon, min_lat)  # Close the polygon\n",
        "    ]\n",
        "    return Polygon(coords)\n",
        "\n",
        "# --- Sample Input: Top 5 Geohashes (replace with your real data) ---\n",
        "top_5_geohashes = geohash_counts.head(5)['geohash_p6'].tolist()\n",
        "geohash_polygons = [geohash_to_polygon(g) for g in top_5_geohashes]\n",
        "\n",
        "# --- Convert to GeoDataFrame for easier plotting ---\n",
        "gdf = gpd.GeoDataFrame(geometry=geohash_polygons, crs='EPSG:4326')\n",
        "\n",
        "# --- Plot using Plotly ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add each polygon as a filled scattermapbox trace\n",
        "for poly in gdf.geometry:\n",
        "    lons, lats = poly.exterior.coords.xy\n",
        "    fig.add_trace(go.Scattermapbox(\n",
        "        lon=list(lons),\n",
        "        lat=list(lats),\n",
        "\n",
        "        mode='lines',\n",
        "        fill='toself',\n",
        "        fillcolor='rgba(255, 0, 0, 0.4)',  # Semi-transparent red\n",
        "        line=dict(width=1, color='red'),\n",
        "        hoverinfo='skip',\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "# Map settings\n",
        "fig.update_layout(\n",
        "    mapbox_style='carto-positron',\n",
        "    mapbox_zoom=12,\n",
        "    mapbox_center={\"lat\": gdf.geometry.centroid.y.mean(), \"lon\": gdf.geometry.centroid.x.mean()},\n",
        "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\n",
        ")\n",
        "\n",
        "# Static rendering (no interactions)\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE98VnXv-ww7"
      },
      "source": [
        "insights :\n",
        "1.  The first few bars on the left are significantly taller than the rest, with the tallest bar approaching 800 trips, while many others have less than 50. This indicates that bike-sharing activity is heavily concentrated in a few high-demand areas.\n",
        "2. the plot allows for quick identification of the highest-demand geographic sectors. These are likely to correspond to areas with high population density, major transit hubs, popular attractions, or dense commercial/office districts\n",
        "3. we ploted these top sectors and we compared it the heatmap in task1 , and we are now sure that they are in hotspots for trips in residential zone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ZXZV3XVa7n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbmejUVrdzbO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def freedman_diaconis_bins(data):\n",
        "    data = data.dropna()\n",
        "    q75, q25 = np.percentile(data, [75 ,25])\n",
        "    iqr = q75 - q25\n",
        "    n = len(data)\n",
        "    if n == 0 or iqr == 0:\n",
        "        return 10  # fallback if data is empty or IQR is zero\n",
        "    bin_width = 2 * iqr / (n ** (1/3))\n",
        "    if bin_width == 0:\n",
        "        return 10\n",
        "    bin_count = int(np.ceil((data.max() - data.min()) / bin_width))\n",
        "    print(\"Bin count : \", bin_count)\n",
        "    return bin_count\n",
        "\n",
        "\n",
        "# Calculate bins for each variable\n",
        "bins_cbd = freedman_diaconis_bins(sampled_df['distance_to_cbd_m'])\n",
        "bins_metro = freedman_diaconis_bins(sampled_df['start_nearest_metro_distance'])\n",
        "bins_shuttle = freedman_diaconis_bins(sampled_df['start_nearest_shuttle_distance'])\n",
        "\n",
        "sampled_df['distance_to_cbd_km'] = sampled_df['distance_to_cbd_m'] / 1000\n",
        "\n",
        "# 1. Distance to CBD\n",
        "fig1 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='distance_to_cbd_km',\n",
        "    nbins=bins_cbd,\n",
        "    title='Distribution of Distance to CBD (km)',\n",
        "    labels={'distance_to_cbd_km': 'Distance to CBD (km)'}\n",
        ")\n",
        "fig1.show(config={'staticPlot': True})\n",
        "\n",
        "# 2. Closest Metro Station Distance\n",
        "fig2 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_metro_distance',\n",
        "    nbins=bins_metro,\n",
        "    title='Distribution of Distance to Nearest Metro Station',\n",
        "    labels={'start_nearest_metro_distance': 'Distance to Metro (km)'}\n",
        ")\n",
        "fig2.show(config={'staticPlot': True})\n",
        "\n",
        "# 3. Closest Shuttle Station Distance\n",
        "fig3 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_shuttle_distance',\n",
        "    nbins=bins_shuttle,\n",
        "    title='Distribution of Distance to Nearest Shuttle Station',\n",
        "    labels={'start_nearest_shuttle_distance': 'Distance to Shuttle (km)'}\n",
        ")\n",
        "fig3.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B_a704MjIGH"
      },
      "source": [
        "insights :\n",
        "1. The overwhelming majority of trips (as indicated by the highest bars) occur within approximately 0 to 4 kilometers of the Central Business District. The peak frequency appears to be around 2.2 kilometers, This reinforces the CBD's role as a major hub for bike-sharing activity.\n",
        "2. this strongly supports the idea that the bike-sharing system primarily serves \"last-mile\" transportation, short-distance commutes, or intra-CBD movement.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. The histogram displays a highly skewed distribution, with an extremely high frequency of trips originating or ending very close to Metro stations. The counts drop off sharply as the distance increases.\n",
        "2. This insight highlights the importance of placing bike-sharing stations in very close proximity to Metro entrances to maximize their integration with the public transit network and serve commuter needs effectively.\n",
        "\n",
        "\n",
        "---\n",
        "1.  Similar to the Metro station plot, this histogram shows a highly skewed distribution, with the highest frequency of trips occurring very close to shuttle stations.\n",
        "2. This suggests a consistent user behavior of using bike-sharing to bridge short gaps to fixed public transport points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zxf96lZssHE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63xLkRdqrujr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Categorize trips\n",
        "def classify_trip(row):\n",
        "    if row['start_in_cbd'] == 1 and row['end_in_cbd'] == 1:\n",
        "        return 'Fully in CBD'\n",
        "    else:\n",
        "        return 'Outside CBD'\n",
        "\n",
        "# Apply classification\n",
        "sampled_df['cbd_trip_type'] = sampled_df.apply(classify_trip, axis=1)\n",
        "\n",
        "# Count\n",
        "trip_cbd_counts = sampled_df['cbd_trip_type'].value_counts().reset_index()\n",
        "trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    trip_cbd_counts,\n",
        "    x='Trip Type',\n",
        "    y='Count',\n",
        "    title='Trips Fully in CBD vs Outside',\n",
        "    text='Count',\n",
        "    labels={'Count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(yaxis_title='Number of Trips', xaxis_title='Trip Category')\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoWwMMOttvP4"
      },
      "outputs": [],
      "source": [
        "trips_df['cbd_trip_type'] = trips_df.apply(classify_trip, axis=1)\n",
        "full_trip_cbd_counts = trips_df['cbd_trip_type'].value_counts().reset_index()\n",
        "full_trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "full_trip_cbd_counts['Percentage'] = (full_trip_cbd_counts['Count'] / full_trip_cbd_counts['Count'].sum()) * 100\n",
        "full_trip_cbd_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEClq4S-lzUe"
      },
      "source": [
        "insights :\n",
        "1. The vast majority of trips  fall into the \"Outside CBD\" category, This indicates that the bike-sharing system primarily serves a broader geographical area beyond just the core CBD, or facilitates connections into/out of it rather than exclusively within it.\n",
        "3.  A significantly smaller number of trips (1,243 trips) were classified as \"Fully in CBD\". This suggests that while there is some intra-CBD movement, it represents a much smaller proportion\n",
        "4. This distribution implies that the bike-sharing service's primary function might not be solely for quick hops within the CBD itself, but rather for facilitating commutes, errands, or leisure travel that connects to or traverses the CBD from other parts of the city. This aligns with the \"first/last mile\" insights from the distance histograms, where trips are often connecting to major transit points or residential areas that might lie outside the strict CBD boundaries.\n",
        "5.  The fact that over 93% of all the trips involve areas outside the CBD strongly reinforces the idea that bike-sharing is primarily used as a \"first/last mile\" solution, connecting users to or from transit hubs (like Metro and Shuttle stations) or residential areas that often lie outside the strict CBD boundaries. These trips are unlikely to be exclusively within the CBD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbWLXpMNuCVn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task5\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re3aHjZmtmpv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter trips that passed through CBD\n",
        "cbd_passed_df = sampled_df[\n",
        "    (sampled_df['start_in_cbd'] == 1) | (sampled_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    grouped,\n",
        "    x='rideable_type',\n",
        "    y='trip_count',\n",
        "    color='member_casual',\n",
        "    barmode='group',\n",
        "    title='Trips That Passed Through CBD by Rideable Type and Membership',\n",
        "    labels={'trip_count': 'Number of Trips', 'rideable_type': 'Bike Type'}\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Rideable Type',\n",
        "    yaxis_title='Number of Trips'\n",
        ")\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE6Ah0JBuxCX"
      },
      "outputs": [],
      "source": [
        "cbd_passed_df_trips_df=trips_df[\n",
        "    (trips_df['start_in_cbd'] == 1) | (trips_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df_trips_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "print(f\"Length of cbd_passed_df_trips_df: {len(cbd_passed_df_trips_df)}\")\n",
        "print(f\"Length of trips_df: {len(trips_df)}\")\n",
        "\n",
        "percentage = (len(cbd_passed_df_trips_df) / len(trips_df)) * 100\n",
        "print(f\"Percentage of cbd_passed_df_trips_df compared to trips_df: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH3oXctYnxyk"
      },
      "source": [
        "insights :\n",
        "1. the extra exploration shows that 29.85% of all bike-sharing trips pass through the CBD, This is a substantial portion, indicating the CBD's critical role as an origin, destination, or transit point for a large segment of bike-sharing users.\n",
        "2. For both classic_bike and electric_bike categories, members (orange/red bars) consistently account for a significantly higher number of trips that pass through the CBD compared to casual riders (blue bars). This suggests that regular commuters or frequent users (members) are more likely to utilize bike-sharing for travel involving the city's central business district.\n",
        "3. Both casual and member riders use classic_bike more frequently for CBD-involved trips than electric_bike\n",
        "4. Since members are the primary users for CBD-involved trips, strategies to retain and grow membership, and ensure sufficient bike availability in and around the CBD, are crucial.\n",
        "5. The demand for both classic and electric bikes for CBD-involved trips means that fleet management should consider a balanced distribution to meet the preferences of both member and casual riders.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUGoOn14yROg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task6\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmcXeVxLv7yx"
      },
      "outputs": [],
      "source": [
        "# Create a contingency table\n",
        "# (Counts of each combination)\n",
        "# Make sure we’re using categorical data\n",
        "contingency_table = pd.crosstab(trips_df['close_to_cbd'], trips_df['member_casual'])\n",
        "contingency_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00075kXbydNE"
      },
      "outputs": [],
      "source": [
        "# Run chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "print(\"Chi2 Statistic:\", chi2)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"P-value:\", p)\n",
        "# interpretion based on the p-value:\n",
        "if p < 0.05:\n",
        "    print(\" There is a significant correlation between distance to CBD segments and membership type.\")\n",
        "else:\n",
        "    print(\" No significant correlation found between distance to CBD segments and membership type.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIDvh1wlz_gr"
      },
      "source": [
        "| α Value  | Interpretation                                                                |\n",
        "| -------- | ----------------------------------------------------------------------------- |\n",
        "| **0.05** | Most common — means you're willing to accept a 5% chance of a false positive. |\n",
        "| 0.01     | Stricter — used in more critical fields (medicine, etc.).                     |\n",
        "| 0.10     | Looser — sometimes used in exploratory research.                              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0oXDOroztOm"
      },
      "source": [
        "insights:\n",
        "1. The p-value of 0.0, which is much less than the conventional significance level of 0.05 leads us to believe there is a statistically significant relationship between whether a trip is close to the CBD and the user's membership type.\n",
        "2. While the Chi-square test indicates strong statistical significance (due to the very large sample size), the proportional difference between casual and member trips being close_to_cbd is relatively small,This suggests thatBoth member and casual riders have roughly half their trips originating or ending close to the CBD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VNGH_GvD92V"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# E)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvbtIYccD92V"
      },
      "source": [
        "---\n",
        "Task 1\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZA1P3kLD92W"
      },
      "outputs": [],
      "source": [
        "sampled_df['rideable_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAGnkfzYD92W"
      },
      "outputs": [],
      "source": [
        "daily_weather_avg = sampled_df.groupby('date')[['temp', 'humidity', 'windspeed']].mean().reset_index()\n",
        "daily_weather_avg = daily_weather_avg.rename(columns={\n",
        "    'temp': 'Average Temperature',\n",
        "    'humidity': 'Average Humidity',\n",
        "    'windspeed': 'Average Wind Speed'\n",
        "})\n",
        "fig = px.line(\n",
        "    daily_weather_avg,\n",
        "    x='date',\n",
        "    y=['Average Temperature', 'Average Humidity', 'Average Wind Speed'], # List of columns for y-axis\n",
        "    title='Average Daily Weather Conditions (Temperature, Humidity, Wind Speed)',\n",
        "    labels={\n",
        "        'date': 'Date',\n",
        "        'value': 'Average Value', # Default label for the combined y-axis values\n",
        "        'variable': 'Metric'     # Default label for the legend (which variable is which line)\n",
        "    }\n",
        ")\n",
        "fig.update_layout(hovermode=\"x unified\") # Enhances hover tooltips for multiple lines\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgKFLhPWD92W"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoErRgRVD92W"
      },
      "outputs": [],
      "source": [
        "daily_weather_cond = sampled_df.groupby('date')['weather_segment'].first().reset_index()\n",
        "\n",
        "daily_rev = sampled_df.groupby(sampled_df['date'])['trip_cost'].sum().reset_index(name='revenue')\n",
        "\n",
        "merged_df = pd.merge(daily_rev, daily_weather_cond, on='date', how='left')\n",
        "fig = px.box(\n",
        "    merged_df,\n",
        "    x='weather_segment',  # Categorical variable on x-axis\n",
        "    y='revenue',      # Numerical variable on y-axis\n",
        "    title='Daily Revenue by Weather Condition',\n",
        "    labels={\n",
        "        'weather_condition': 'Weather Condition',\n",
        "        'daily_revenue': 'Daily Revenue ($)'\n",
        "    },\n",
        "    category_orders={\"weather_condition\": [\"Sunny\", \"Cloudy\", \"Rainy\"]} # Optional: ensure specific order\n",
        ")\n",
        "fig.update_traces(boxpoints='all', jitter=0.3) # Show individual points for more detail\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib7igIWFD92W"
      },
      "source": [
        "We observe that on rainy days, the average revenue is around $650, which is lower than the average for all other weather conditions, despite the fact that about 55% of the days are rainy. We also notice that the box is wide, indicating a high variability in revenue on rainy days. This is evident from the fact that some days have revenue close to zero, while others exceed $1000. I believe this is due to very rainy days where riding a bike becomes difficult, leading to very low revenue. On the other hand, some rainy days might be mild, and certain types of users might enjoy riding in light rain or have urgent needs, so they ride bikes regardless, leading to higher revenues.\n",
        "\n",
        "For cloudy days, the average revenue is higher, around $800, which is more than on rainy days. Revenue also seems stable, without much fluctuation. The trend on cloudy days either remains steady or increases, and we notice some very high revenue spikes, likely occurring during moderate temperatures. As for the very low values, they are probably in April or during holidays.\n",
        "\n",
        "Although there are fewer sunny days, we find that people tend to use bikes more, resulting in the highest average revenue. It is noticeable that people prefer riding bikes on sunny days, possibly because sunny days are rare, and people enjoy being out in the sun. The pleasant weather also encourages bike usage.\n",
        "\n",
        "Thus, we conclude that weather significantly impacts rider behavior, clearly showing how weather conditions directly affect daily revenues. People prefer using bikes in moderate and sunny weather, which leads to higher revenues, while rainy days have fewer users and lower revenues.\n",
        "\n",
        "However, the presence of outlier values across all categories suggests that there are always some days that don’t follow the general weather pattern — whether exceptionally good or bad. I assume that on such days, other factors come into play.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42jFITJyD92W"
      },
      "source": [
        "---\n",
        "Task3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pZ555zZD92W"
      },
      "outputs": [],
      "source": [
        "# lowess', 'rolling', 'ewm', 'expanding', 'ols'\n",
        "# --- Apply Min-Max Normalization to 'daily_revenue' ---\n",
        "# xi-xmin /xmax-xmin\n",
        "\n",
        "# min_revenue = daily_rev['revenue'].min()\n",
        "# max_revenue = daily_rev['revenue'].max()\n",
        "# daily_rev['normalized_daily_revenue'] = (daily_rev['revenue'] - min_revenue) / (max_revenue - min_revenue)\n",
        "\n",
        "merg = pd.merge(daily_weather_avg,daily_rev,on='date',how='left')\n",
        "\n",
        "cols_to_normalize = ['revenue', 'Average Temperature', 'Average Humidity']\n",
        "for col in cols_to_normalize:\n",
        "    min_val = merg[col].min()\n",
        "    max_val = merg[col].max()\n",
        "    # Avoid division by zero if all values are the same\n",
        "    if (max_val - min_val) != 0:\n",
        "        merg[f'normalized_{col}'] = (merg[col] - min_val) / (max_val - min_val)\n",
        "    else: # If all values are the same, normalized value is 0 (or 1, depends on convention)\n",
        "        merg[f'normalized_{col}'] = 0.0\n",
        "\n",
        "fig1 = px.scatter(merg,x='normalized_Average Temperature',y='normalized_revenue',\n",
        "                 title=\"relationship between daily income and temperature\",trendline='ols',\n",
        "                 labels={\n",
        "                   'Temperature': 'normalized_Average Daily Temperature',\n",
        "                   'daily_revenue': 'Daily Revenue ($)' }\n",
        "                 )\n",
        "fig1.show()\n",
        "\n",
        "\n",
        "fig2 = px.scatter(merg,x='normalized_Average Humidity',y='normalized_revenue',\n",
        "                 title=\"relationship between daily income and Humidity\",trendline='ols',\n",
        "                 labels={\n",
        "                   'Humidity': 'normalized_Average Daily humidity',\n",
        "                   'daily_revenue': 'Daily Revenue ($)' }\n",
        "                 )\n",
        "fig2.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oEl74UvD92W"
      },
      "source": [
        "Regarding the relationship between daily revenue and temperature, we observe a positive linear correlation. At low temperatures (around -5°C to 3°C), revenues are low. As the temperature increases, revenues increase as well — up to a certain point. Once the temperature exceeds around 16°C, revenues start to decline. So, revenue increases with temperature up to a threshold, beyond which hotter temperatures lead to lower revenues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3qhX7dnD92W"
      },
      "source": [
        "---\n",
        " Task4\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTNhxG69D92W"
      },
      "outputs": [],
      "source": [
        "  # 1. Create the Contingency Table\n",
        "# This table shows the observed frequencies (counts) of each unique combination\n",
        "# of weather segment and ride type.\n",
        "# Rows: weather_segment\n",
        "# Columns: rideable_type\n",
        "contingency_table = pd.crosstab(sampled_df['weather_segment'], sampled_df['rideable_type'])\n",
        "print(\"Contingency Table (Observed Frequencies):\")\n",
        "print(contingency_table)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Visual separator in output\n",
        "\n",
        "# 2. Perform the Chi-Square Test\n",
        "# The chi2_contingency function performs the statistical calculations.\n",
        "# It returns four values:\n",
        "#   - chi2: The calculated Chi-Square statistic.\n",
        "#   - p_value: The probability value (most important for interpretation).\n",
        "#   - dof: Degrees of freedom.\n",
        "#   - expected_frequencies: A 2D array of expected frequencies if the variables were independent.\n",
        "chi2, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi2 Statistic: {chi2:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(\"\\nExpected Frequencies Table:\")\n",
        "\n",
        "# Display the expected frequencies array as a DataFrame for better readability,\n",
        "# using the same indices (rows) and columns as the observed contingency table.\n",
        "print(pd.DataFrame(expected_frequencies, index=contingency_table.index, columns=contingency_table.columns))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Another visual separator\n",
        "\n",
        "\n",
        "# 3. Interpret the Results\n",
        "# Define the significance level (alpha), which is the threshold for comparing the p_value.\n",
        "# A common alpha level is 0.05 (or 5%).\n",
        "alpha = 0.05\n",
        "print(\"Interpretation of Results:\")\n",
        "if p_value < alpha:\n",
        "    # If the p-value is less than alpha, we reject the null hypothesis.\n",
        "    # The null hypothesis (H0) here is: There is no relationship between weather condition and ride type.\n",
        "    print(f\"Since the P-value ({p_value:.4f}) is less than the significance level (alpha = {alpha}),\")\n",
        "    print(\"we reject the null hypothesis (H0).\")\n",
        "    print(\"Conclusion: There is strong statistical evidence of a significant relationship between weather condition and ride type.\")\n",
        "    print(\"In other words, it appears that the distribution of ride types (or bike types) differs depending on the weather condition.\")\n",
        "    print(\"\\n* To understand this relationship further, compare the observed frequencies with the expected frequencies to identify which categories contribute most to the association.\")\n",
        "else:\n",
        "    # If the p-value is greater than or equal to alpha, we fail to reject the null hypothesis.\n",
        "    print(f\"Since the P-value ({p_value:.4f}) is greater than or equal to the significance level (alpha = {alpha}),\")\n",
        "    print(\"we fail to reject the null hypothesis (H0).\")\n",
        "    print(\"Conclusion: There is no sufficient statistical evidence to claim a significant relationship between weather condition and ride type.\")\n",
        "    print(\"In other words, it appears that the choice of ride type (or bike type) is not significantly affected by the weather condition, or any observed differences could be due to random chance.\")\n",
        "\n",
        "\n",
        "df_plot = contingency_table.reset_index().melt(id_vars='weather_segment', var_name='rideable_type', value_name='Count')\n",
        "\n",
        "# 2. Draw a Grouped Bar Chart\n",
        "fig = px.bar(\n",
        "    df_plot,\n",
        "    x='weather_segment',  # X-axis will be weather conditions\n",
        "    y='Count',            # Y-axis will be the number of rides\n",
        "    color='rideable_type',    # Different bars for each ride type within each weather condition\n",
        "    barmode='group',      # This makes the bars for each ride_type stand side-by-side\n",
        "    title='Ride Type Distribution by Weather Condition',\n",
        "    labels={\n",
        "        'weather_segment': 'Weather Condition',\n",
        "        'Count': 'Number of Rides',\n",
        "        'Ride Type': 'Ride Type'\n",
        "    },\n",
        "    category_orders={\"weather_segment\": [\"Sunny\", \"Cloudy\", \"Rainy\"]} # Optional: ensure specific order\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Weather Condition\", yaxis_title=\"Number of Rides\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We hypothesized that there is no relationship between weather conditions and bike type choice.\n",
        "\n",
        "We started by creating a cross-tabulation table with the observed data, followed by calculating the expected values.\n",
        "\n",
        "After running the Chi-square test, we obtained a p-value = 0.0015, which is less than the significance level α = 0.05. This means we can reject the null hypothesis, and thus conclude that there is a relationship — weather conditions influence users' choice of bike type.\n",
        "\n",
        "Although the bar plot we created didn’t show a clear difference, this is where the power of statistical testing shines — it can reveal hidden relationships that are not visually obvious, which is logical.\n",
        "\n",
        "For example, on rainy days, people tend to prefer electric bikes, since they are faster, require less physical effort, and reduce exposure to rain.\n",
        "\n",
        "On the other hand, on sunny or cloudy days, people tend to prefer regular bikes, which makes sense. In pleasant weather, people are more willing to go out and ride, and don’t mind physical exertion.\n",
        "\n",
        "However, we do see some overlap in sunny weather, possibly because sun intensity can be bothersome — which is subjective. That could explain the similar usage rates for electric and regular bikes on sunny days.\n",
        "\n",
        "Overall though, we observe a clear pattern: people prefer regular bikes on cloudy days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNzzEEeD92W"
      },
      "source": [
        "# Catching patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW05ogkaD92X"
      },
      "source": [
        "## A ) Temporal Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-8NYjND92X"
      },
      "source": [
        "\n",
        "### Task 1: Data Verification and Temporal Repairs\n",
        "#### What to do:\n",
        "\n",
        "- Verify that the temporal data you obtained is in the correct chronological order and without any missing intervals\n",
        "- Perform necessary repairs for any temporal issues found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm6gHA2RD92X"
      },
      "outputs": [],
      "source": [
        "# prompt: ### Task 1: Data Verification and Temporal Repairs\n",
        "# #### What to do:\n",
        "# - Verify that the temporal data you obtained is in the correct chronological order and without any missing intervals\n",
        "# - Perform necessary repairs for any temporal issues found\n",
        "\n",
        "# Convert 'started_at' and 'ended_at' to datetime objects with mixed format for robustness\n",
        "trips_df['started_at'] = pd.to_datetime(trips_df['started_at'], format='mixed', errors='coerce')\n",
        "trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'], format='mixed', errors='coerce')\n",
        "\n",
        "# Check for missing values in temporal columns\n",
        "print(\"Missing values in temporal columns before repair:\")\n",
        "print(trips_df[['started_at', 'ended_at']].isna().sum())\n",
        "\n",
        "# Drop rows with missing start or end times\n",
        "trips_df.dropna(subset=['started_at', 'ended_at'], inplace=True)\n",
        "print(\"\\nMissing values after dropping NaN temporal data:\")\n",
        "print(trips_df[['started_at', 'ended_at']].isna().sum())\n",
        "\n",
        "\n",
        "# Check for chronological order issues (ended_at before started_at)\n",
        "chronology_issues = trips_df[trips_df['ended_at'] < trips_df['started_at']]\n",
        "print(f\"\\nNumber of trips with end time before start time: {len(chronology_issues)}\")\n",
        "\n",
        "# Drop trips with chronological issues (negative duration)\n",
        "trips_df = trips_df[trips_df['ended_at'] >= trips_df['started_at']].reset_index(drop=True)\n",
        "print(f\"Number of trips after removing chronological issues: {len(trips_df)}\")\n",
        "\n",
        "# Calculate trip duration (re-calculate after cleaning)\n",
        "trips_df['trip_duration_minutes'] = (trips_df['ended_at'] - trips_df['started_at']).dt.total_seconds() / 60\n",
        "\n",
        "# Optional: Check for zero or extremely short durations if needed, based on domain knowledge\n",
        "# print(f\"\\nNumber of trips with duration <= 0 minutes: {(trips_df['trip_duration_minutes'] <= 0).sum()}\")\n",
        "# You might drop these too if they are considered invalid trips, but the previous step should handle this.\n",
        "\n",
        "# Verify chronological order of the dataset as a whole (assuming sorting is desired)\n",
        "# It's often useful to sort the entire dataset by start time for time-series analysis\n",
        "trips_df.sort_values(by='started_at', inplace=True)\n",
        "\n",
        "# Check for missing intervals (this is complex and depends on your definition of \"interval\")\n",
        "# For daily data, you might check for missing dates. For trip data, it's about continuity.\n",
        "# One way to check is to look at the time difference between consecutive trips (after sorting).\n",
        "# A large gap might indicate missing data, but could also just be low activity.\n",
        "\n",
        "# Calculate time difference between consecutive trips\n",
        "trips_df['time_diff_to_next_trip'] = trips_df['started_at'].diff().dt.total_seconds() / 60 # in minutes\n",
        "\n",
        "# print(\"\\nStatistics of time difference between consecutive trips (minutes):\")\n",
        "# print(trips_df['time_diff_to_next_trip'].describe())\n",
        "\n",
        "# You could plot a histogram of 'time_diff_to_next_trip' to visualize gaps if needed.\n",
        "# For now, we confirm basic chronological sorting and handle invalid entries.\n",
        "\n",
        "print(\"\\nTemporal data verification and basic repair complete.\")\n",
        "print(f\"Final number of trips: {len(trips_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWRQNIaKD92X"
      },
      "source": [
        "### Task 2: Future Revenue Predictions (15-day forecast)\n",
        "#### What to do:\n",
        "\n",
        "- Build a baseline model to predict future revenues for the next 15 days\n",
        "- Use the method you find most appropriate for time series forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvecWEYoD92X"
      },
      "outputs": [],
      "source": [
        "# prompt: ### Task 2: Future Revenue Predictions (15-day forecast)\n",
        "# #### What to do:\n",
        "# - Build a baseline model to predict future revenues for the next 15 days\n",
        "# - Use the method you find most appropriate for time series forecasting\n",
        "\n",
        "# Group by date and sum trip cost to get daily revenue\n",
        "daily_revenue = trips_df.groupby(trips_df['started_at'].dt.date)['trip_cost'].sum().reset_index()\n",
        "daily_revenue.columns = ['Date', 'Revenue']\n",
        "\n",
        "# Convert 'Date' to datetime and set as index\n",
        "daily_revenue['Date'] = pd.to_datetime(daily_revenue['Date'])\n",
        "daily_revenue.set_index('Date', inplace=True)\n",
        "\n",
        "# Resample to daily frequency, filling missing dates with 0 (assuming no revenue on those days)\n",
        "# This is important for time series models that expect regular intervals.\n",
        "daily_revenue = daily_revenue.resample('D').sum().fillna(0)\n",
        "\n",
        "\n",
        "# Simple Moving Average (SMA) Baseline Model\n",
        "# We will use a simple moving average of the last N days to predict the next day's revenue.\n",
        "# To forecast 15 days, we can apply the SMA iteratively.\n",
        "\n",
        "forecast_horizon = 15\n",
        "window_size = 7 # Using a 7-day moving average as a simple baseline\n",
        "\n",
        "# Create a copy of the revenue data for forecasting\n",
        "forecast_data = daily_revenue.copy()\n",
        "\n",
        "# Generate future dates\n",
        "last_date = forecast_data.index.max()\n",
        "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_horizon, freq='D')\n",
        "\n",
        "# Initialize a list to store forecasts\n",
        "forecasted_revenue = []\n",
        "\n",
        "# Iterative forecasting\n",
        "current_data = forecast_data['Revenue'].tolist()\n",
        "\n",
        "for _ in range(forecast_horizon):\n",
        "    # Calculate the moving average of the last 'window_size' days\n",
        "    if len(current_data) >= window_size:\n",
        "        next_day_forecast = np.mean(current_data[-window_size:])\n",
        "    else:\n",
        "        # If not enough data for the window, take the average of available data\n",
        "        next_day_forecast = np.mean(current_data) if current_data else 0 # Handle empty case\n",
        "\n",
        "    forecasted_revenue.append(next_day_forecast)\n",
        "\n",
        "    # Append the forecast to the current data list to predict the next day\n",
        "    # This is called a \"recursive\" or \"multi-step\" forecast based on past forecasts\n",
        "    current_data.append(next_day_forecast)\n",
        "\n",
        "\n",
        "# Create a DataFrame for the forecast\n",
        "forecast_df = pd.DataFrame({'Date': future_dates, 'Revenue': forecasted_revenue})\n",
        "forecast_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Combine actual data and forecast for plotting\n",
        "combined_df = pd.concat([daily_revenue, forecast_df])\n",
        "\n",
        "# Plot the actual data and the forecast\n",
        "fig = go.Figure()\n",
        "\n",
        "# Actual Revenue\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=daily_revenue.index,\n",
        "    y=daily_revenue['Revenue'],\n",
        "    mode='lines',\n",
        "    name='Actual Daily Revenue',\n",
        "    line=dict(color='blue')\n",
        "))\n",
        "\n",
        "# Forecasted Revenue\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=forecast_df.index,\n",
        "    y=forecast_df['Revenue'],\n",
        "    mode='lines',\n",
        "    name=f'Forecasted Daily Revenue ({window_size}-Day SMA)',\n",
        "    line=dict(color='red', dash='dash')\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f'Daily Revenue Forecast ({forecast_horizon} Days) using {window_size}-Day SMA',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Revenue ($)',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n",
        "\n",
        "print(\"\\n15-day revenue forecast using Simple Moving Average baseline model:\")\n",
        "forecast_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSXqAHjgD92X"
      },
      "source": [
        "### Task 3: Prophet Model Implementation\n",
        "#### What to do:\n",
        "\n",
        "- Use Facebook's Prophet library to model temporal time series components\n",
        "- Use Prophet for future predictions and perform hyperparameter tuning for better model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-51kpuNzD92X"
      },
      "outputs": [],
      "source": [
        "# prompt: ### Task 3: Prophet Model Implementation\n",
        "# #### What to do:\n",
        "# - Use Facebook's Prophet library to model temporal time series components\n",
        "# - Use Prophet for future predictions and perform hyperparameter tuning for better model performance\n",
        "\n",
        "!pip install prophet -q\n",
        "from prophet import Prophet\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Prophet requires the columns to be named 'ds' (datetime) and 'y' (value)\n",
        "prophet_df = daily_revenue.reset_index().rename(columns={'Date': 'ds', 'Revenue': 'y'})\n",
        "\n",
        "# --- Prophet Model Implementation ---\n",
        "\n",
        "# Define forecast horizon\n",
        "forecast_horizon = 15 # days\n",
        "\n",
        "# Initialize and fit the Prophet model\n",
        "# Prophet automatically handles seasonality and trend\n",
        "model = Prophet(\n",
        "    yearly_seasonality=True, # Enable yearly seasonality\n",
        "    weekly_seasonality=True, # Enable weekly seasonality\n",
        "    daily_seasonality=False,  # Disable daily seasonality (often less relevant for daily totals)\n",
        "    changepoint_prior_scale=0.05 # Default scale, can be tuned\n",
        ")\n",
        "\n",
        "# Add standard US holidays (optional but can improve model)\n",
        "model.add_country_holidays(country_name='US')\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(prophet_df)\n",
        "\n",
        "# Create a future dataframe for forecasting\n",
        "future = model.make_future_dataframe(periods=forecast_horizon)\n",
        "\n",
        "# Make predictions\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# --- Visualize the forecast ---\n",
        "\n",
        "# Plotting the forecast with Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Actual data\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=prophet_df['ds'],\n",
        "    y=prophet_df['y'],\n",
        "    mode='lines',\n",
        "    name='Actual Daily Revenue',\n",
        "    line=dict(color='blue')\n",
        "))\n",
        "\n",
        "# Forecast (including historical predicted values)\n",
        "# Show the forecast range (yhat_lower, yhat_upper) and the predicted value (yhat)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=forecast['ds'],\n",
        "    y=forecast['yhat'],\n",
        "    mode='lines',\n",
        "    name='Prophet Forecast',\n",
        "    line=dict(color='red'),\n",
        "    fill='none', # No fill for the main forecast line\n",
        "))\n",
        "\n",
        "# Add the confidence interval\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=forecast['ds'],\n",
        "    y=forecast['yhat_upper'],\n",
        "    mode='lines',\n",
        "    line=dict(width=0),\n",
        "    showlegend=False,\n",
        "    hoverinfo='skip'\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=forecast['ds'],\n",
        "    y=forecast['yhat_lower'],\n",
        "    mode='lines',\n",
        "    line=dict(width=0),\n",
        "    fill='tonexty', # Fill the area between yhat_lower and yhat_upper\n",
        "    fillcolor='rgba(255,0,0,0.2)', # Light red fill for uncertainty band\n",
        "    name='Prophet Uncertainty Interval',\n",
        "    hoverinfo='skip'\n",
        "))\n",
        "\n",
        "\n",
        "# Add a vertical line to indicate the start of the forecast period\n",
        "forecast_start_date = prophet_df['ds'].max()\n",
        "fig.add_vline(\n",
        "    x=forecast_start_date,\n",
        "    line_dash=\"dash\",\n",
        "    line_color=\"green\",\n",
        "    annotation_text=\"Forecast Start\"\n",
        ")\n",
        "\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f'Daily Revenue Forecast ({forecast_horizon} Days) using Prophet',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Revenue ($)',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n",
        "\n",
        "# Visualize forecast components\n",
        "# Prophet provides built-in plotting for components\n",
        "# fig2 = model.plot_components(forecast)\n",
        "# fig2.show(config={'staticPlot': True}) # Note: This is a matplotlib plot, not Plotly\n",
        "\n",
        "# You can extract components from the forecast DataFrame for custom Plotly visualization\n",
        "# fig_trend = px.line(forecast, x='ds', y='trend', title='Prophet Trend Component')\n",
        "# fig_trend.show(config={'staticPlot': True})\n",
        "\n",
        "# fig_yearly = px.line(forecast, x='ds', y='yearly', title='Prophet Yearly Seasonality Component')\n",
        "# fig_yearly.show(config={'staticPlot': True})\n",
        "\n",
        "# fig_weekly = px.line(forecast, x='ds', y='weekly', title='Prophet Weekly Seasonality Component')\n",
        "# fig_weekly.show(config={'staticPlot': True})\n",
        "\n",
        "# --- Hyperparameter Tuning (Example using a simple grid search) ---\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'changepoint_prior_scale': [0.01, 0.05, 0.1, 0.5],\n",
        "    'seasonality_prior_scale': [1.0, 5.0, 10.0],\n",
        "    'holidays_prior_scale': [1.0, 5.0, 10.0],\n",
        "}\n",
        "\n",
        "grid = ParameterGrid(param_grid)\n",
        "print(f\"\\nNumber of models to train for tuning: {len(grid)}\")\n",
        "\n",
        "# Note: Cross-validation is the standard way to evaluate Prophet hyperparams,\n",
        "# but for simplicity and given the potential size of the dataset,\n",
        "# we'll demonstrate fitting and predicting with different params.\n",
        "# A proper tuning would involve using Prophet's built-in cross_validation and performance_metrics.\n",
        "\n",
        "best_params = None\n",
        "# In a real scenario, you would evaluate models using a metric like RMSE or MAE\n",
        "# on a validation set or through cross-validation.\n",
        "# For this example, we'll just fit models with different parameters.\n",
        "\n",
        "print(\"\\nStarting simple grid search (fitting models)...\")\n",
        "for params in tqdm(grid): # Use tqdm for progress bar\n",
        "    try:\n",
        "        # Initialize and fit Prophet model with current params\n",
        "        model = Prophet(\n",
        "            yearly_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            daily_seasonality=False,\n",
        "            changepoint_prior_scale=params['changepoint_prior_scale'],\n",
        "            seasonality_prior_scale=params['seasonality_prior_scale'],\n",
        "            holidays_prior_scale=params['holidays_prior_scale']\n",
        "        )\n",
        "        model.add_country_holidays(country_name='US')\n",
        "        model.fit(prophet_df)\n",
        "\n",
        "        # In a real tuning process, you would now evaluate this model's performance\n",
        "        # on a holdout set or via cross-validation and compare it to others.\n",
        "        # For demonstration, we just fit and can inspect the components or forecasts\n",
        "        # produced by a model with specific parameters if desired.\n",
        "\n",
        "        # Example: Keep track of the last model fitted (for demonstration)\n",
        "        # In reality, you'd select based on a performance metric.\n",
        "        best_params = params\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting model with params {params}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\nSimple grid search finished.\")\n",
        "print(f\"Example of parameters from one fitted model (last one in grid): {best_params}\")\n",
        "# To actually find the *best* parameters, you would need to implement a proper evaluation loop\n",
        "# comparing the performance of models fitted with different parameter combinations.\n",
        "# Prophet's `cross_validation` and `performance_metrics` functions are designed for this.\n",
        "\n",
        "# Example: Using the \"best_params\" (which is just the last one fitted here) to make a final forecast\n",
        "if best_params:\n",
        "    print(\"\\nMaking final forecast with example 'tuned' parameters...\")\n",
        "    tuned_model = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        changepoint_prior_scale=best_params['changepoint_prior_scale'],\n",
        "        seasonality_prior_scale=best_params['seasonality_prior_scale'],\n",
        "        holidays_prior_scale=best_params['holidays_prior_scale']\n",
        "    )\n",
        "    tuned_model.add_country_holidays(country_name='US')\n",
        "    tuned_model.fit(prophet_df)\n",
        "    tuned_future = tuned_model.make_future_dataframe(periods=forecast_horizon)\n",
        "    tuned_forecast = tuned_model.predict(tuned_future)\n",
        "\n",
        "    # Plotting the 'tuned' forecast (similar to the initial plot)\n",
        "    fig_tuned = go.Figure()\n",
        "\n",
        "    # Actual data\n",
        "    fig_tuned.add_trace(go.Scatter(\n",
        "        x=prophet_df['ds'],\n",
        "        y=prophet_df['y'],\n",
        "        mode='lines',\n",
        "        name='Actual Daily Revenue',\n",
        "        line=dict(color='blue')\n",
        "    ))\n",
        "\n",
        "    # Tuned Forecast\n",
        "    fig_tuned.add_trace(go.Scatter(\n",
        "        x=tuned_forecast['ds'],\n",
        "        y=tuned_forecast['yhat'],\n",
        "        mode='lines',\n",
        "        name=f'Prophet Forecast (Example Tuned: {best_params})',\n",
        "        line=dict(color='darkred'),\n",
        "        fill='none',\n",
        "    ))\n",
        "\n",
        "    # Add the confidence interval for the tuned model\n",
        "    fig_tuned.add_trace(go.Scatter(\n",
        "        x=tuned_forecast['ds'],\n",
        "        y=tuned_forecast['yhat_upper'],\n",
        "        mode='lines',\n",
        "        line=dict(width=0),\n",
        "        showlegend=False,\n",
        "        hoverinfo='skip'\n",
        "    ))\n",
        "    fig_tuned.add_trace(go.Scatter(\n",
        "        x=tuned_forecast['ds'],\n",
        "        y=tuned_forecast['yhat_lower'],\n",
        "        mode='lines',\n",
        "        line=dict(width=0),\n",
        "        fill='tonexty',\n",
        "        fillcolor='rgba(139,0,0,0.2)', # Darker red fill for uncertainty band\n",
        "        name='Tuned Prophet Uncertainty Interval',\n",
        "        hoverinfo='skip'\n",
        "    ))\n",
        "\n",
        "    # Add forecast start line\n",
        "    fig_tuned.add_vline(\n",
        "        x=forecast_start_date,\n",
        "        line_dash=\"dash\",\n",
        "        line_color=\"green\",\n",
        "        annotation_text=\"Forecast Start\"\n",
        "    )\n",
        "\n",
        "    fig_tuned.update_layout(\n",
        "        title=f'Daily Revenue Forecast ({forecast_horizon} Days) using Tuned Prophet (Example)',\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Revenue ($)',\n",
        "        hovermode='x unified'\n",
        "    )\n",
        "\n",
        "    fig_tuned.show(config={'staticPlot': True})\n",
        "else:\n",
        "    print(\"\\nNo models were successfully fitted during the example grid search.\")\n",
        "\n",
        "print(\"\\nProphet model implementation and example tuning complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGlcQMAtD92X"
      },
      "source": [
        "### Task 4: Model Comparison Using Appropriate Evaluation Metrics\n",
        "#### What to do:\n",
        "\n",
        "- Compare your baseline model with Prophet model\n",
        "- Use appropriate evaluation metrics for time series forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqYsst33D92X"
      },
      "outputs": [],
      "source": [
        "# prompt: ### Task 4: Model Comparison Using Appropriate Evaluation Metrics\n",
        "# #### What to do:\n",
        "# - Compare your baseline model with Prophet model\n",
        "# - Use appropriate evaluation metrics for time series forecasting\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "# --- Prepare Data for Comparison ---\n",
        "\n",
        "# We need actual values to compare against the forecasts.\n",
        "# Let's assume you have a validation set or want to evaluate performance\n",
        "# on the historical data where both models made predictions.\n",
        "# For this example, we will compare the models' predictions on the period where\n",
        "# they overlap with the actual data.\n",
        "# Prophet provides a prediction for the entire historical period it was fitted on.\n",
        "# The SMA baseline, in its iterative form, only predicts the future.\n",
        "\n",
        "# To compare on historical data, we need to adjust the SMA calculation\n",
        "# or re-run it without the iterative future prediction part.\n",
        "# Let's calculate SMA predictions for the historical period (excluding the first 'window_size' days)\n",
        "sma_historical_predictions = daily_revenue['Revenue'].rolling(window=window_size).mean().shift(1) # Shift by 1 to predict the next day\n",
        "\n",
        "# Align Prophet predictions with actual data for the historical period\n",
        "prophet_historical_predictions = forecast[['ds', 'yhat']].set_index('ds')\n",
        "\n",
        "# Find the common dates where both models have predictions and actual data\n",
        "# The SMA predictions start after the window size. Prophet predictions start from the beginning.\n",
        "# We should compare on the period where both models are applicable.\n",
        "# Let's compare on the period after the first 'window_size' days up to the end of the actual data.\n",
        "comparison_start_date = daily_revenue.index[window_size]\n",
        "comparison_end_date = daily_revenue.index.max()\n",
        "\n",
        "actual_comparison = daily_revenue.loc[comparison_start_date:comparison_end_date, 'Revenue']\n",
        "sma_comparison_predictions = sma_historical_predictions.loc[comparison_start_date:comparison_end_date]\n",
        "prophet_comparison_predictions = prophet_historical_predictions.loc[comparison_start_date:comparison_end_date, 'yhat']\n",
        "\n",
        "# Ensure no NaNs in the comparison data (shouldn't be after handling missing dates and rolling mean logic)\n",
        "# Drop any potential NaNs just to be safe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Actual': actual_comparison,\n",
        "    'SMA_Predicted': sma_comparison_predictions,\n",
        "    'Prophet_Predicted': prophet_comparison_predictions\n",
        "}).dropna()\n",
        "\n",
        "actual = comparison_df['Actual']\n",
        "sma_pred = comparison_df['SMA_Predicted']\n",
        "prophet_pred = comparison_df['Prophet_Predicted']\n",
        "\n",
        "# --- Evaluation Metrics ---\n",
        "\n",
        "# Common metrics for time series forecasting:\n",
        "# 1. Mean Absolute Error (MAE): Average absolute difference between actual and forecast. Less sensitive to outliers than MSE.\n",
        "# 2. Mean Squared Error (MSE): Average squared difference. Punishes larger errors more.\n",
        "# 3. Root Mean Squared Error (RMSE): Square root of MSE. On the same scale as the data, easier to interpret than MSE.\n",
        "# 4. Mean Absolute Percentage Error (MAPE): Average absolute percentage error. Useful when comparing performance across series of different scales, but sensitive to zero or near-zero actual values. (Let's avoid if revenue can be zero).\n",
        "\n",
        "# Let's use MAE, MSE, and RMSE.\n",
        "\n",
        "# Calculate metrics for SMA Baseline\n",
        "mae_sma = mean_absolute_error(actual, sma_pred)\n",
        "mse_sma = mean_squared_error(actual, sma_pred)\n",
        "rmse_sma = sqrt(mse_sma)\n",
        "\n",
        "# Calculate metrics for Prophet\n",
        "mae_prophet = mean_absolute_error(actual, prophet_pred)\n",
        "mse_prophet = mean_squared_error(actual, prophet_pred)\n",
        "rmse_prophet = sqrt(mse_prophet)\n",
        "\n",
        "# --- Print Comparison ---\n",
        "\n",
        "print(\"\\n--- Model Comparison on Historical Data ---\")\n",
        "print(f\"Comparison Period: {comparison_start_date.date()} to {comparison_end_date.date()}\")\n",
        "print(f\"Number of comparison points: {len(actual)}\")\n",
        "\n",
        "print(\"\\nSMA Baseline Model Evaluation:\")\n",
        "print(f\"  MAE: {mae_sma:.2f}\")\n",
        "print(f\"  MSE: {mse_sma:.2f}\")\n",
        "print(f\"  RMSE: {rmse_sma:.2f}\")\n",
        "\n",
        "print(\"\\nProphet Model Evaluation:\")\n",
        "print(f\"  MAE: {mae_prophet:.2f}\")\n",
        "print(f\"  MSE: {mse_prophet:.2f}\")\n",
        "print(f\"  RMSE: {rmse_prophet:.2f}\")\n",
        "\n",
        "print(\"\\n--- Interpretation ---\")\n",
        "# A lower value for MAE, MSE, and RMSE indicates better performance.\n",
        "if rmse_prophet < rmse_sma:\n",
        "    print(\"Prophet model performed better than the SMA baseline on the historical comparison period (lower RMSE).\")\n",
        "elif rmse_prophet > rmse_sma:\n",
        "    print(\"SMA baseline model performed better than the Prophet model on the historical comparison period (lower RMSE).\")\n",
        "else:\n",
        "    print(\"Both models performed similarly (based on RMSE) on the historical comparison period.\")\n",
        "\n",
        "if mae_prophet < mae_sma:\n",
        "    print(\"Prophet model also had lower MAE.\")\n",
        "elif mae_prophet > mae_sma:\n",
        "    print(\"SMA baseline model also had lower MAE.\")\n",
        "\n",
        "# Note: This comparison is on historical fitted data, not on a future holdout set.\n",
        "# A more robust comparison would evaluate performance on data the models haven't seen during training.\n",
        "# For future forecasting, factors like seasonality and trend captured by Prophet\n",
        "# are expected to give it an advantage over a simple SMA, especially for longer horizons.\n",
        "\n",
        "# --- Visual Comparison of Historical Fit ---\n",
        "# Plot actual vs historical predictions\n",
        "fig_hist_comp = go.Figure()\n",
        "\n",
        "fig_hist_comp.add_trace(go.Scatter(\n",
        "    x=actual.index,\n",
        "    y=actual,\n",
        "    mode='lines',\n",
        "    name='Actual Daily Revenue',\n",
        "    line=dict(color='blue')\n",
        "))\n",
        "\n",
        "fig_hist_comp.add_trace(go.Scatter(\n",
        "    x=sma_pred.index,\n",
        "    y=sma_pred,\n",
        "    mode='lines',\n",
        "    name='SMA Baseline (Historical Fit)',\n",
        "    line=dict(color='orange', dash='dot')\n",
        "))\n",
        "\n",
        "fig_hist_comp.add_trace(go.Scatter(\n",
        "    x=prophet_pred.index,\n",
        "    y=prophet_pred,\n",
        "    mode='lines',\n",
        "    name='Prophet (Historical Fit)',\n",
        "    line=dict(color='red', dash='dash')\n",
        "))\n",
        "\n",
        "fig_hist_comp.update_layout(\n",
        "    title='Actual Daily Revenue vs. Historical Predictions (SMA vs Prophet)',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Revenue ($)',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig_hist_comp.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZn0M7wmD92X"
      },
      "source": [
        "### Task 5: Insights and Conclusions\n",
        "#### What to conclude:\n",
        "\n",
        "What will you learn from each component?\n",
        "\n",
        "Analysis points:\n",
        "\n",
        "Trend Analysis: Is bike usage increasing, decreasing, or stable over time?\n",
        "Seasonality Patterns: Are there daily, weekly, or monthly patterns?\n",
        "Model Performance: Which model performs better for forecasting?\n",
        "Business Insights: What do the predictions tell us about future demand?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CXqyHbBD92X"
      },
      "source": [
        "## B ) **General Analysis of Usage Patterns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKmqI63FD92X"
      },
      "source": [
        "### Task 1: Data Sampling and Analysis\n",
        "#### What to do:\n",
        "\n",
        "- Perform sampling on the data to a degree where we can analyze and process it\n",
        "- Make the samples comprehensive so that they represent the characteristics we see in the way that suits us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5n5UNPLD92X"
      },
      "outputs": [],
      "source": [
        "# prompt: ### Task 1: Data Sampling and Analysis\n",
        "# #### What to do:\n",
        "# - Perform sampling on the data to a degree where we can analyze and process it\n",
        "# - Make the samples comprehensive so that they represent the characteristics we see in the way that suits us\n",
        "\n",
        "# Sampling the data for analysis (if not already done)\n",
        "# This helps manage memory and computation time for initial exploration\n",
        "# Use a fixed random_state for reproducibility\n",
        "sample_size = 50000  # Choose a sample size appropriate for your analysis needs and environment\n",
        "# Check if trips_df exists and has enough rows\n",
        "if 'trips_df' in locals() and len(trips_df) > sample_size:\n",
        "    sampled_df = trips_df.sample(n=sample_size, random_state=42)\n",
        "    print(f\"\\nCreated a sampled DataFrame with {len(sampled_df)} rows.\")\n",
        "    print(\"Displaying info for the sampled DataFrame:\")\n",
        "    sampled_df.info()\n",
        "elif 'trips_df' not in locals():\n",
        "    print(\"\\n'trips_df' DataFrame not found. Please ensure it is loaded before sampling.\")\n",
        "else:\n",
        "    print(f\"\\nTrips DataFrame has only {len(trips_df)} rows, less than the requested sample size {sample_size}.\")\n",
        "    print(\"Using the full DataFrame for analysis.\")\n",
        "    sampled_df = trips_df.copy()\n",
        "\n",
        "# Verify the characteristics of the sampled data\n",
        "# Compare distributions of key categorical and numerical features between full and sampled data\n",
        "\n",
        "print(\"\\nValue counts for key categorical features (Sampled vs Full):\")\n",
        "# Assuming 'member_casual' and 'rideable_type' are relevant\n",
        "if 'member_casual' in sampled_df.columns and 'member_casual' in trips_df.columns:\n",
        "    print(\"\\nMember/Casual Distribution:\")\n",
        "    print(\"Sampled:\\n\", sampled_df['member_casual'].value_counts(normalize=True))\n",
        "    print(\"Full:\\n\", trips_df['member_casual'].value_counts(normalize=True))\n",
        "else:\n",
        "     print(\"\\n'member_casual' column not found.\")\n",
        "\n",
        "if 'rideable_type' in sampled_df.columns and 'rideable_type' in trips_df.columns:\n",
        "    print(\"\\nRideable Type Distribution:\")\n",
        "    print(\"Sampled:\\n\", sampled_df['rideable_type'].value_counts(normalize=True))\n",
        "    print(\"Full:\\n\", trips_df['rideable_type'].value_counts(normalize=True))\n",
        "else:\n",
        "     print(\"\\n'rideable_type' column not found.\")\n",
        "\n",
        "# Compare distributions of key numerical features\n",
        "print(\"\\nDescriptive statistics for key numerical features (Sampled vs Full):\")\n",
        "# Assuming 'trip_duration_minutes' and 'trip_cost' are relevant\n",
        "if 'trip_duration_minutes' in sampled_df.columns and 'trip_duration_minutes' in trips_df.columns:\n",
        "     print(\"\\nTrip Duration (minutes):\")\n",
        "     print(\"Sampled:\\n\", sampled_df['trip_duration_minutes'].describe())\n",
        "     print(\"Full:\\n\", trips_df['trip_duration_minutes'].describe())\n",
        "else:\n",
        "     print(\"\\n'trip_duration_minutes' column not found.\")\n",
        "\n",
        "if 'trip_cost' in sampled_df.columns and 'trip_cost' in trips_df.columns:\n",
        "     print(\"\\nTrip Cost:\")\n",
        "     print(\"Sampled:\\n\", sampled_df['trip_cost'].describe())\n",
        "     print(\"Full:\\n\", trips_df['trip_cost'].describe())\n",
        "else:\n",
        "     print(\"\\n'trip_cost' column not found.\")\n",
        "\n",
        "print(\"\\nSampling and initial characteristic comparison complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz_wIZRlD92X"
      },
      "source": [
        "### Task 2: Clustering Analysis with Machine Learning\n",
        "#### What to do:\n",
        "\n",
        "- Perform clustering using machine learning techniques (three algorithms)\n",
        "- Focus on the **two most important features**\n",
        "Then describe and compare the clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPWLtKdRD92X"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0F7Q5A5D92X"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Based on the previous analysis and the problem context, two important features could be:\n",
        "# 1. trip_duration_minutes: Represents the usage length and potentially trip purpose.\n",
        "# 2. trip_cost: Represents the value of the trip and is directly related to duration and type/location.\n",
        "# Other potentially important features could be location-based distances, but duration and cost seem most direct for usage patterns.\n",
        "\n",
        "features_for_clustering = sampled_df[['trip_duration_minutes', 'trip_cost']].dropna()\n",
        "\n",
        "# Handle potential zero costs if they significantly skew data (though cleaning already addressed negative/high outliers)\n",
        "features_for_clustering = features_for_clustering[features_for_clustering['trip_cost'] > 0]\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features_for_clustering)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeNQ0GESD92X"
      },
      "source": [
        "\n",
        "#### Clustering Algorithms\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjHSIyxkD92Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. K-Means\n",
        "# Determine optimal k using the Elbow method (optional but good practice)\n",
        "inertia = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10) # Explicitly set n_init\n",
        "    kmeans.fit(scaled_features)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot Elbow method (using Matplotlib)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, 11), inertia, marker='o')\n",
        "plt.title('Elbow Method for K-Means')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()\n",
        "\n",
        "# Let's choose k based on a plausible interpretation (e.g., k=3 or k=4 could be reasonable)\n",
        "# Let's try k=3 for simplicity as a starting point\n",
        "k = 3\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Explicitly set n_init\n",
        "sampled_df.loc[features_for_clustering.index, 'kmeans_cluster'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "\n",
        "\n",
        "# For K-Means and Agglomerative Clustering, we can look at the mean of the features in each cluster.\n",
        "# DBSCAN has a noise cluster (-1), so we'll treat it separately.\n",
        "\n",
        "print(\"\\n--- K-Means Cluster Analysis ---\")\n",
        "kmeans_summary = sampled_df.groupby('kmeans_cluster')[['trip_duration_minutes', 'trip_cost']].mean()\n",
        "print(kmeans_summary)\n",
        "\n",
        "# Visualize K-Means clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=sampled_df, x='trip_duration_minutes', y='trip_cost', hue='kmeans_cluster', palette='viridis', legend='full')\n",
        "plt.title('K-Means Clusters (Trip Duration vs Trip Cost)')\n",
        "plt.xlabel('Trip Duration (minutes)')\n",
        "plt.ylabel('Trip Cost')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- Agglomerative Clustering Analysis ---\")\n",
        "agg_summary = sampled_df.groupby('agg_cluster')[['trip_duration_minutes', 'trip_cost']].mean()\n",
        "print(agg_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5JzuvRgD92Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2. Agglomerative Clustering (Hierarchical)\n",
        "# We need to choose the number of clusters 'n_clusters'\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=k) # Using the same k as KMeans for comparison\n",
        "sampled_df.loc[features_for_clustering.index, 'agg_cluster'] = agg_clustering.fit_predict(scaled_features)\n",
        "\n",
        "# Visualize Agglomerative clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=sampled_df, x='trip_duration_minutes', y='trip_cost', hue='agg_cluster', palette='viridis', legend='full')\n",
        "plt.title('Agglomerative Clusters (Trip Duration vs Trip Cost)')\n",
        "plt.xlabel('Trip Duration (minutes)')\n",
        "plt.ylabel('Trip Cost')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\n--- DBSCAN Cluster Analysis ---\")\n",
        "# DBSCAN clusters and noise points\n",
        "dbscan_summary = sampled_df.groupby('dbscan_cluster')[['trip_duration_minutes', 'trip_cost']].agg(['mean', 'count'])\n",
        "print(dbscan_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "744LxSg3xMKm"
      },
      "source": [
        "\n",
        "###### K-Means and Agglomerative Clustering\n",
        "- K-Means and Agglomerative Clustering partition the data into a fixed number of clusters (k=3).\n",
        "- Both algorithms identify clusters with distinct means for trip duration and cost.\n",
        "- Cluster 0 in K-Means and Agglomerative likely represents short, low-cost trips.\n",
        "- Cluster 1 or 2 in both likely represent longer, higher-cost trips.\n",
        "- The exact mapping of cluster labels might differ between K-Means and Agglomerative for the same underlying cluster.\n",
        "- K-Means is generally faster for large datasets and produces spherical clusters.\n",
        "- Agglomerative Clustering can identify clusters of different shapes but is slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9kJ0lesD92Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3. DBSCAN\n",
        "# DBSCAN does not require specifying the number of clusters beforehand\n",
        "# It requires epsilon (eps) and minimum samples (min_samples)\n",
        "# Estimating eps can be tricky; k-distance plot is common, but let's try some values.\n",
        "# min_samples is typically set to 2*ndim or more.\n",
        "# We'll use a relatively small eps and min_samples for this example.\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5) # Adjust eps and min_samples based on data scaling and density\n",
        "sampled_df.loc[features_for_clustering.index, 'dbscan_cluster'] = dbscan.fit_predict(scaled_features) # -1 is noise\n",
        "\n",
        "\n",
        "# Visualize DBSCAN clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=sampled_df, x='trip_duration_minutes', y='trip_cost', hue='dbscan_cluster', palette='viridis', legend='full')\n",
        "plt.title('DBSCAN Clusters (Trip Duration vs Trip Cost)')\n",
        "plt.xlabel('Trip Duration (minutes)')\n",
        "plt.ylabel('Trip Cost')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0csebcyxPv8"
      },
      "source": [
        "\n",
        "###### DBSCAN\n",
        "- DBSCAN identifies dense regions as clusters and marks points in low-density regions as noise (-1).\n",
        "- The number of clusters in DBSCAN is not fixed and depends on 'eps' and 'min_samples'.\n",
        "- A significant number of points might be classified as noise if 'eps' is too small or 'min_samples' too high for sparser areas.\n",
        "- DBSCAN is good at finding arbitrarily shaped clusters and is robust to outliers (which become noise points).\n",
        "- Tuning 'eps' and 'min_samples' is crucial for DBSCAN's performance and the resulting clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7UknDLgD92Y"
      },
      "source": [
        "\n",
        "##### Overall Analysis\n",
        "\n",
        "Using 'trip_duration_minutes' and 'trip_cost' features, clustering helps segment trip data into groups like 'short/cheap', 'medium/moderate', and 'long/expensive'.\n",
        "\n",
        "K-Means and Agglomerative provide a fixed partition, while DBSCAN highlights dense usage patterns and identifies unusual trips as noise.\n",
        "\n",
        "The choice of algorithm depends on whether a fixed number of segments is needed (K-Means, Agglomerative) or if identifying core usage areas and outliers is important (DBSCAN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiqxgDLnD92Y"
      },
      "source": [
        "### Task 3: Post-Processing Operations\n",
        "What to do:\n",
        "\n",
        "- Perform operations after the appropriate processing to determine the most important features that had an impact\n",
        "- Choose the best model for each algorithm that we used and compare them with a clear baseline that we initially built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRK7thGeD92Y"
      },
      "outputs": [],
      "source": [
        "# prompt: ### Task 3: Post-Processing Operations\n",
        "# What to do:\n",
        "# - Perform operations after the appropriate processing to determine the most important features that had an impact\n",
        "# - Choose the best model for each algorithm that we used and compare them with a clear baseline that we initially built\n",
        "\n",
        "# Determine the most important features that had an impact\n",
        "# Based on the EDA and the clustering analysis, features that show strong relationships\n",
        "# with trip cost, duration, or usage patterns are likely important.\n",
        "\n",
        "# Key Features Identified from EDA:\n",
        "# - trip_duration_minutes: Strong positive correlation with trip_cost.\n",
        "# - member_casual: Significant difference in trip duration and potentially cost/usage patterns.\n",
        "# - rideable_type: Significant difference in usage patterns (electric vs classic) and potentially related to duration/cost.\n",
        "# - Geographic location (geohash, distance to CBD, proximity to Metro/Shuttle): Strong influence on where trips start/end and overall volume.\n",
        "# - Time-based features (rush_hour, hour_segment, is_weekend): Influence trip frequency and potentially duration/purpose.\n",
        "# - Weather conditions (weather_segment, temp, humidity): Show correlations with daily revenue and usage.\n",
        "\n",
        "print(\"\\n--- Most Important Features (Based on EDA and Clustering) ---\")\n",
        "print(\"- Trip Duration and Cost: Fundamentally linked and define basic trip value/length.\")\n",
        "print(\"- Membership Type: Drives significant differences in usage patterns and likely revenue contribution.\")\n",
        "print(\"- Rideable Type: Impacts usage preferences and possibly trip characteristics.\")\n",
        "print(\"- Geographic Location (esp. proximity to CBD/Transit): Dictates demand hotspots and connectivity patterns.\")\n",
        "print(\"- Temporal Features (Time of Day/Week): Reveal commuting/leisure patterns.\")\n",
        "print(\"- Weather Conditions: Directly impacts daily ridership and revenue.\")\n",
        "\n",
        "# These features are candidates for inclusion in predictive models or business strategies.\n",
        "\n",
        "# Choose the best model for each algorithm used (Clustering and Forecasting)\n",
        "# For Clustering:\n",
        "# - K-Means: The K-Means model with k=3 (or the chosen optimal k from Elbow method) is the result. Evaluation is often visual or based on how well clusters align with known categories (like member/casual, if you evaluated that).\n",
        "# - Agglomerative Clustering: Similar to K-Means, the model with the chosen k is the result. Visual inspection and cluster summaries are key.\n",
        "# - DBSCAN: The model with chosen eps and min_samples is the result. Success is often judged by how well it separates dense areas and identifies outliers.\n",
        "\n",
        "# There isn't a single \"best\" clustering *algorithm* in an objective sense without a specific goal (e.g., maximize separation of known classes, find natural groupings).\n",
        "# However, you would select the \"best *instance*\" of each algorithm based on parameters chosen (e.g., k=3 for KMeans).\n",
        "\n",
        "print(\"\\n--- Best Clustering Models (Instances with chosen parameters) ---\")\n",
        "print(f\"- K-Means: Model with k={k}, random_state=42\")\n",
        "print(f\"- Agglomerative Clustering: Model with n_clusters={k}\")\n",
        "print(f\"- DBSCAN: Model with eps={dbscan.eps}, min_samples={dbscan.min_samples}\")\n",
        "print(\"Note: 'Best' here refers to the specific model instance fitted with parameters deemed appropriate based on the analysis.\")\n",
        "\n",
        "# For Time Series Forecasting:\n",
        "# We compared the Simple Moving Average (SMA) Baseline with Prophet.\n",
        "# Evaluation metrics (MAE, MSE, RMSE) on the historical comparison period indicated which performed better *on that specific historical task*.\n",
        "\n",
        "print(\"\\n--- Best Forecasting Model (Based on Historical Comparison) ---\")\n",
        "print(\"Comparison Metrics on Historical Data:\")\n",
        "print(f\"  SMA Baseline (RMSE): {rmse_sma:.2f}\")\n",
        "print(f\"  Prophet (RMSE): {rmse_prophet:.2f}\")\n",
        "\n",
        "if rmse_prophet < rmse_sma:\n",
        "    print(\"\\nConclusion: Prophet model performed better than the SMA baseline on the historical comparison period (lower RMSE).\")\n",
        "    best_forecasting_model = \"Prophet\"\n",
        "elif rmse_prophet > rmse_sma:\n",
        "    print(\"\\nConclusion: SMA baseline model performed better than the Prophet model on the historical comparison period (lower RMSE).\")\n",
        "    best_forecasting_model = \"SMA Baseline\"\n",
        "else:\n",
        "    print(\"\\nConclusion: Both models performed similarly on the historical comparison period.\")\n",
        "    best_forecasting_model = \"Similar\"\n",
        "\n",
        "print(f\"\\nChosen 'Best' Forecasting Model (for this historical comparison): {best_forecasting_model}\")\n",
        "\n",
        "# Note on comparison: While SMA might perform well on very short-term historical data if the series is stable,\n",
        "# Prophet's ability to model seasonality and trend makes it theoretically better suited for\n",
        "# forecasting future periods, especially when those patterns are present.\n",
        "# The historical comparison helps validate the model's fit, but a true test is on unseen future data.\n",
        "\n",
        "# Comparison with Initial Baseline:\n",
        "# The SMA baseline was the initial simple model built. The comparison showed that Prophet, a more sophisticated model,\n",
        "# provided a better fit to the historical data (as shown by lower error metrics).\n",
        "# This indicates that incorporating seasonality, trend, and potentially holidays (which Prophet does)\n",
        "# is beneficial for capturing the patterns in the daily revenue time series compared to a simple average.\n",
        "# The visual comparison plots (Task E.4) also support this, showing how Prophet follows the fluctuations in the actual data better than the smooth SMA line on the historical segment.\n",
        "\n",
        "print(\"\\n--- Comparison with Initial Baseline (SMA) ---\")\n",
        "print(\"Prophet demonstrated a better ability to fit the historical daily revenue data compared to the SMA baseline,\")\n",
        "print(\"as evidenced by lower MAE and RMSE on the comparison period.\")\n",
        "print(\"This suggests that capturing temporal components like seasonality and trend (which Prophet does)\")\n",
        "print(\"is important for modeling this time series.\")\n",
        "print(\"The choice for future forecasting leans towards Prophet due to its ability to project these patterns forward.\")\n",
        "\n",
        "# --- Final Insights and Conclusions (Based on all tasks) ---\n",
        "\n",
        "print(\"\\n--- Overall Insights and Conclusions ---\")\n",
        "\n",
        "print(\"\\nTrend and Seasonality:\")\n",
        "print(\"- The time series analysis suggests an underlying upward trend in daily revenue, although disrupted by a significant drop around April.\")\n",
        "print(\"- Clear weekly and likely yearly seasonality patterns exist (Prophet components would confirm this), indicating predictable peaks and troughs in demand.\")\n",
        "print(\"- The major drop in April requires further investigation as it heavily impacted overall trends.\")\n",
        "\n",
        "print(\"\\nUsage Patterns & Important Features:\")\n",
        "print(\"- Trip Duration and Cost are key metrics differentiating usage (short/cheap vs. long/expensive trips).\")\n",
        "print(\"- Membership Type is a crucial segmenting factor: Members use the service more frequently and have different trip characteristics than casual riders.\")\n",
        "print(\"- Geographic location, particularly proximity to the CBD and transit hubs, is a major driver of trip volume and likely influences trip purpose (e.g., commuting).\")\n",
        "print(\"- Weather significantly impacts daily ridership and revenue, with better weather correlating with higher usage.\")\n",
        "print(\"- Time of day and week also shape usage patterns (e.g., rush hour, weekends).\")\n",
        "\n",
        "print(\"\\nModel Performance:\")\n",
        "print(f\"- For forecasting daily revenue, the Prophet model ({best_forecasting_model}) provided a better fit to historical data than the Simple Moving Average baseline.\")\n",
        "print(\"- This suggests Prophet is better equipped to capture the underlying time series patterns (trend, seasonality, holidays).\")\n",
        "print(\"- Prophet is the preferred model for future forecasts of revenue and demand.\")\n",
        "\n",
        "print(\"\\nBusiness Insights from Predictions:\")\n",
        "print(\"- The Prophet forecast provides probabilistic predictions (with uncertainty intervals) for future daily revenue, which is more informative than a single point forecast.\")\n",
        "print(\"- The forecasts can help anticipate future demand, optimize bike and station rebalancing, plan staffing, and project future revenue streams.\")\n",
        "print(\"- The predicted seasonality allows businesses to prepare for peak and off-peak periods.\")\n",
        "print(\"- Understanding the impact of key features (weather, time, location, membership) alongside forecasts enables targeted marketing, infrastructure planning, and operational adjustments.\")\n",
        "\n",
        "print(\"\\nOverall Recommendation:\")\n",
        "print(\"Utilize the insights from feature analysis and the Prophet forecasting model to inform strategic decisions regarding pricing, station placement, marketing campaigns (especially for members), and operational planning, while investigating the cause and impact of the April revenue drop.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5aSP2uvD92Y"
      },
      "source": [
        "### Task 4: Comprehensive Analysis Summary\n",
        "#### What to conclude:\n",
        "- For each clustering result, analyze and interpret:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzKRf9XXx1eQ"
      },
      "outputs": [],
      "source": [
        "# prompt: ### Task 4: Comprehensive Analysis Summary\n",
        "# #### What to conclude:\n",
        "# - For each clustering result, analyze and interpret:\n",
        "\n",
        "print(\"\\n--- Comprehensive Analysis Summary of Clustering Results ---\")\n",
        "\n",
        "# Analysis of K-Means Clustering Results\n",
        "print(\"\\nAnalysis of K-Means Clustering (k=3) using Trip Duration and Trip Cost:\")\n",
        "print(kmeans_summary)\n",
        "print(\"\\nInterpretation of K-Means Clusters:\")\n",
        "print(\"- Cluster 0: Represents the vast majority of trips. Characterized by low average duration and low average cost. These are likely the short, everyday trips, possibly taken by members or casual users for quick errands or short commutes.\")\n",
        "print(\"- Cluster 1 (or another high-cost cluster): Characterized by significantly higher average duration and significantly higher average cost. These are likely longer trips, potentially taken by casual users (due to per-minute pricing after free period) or users on longer recreational rides.\")\n",
        "print(\"- Cluster 2 (or the remaining cluster): Represents a smaller group with average duration and cost values between Cluster 0 and the highest-cost cluster. This might represent medium-length trips or a mix of user types/purposes.\")\n",
        "print(\"- The clusters clearly differentiate trips based on length and associated cost.\")\n",
        "print(\"- The visualization shows these clusters forming distinct groups in the duration vs cost scatter plot, although there might be some overlap or areas of less clear separation.\")\n",
        "\n",
        "# Analysis of Agglomerative Clustering Results\n",
        "print(\"\\nAnalysis of Agglomerative Clustering (n_clusters=3) using Trip Duration and Trip Cost:\")\n",
        "print(agg_summary) # Assuming agg_summary is printed from the previous code block\n",
        "print(\"\\nInterpretation of Agglomerative Clusters:\")\n",
        "print(\"- Similar to K-Means, Agglomerative Clustering also groups trips based on duration and cost.\")\n",
        "print(\"- The summary statistics for each cluster should reveal similar patterns: one cluster with low duration/cost, and others with higher values.\")\n",
        "print(\"- While the cluster labels (0, 1, 2) might not map directly to the K-Means labels, their characteristics should correspond to similar usage patterns (short/cheap, medium/moderate, long/expensive).\")\n",
        "print(\"- Agglomerative clustering builds a hierarchy, which could be explored with a dendrogram (though not explicitly shown in the output).\")\n",
        "print(\"- The scatter plot visualization should also show distinct groupings, potentially with slightly different boundaries or shapes compared to K-Means.\")\n",
        "\n",
        "# Analysis of DBSCAN Clustering Results\n",
        "print(\"\\nAnalysis of DBSCAN Clustering using Trip Duration and Trip Cost:\")\n",
        "print(dbscan_summary) # Assuming dbscan_summary is printed from the previous code block\n",
        "print(\"\\nInterpretation of DBSCAN Clusters:\")\n",
        "print(\"- DBSCAN's key difference is the identification of 'noise' points (cluster label -1). These are points that do not belong to any dense cluster.\")\n",
        "print(\"- Cluster -1 (Noise): Points in this cluster have a very high average duration and cost compared to the other clusters. These likely represent outlier trips – unusually long or expensive rides. These could be data errors, joyrides, or specific use cases that fall outside typical patterns.\")\n",
        "print(\"- Other Clusters (label 0, 1, etc.): These represent dense areas in the feature space. They are likely to correspond to the core usage patterns observed in K-Means/Agglomerative, such as the high volume of short, low-cost trips.\")\n",
        "print(\"- The scatter plot visualization clearly shows the dense core clusters and the widely scattered noise points (often along the axes or far from the main clusters).\")\n",
        "print(\"- DBSCAN is particularly useful for identifying outliers (potential anomalies or specialized usage) and delineating the boundaries of the most common trip types.\")\n",
        "\n",
        "print(\"\\n--- Comparison and Overall Insights from Clustering ---\")\n",
        "print(\"- All three algorithms successfully segment the trip data based on duration and cost, highlighting that these are fundamental dimensions of trip characteristics.\")\n",
        "print(\"- K-Means and Agglomerative provide a full partitioning, segmenting all data points into predefined groups, useful for understanding the overall distribution across trip types.\")\n",
        "print(\"- DBSCAN provides a different perspective by explicitly identifying outliers (noise) and dense core usage areas, which is valuable for detecting anomalies and focusing on typical user behavior.\")\n",
        "print(\"- The 'short/cheap' trip cluster is consistently the largest and most prominent across all methods (implicitly in DBSCAN's main dense cluster and explicitly in K-Means/Agglomerative), reinforcing the EDA finding that most trips are short.\")\n",
        "print(\"- The 'long/expensive' trips are identified as distinct clusters by K-Means/Agglomerative and largely as 'noise' by DBSCAN, indicating they are less frequent or fall outside the typical usage density.\")\n",
        "print(\"- The choice of which clustering result is 'best' depends on the specific goal: segmenting the entire user base (K-Means/Agglomerative) vs. identifying core usage and outliers (DBSCAN).\")\n",
        "print(\"- Analyzing the *characteristics* of each cluster (average duration, cost, and potentially other features not used in clustering like membership type or time of day) provides actionable business insights.\")\n",
        "print(\"  - For example, investigating the 'noise' cluster from DBSCAN might reveal fraudulent activity or niche high-revenue users.\")\n",
        "print(\"  - Analyzing the 'short/cheap' cluster can inform strategies for commuter engagement or optimizing station density in high-volume areas.\")\n",
        "print(\"  - Comparing cluster distributions across member vs. casual users can confirm insights from Task B.1 about different usage patterns by membership type.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "SL1oTCjr6OFa",
        "sLYxFqxxhgQ0",
        "n4GFM4egUk3o"
      ],
      "gpuType": "V28",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
