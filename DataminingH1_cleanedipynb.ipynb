{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF3CEiZb4GtS"
      },
      "source": [
        "# **Loading libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xs-YTsgO8QA"
      },
      "outputs": [],
      "source": [
        "%pip install gdown\n",
        "%pip install tqdm scikit-learn\n",
        "%pip install geopandas\n",
        "%pip install geohash2\n",
        "%pip install folium\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import gdown\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "from google.colab import drive\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "from tqdm import tqdm\n",
        "import geohash2\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I05pgLlnO8QA"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTfRsiq6CQa"
      },
      "source": [
        "# **Loading the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9lConBSO8QB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "downloading the dataset\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkCADdlJzQGc"
      },
      "outputs": [],
      "source": [
        "folder_id = '1O3w5OKnS__hzlL8kTSfGCUc_iX8XNjEN'\n",
        "output_dir = 'Homework'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "print(f\"Attempting to download content from folder ID: {folder_id} into {output_dir}\")\n",
        "try:\n",
        "    gdown.download_folder(id=folder_id, output=output_dir, quiet=False, use_cookies=False)\n",
        "    print(f\"\\nSuccessfully downloaded content to: /content/{output_dir}\")\n",
        "    print(\"You can now find the downloaded content in the 'downloaded_external_folder' directory in your Colab files browser.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during download: {e}\")\n",
        "    print(\"Please ensure the Google Drive folder is publicly accessible or shared with 'Anyone with the link can view'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBxHrxew0knQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load tabular data\n",
        "weather_df = pd.read_csv(\"Homework/data/Washington,DC,USA 2024-01-01 to 2024-12-31.csv\")\n",
        "trips_df = pd.read_parquet('Homework/data/daily-rent.parquet')\n",
        "\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "\n",
        "stations_df = pd.read_csv(\"Homework/data/Capital_Bikeshare_Locations.csv\")\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Downloading the combined and modified dataset (for ease of use )\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Nxjq-QmE5b9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data straight\n",
        "import gdown as gdown\n",
        "# file_id = \"1eOCLRqXFnzvIz4I3S2uk0STHCk_Eg3pP\"\n",
        "file_id =\"114g7JYuZ00i864przAIJQYymib_5h6Qa\"\n",
        "output_file = \"trips_df.csv\"\n",
        "gdown.download(id=file_id, output=output_file, quiet=False)\n",
        "num_rows_to_read = 1_000_000\n",
        "\n",
        "print(f\"File downloaded to {output_file}\")\n",
        "trips_df = pd.read_csv(output_file,nrows=num_rows_to_read)\n",
        "trips_df.head()"
      ],
      "metadata": {
        "id": "nI0R9q-L5l8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df.columns"
      ],
      "metadata": {
        "id": "7Q2rQL-n7LvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL1oTCjr6OFa"
      },
      "source": [
        "# **Cleaning & inspecting the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gt_DuEv4w9P"
      },
      "source": [
        "\n",
        "There is a problem with missing start/id , almost 20% of the data are null so we must find a way to fill these up\n",
        "\n",
        "**Try1 : spatial join**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "using lang and lati we can match it to the nearest station and then assign this id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h20Hhlhg1IFk"
      },
      "outputs": [],
      "source": [
        "trips_df = trips_df.dropna(subset=['end_lat', 'end_lng'])\n",
        "\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw3RvCyR1T9h"
      },
      "outputs": [],
      "source": [
        "# EPSG:4326 = lat/lon\n",
        "trips_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "stations_gdf = gpd.GeoDataFrame(\n",
        "    stations_df,\n",
        "    geometry=gpd.points_from_xy(stations_df['LONGITUDE'], stations_df['LATITUDE']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "stations_gdf.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA47hcEu1ldC"
      },
      "outputs": [],
      "source": [
        "# Find nearest station to each ride\n",
        "trips_with_nearest_station = gpd.sjoin_nearest(\n",
        "    trips_gdf, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"distance\"\n",
        ")\n",
        "\n",
        "# Now we fill missing station_id with nearest one\n",
        "trips_df['start_station_id'] = trips_df['start_station_id'].fillna(\n",
        "    trips_with_nearest_station['STATION_ID']\n",
        ")\n",
        "# Create a mapping from STATION_ID to STATION_NAME\n",
        "id_to_name = stations_df.set_index('STATION_ID')['NAME'].to_dict()\n",
        "\n",
        "# Fill in missing start_station_name using start_station_id\n",
        "trips_df['start_station_name'] = trips_df['start_station_name'].fillna(\n",
        "    trips_df['start_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8isYZzA4yfR"
      },
      "source": [
        "Repeating the process to end id and name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIcXJiy95mU9"
      },
      "outputs": [],
      "source": [
        "trips_gdf_end = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "trips_with_nearest_end_station = gpd.sjoin_nearest(\n",
        "    trips_gdf_end, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"end_distance\"\n",
        ")\n",
        "\n",
        "trips_df['end_station_id'] = trips_df['end_station_id'].fillna(\n",
        "    trips_with_nearest_end_station['STATION_ID']\n",
        ")\n",
        "trips_df['end_station_name'] = trips_df['end_station_name'].fillna(\n",
        "    trips_df['end_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df=trips_df.drop_duplicates()\n",
        "trips_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will continue inspecting the rest of the data"
      ],
      "metadata": {
        "id": "zkda9AV_6sc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273sHOD85t0E"
      },
      "outputs": [],
      "source": [
        "stations_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo2_a0EG6qbm"
      },
      "outputs": [],
      "source": [
        "stations_df=stations_df.drop_duplicates()\n",
        "stations_df.isna().sum()  # we dont need to drop null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4BwUW136uQH"
      },
      "outputs": [],
      "source": [
        "weather_df=weather_df.drop_duplicates()\n",
        "weather_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQGps8Rt62eU"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrMSX_Av6vdY"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhqxpEU267b0"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf = parking_zones_gdf.drop(columns=['CREATOR', 'CREATED','EDITOR','EDITED'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhlna1xj9EeU"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_WPYmtO8iLt"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FHwlBbZ9ez-"
      },
      "source": [
        "# **PreProcessing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhYVVGgM9mwY"
      },
      "outputs": [],
      "source": [
        "weather_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHgrh-3o-F1Y"
      },
      "outputs": [],
      "source": [
        "# first we make sure all the dates are in the same format (by checking the length)\n",
        "datetime_lengths = weather_df[\"datetime\"].astype(str).apply(len)\n",
        "print(datetime_lengths.value_counts())\n",
        "weather_df[\"date\"] = pd.to_datetime(weather_df[\"datetime\"])\n",
        "print(weather_df[\"date\"].dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU_XzGfm-3st"
      },
      "outputs": [],
      "source": [
        "trips_df[\"start_time\"] = pd.to_datetime(trips_df[\"started_at\"],format='mixed')\n",
        "trips_df[\"end_time\"] = pd.to_datetime(trips_df[\"ended_at\"],format='mixed')\n",
        "# ensuring that CRS is EPSG:4326\n",
        "if parking_zones_gdf.crs != \"EPSG:4326\":\n",
        "    parking_zones_gdf = parking_zones_gdf.to_crs(\"EPSG:4326\")\n",
        "# Spatial Join to Map Stations to Parking Zones\n",
        "# Spatial join: add zone info to each station\n",
        "stations_with_zone = gpd.sjoin(\n",
        "    stations_gdf,\n",
        "    parking_zones_gdf[[\"NAME\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"\n",
        ")\n",
        "# Rename column for clarity\n",
        "stations_with_zone = stations_with_zone.rename(columns={\"zone_name\": \"residential_zone\"})\n",
        "# Joining Weather Data\n",
        "# Extract date from start_time for weather join\n",
        "trips_df[\"date\"] = trips_df[\"start_time\"].dt.date\n",
        "weather_df[\"date\"] = weather_df[\"date\"].dt.date\n",
        "\n",
        "# Join weather by date\n",
        "trips_df = trips_df.merge(weather_df, on=\"date\", how=\"left\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQWxs96q_uH-"
      },
      "outputs": [],
      "source": [
        "trips_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MHwqZPyBL_t"
      },
      "outputs": [],
      "source": [
        "trips_df[['start_station_id', 'end_station_id', 'start_station_name', 'end_station_name']].isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnhDD_fQBZvY"
      },
      "outputs": [],
      "source": [
        "trips_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRUlFc1tO8QN"
      },
      "source": [
        "\n",
        "---\n",
        "B1\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48-Qa2HGpClZ"
      },
      "outputs": [],
      "source": [
        "# # B1\n",
        "\n",
        "# # From started_at\n",
        "# trips_df['start_year'] = trips_df['started_at'].dt.year\n",
        "# trips_df['start_month'] = trips_df['started_at'].dt.month\n",
        "# trips_df['start_day_num'] = trips_df['started_at'].dt.day\n",
        "# trips_df['start_day_name'] = trips_df['started_at'].dt.day_name()\n",
        "\n",
        "# # From ended_at\n",
        "# trips_df['end_year'] = trips_df['ended_at'].dt.year\n",
        "# trips_df['end_month'] = trips_df['ended_at'].dt.month\n",
        "# trips_df['end_day_num'] = trips_df['ended_at'].dt.day\n",
        "# trips_df['end_day_name'] = trips_df['ended_at'].dt.day_name()\n",
        "# trips_df.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6HWUwZfBbaR"
      },
      "source": [
        "\n",
        "---\n",
        "B2\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df['trip_duration_minutes'] = (trips_df['end_time'] - trips_df['start_time']).dt.total_seconds() / 60\n",
        "trips_df['trip_duration_minutes']=trips_df['trip_duration_minutes'].round(2)\n",
        "trips_df['trip_duration_minutes'].head(5)"
      ],
      "metadata": {
        "id": "z7eipZArwwpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The trip_duration_minutes problem**"
      ],
      "metadata": {
        "id": "SN-VHZhttU30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df['trip_duration_minutes'].describe()"
      ],
      "metadata": {
        "id": "DZ0TSaictSZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*we can clearly see that there is a problem with the tripd_durations, the min is a negative value and that is not right*"
      ],
      "metadata": {
        "id": "dugc51rytbJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show trips with negative or 0 duration\n",
        "invalid_durations = trips_df[trips_df['trip_duration_minutes'] <= 0]\n",
        "print(f\"Invalid rows: {len(invalid_durations)}\")\n",
        "invalid_durations[['ride_id', 'started_at', 'ended_at', 'trip_duration_minutes']].head()\n"
      ],
      "metadata": {
        "id": "ltwKRGertgvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only valid trips\n",
        "trips_df = trips_df[trips_df['trip_duration_minutes'] > 0]\n",
        "trips_df['trip_duration_minutes'].describe()\n"
      ],
      "metadata": {
        "id": "W-wTEW_ZtjcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MC8yVG5pmu6"
      },
      "source": [
        "---\n",
        "B3\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df['member_casual'].value_counts()"
      ],
      "metadata": {
        "id": "5iMUivT3_god"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy1RkWaJodJF"
      },
      "outputs": [],
      "source": [
        "trips_df['rideable_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Q2d88_r7xx"
      },
      "outputs": [],
      "source": [
        "# Initialize base cost\n",
        "# Start with 0 cost\n",
        "trips_df['trip_cost'] = 0.0\n",
        "\n",
        "# Define fixed costs\n",
        "trips_df.loc[trips_df['member_casual'] == 'member', 'trip_cost'] = 3.95\n",
        "trips_df.loc[trips_df['member_casual'] == 'casual', 'trip_cost'] = 1.00\n",
        "\n",
        "# Add extra cost for duration\n",
        "# for members :\n",
        "# Create condition for member rides longer than 45 mins\n",
        "cond_member_extra = (trips_df['member_casual'] == 'member') & (trips_df['trip_duration_minutes'] > 45)\n",
        "\n",
        "# Electric bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'electric_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.10\n",
        "\n",
        "# Classic bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'classic_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.05\n",
        "# Electric bike for casuals\n",
        "cond_casual_electric = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'electric_bike')\n",
        "trips_df.loc[cond_casual_electric, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.15\n",
        "\n",
        "# Classic bike for casuals\n",
        "cond_casual_classic = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'classic_bike')\n",
        "trips_df.loc[cond_casual_classic, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.05\n",
        "# Add Central Business District (CBD) fee\n",
        "# Preparaing your geometry points\n",
        "# Create GeoDataFrame of start points\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "# #  Load CBD Polygon\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=4326)  # Ensures it's in WGS 84\n",
        "\n",
        "\n",
        "# Convert to GeoDataFrames with correct CRS\n",
        "start_gdf = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "end_gdf = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "\n",
        "# Load CBD polygon and project to EPSG:6933\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.unary_union  # Get full boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "306f9C9Nuh0x"
      },
      "outputs": [],
      "source": [
        "# # Spatial containment check\n",
        "# # Get the actual polygon geometry from CBD GeoDataFrame\n",
        "# cbd_polygon = CBD.geometry.unary_union  # safe in case of multipolygon\n",
        "\n",
        "# # Check for each row\n",
        "# trips_df['start_in_cbd'] = trips_df['start_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "# trips_df['end_in_cbd'] = trips_df['end_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "\n",
        "# # Final condition: start or end inside CBD\n",
        "# trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "# Check spatial containment in EPSG:6933\n",
        "trips_df['start_in_cbd'] = start_gdf['start_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "trips_df['end_in_cbd'] = end_gdf['end_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "\n",
        "# Final condition and cost update\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df.loc[trips_df['in_cbd'], 'trip_cost'] += 0.5\n",
        "trips_df['trip_cost'].head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYgY5Pp2jR5W"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_cost'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck8sSCqNzuXo"
      },
      "source": [
        "*we can see a clear issue in the data ,  and super high values (4.3 mil in the max ) and std is very high (4837.62) , so we must identify this outliers and deal with them*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-WAd13voMS"
      },
      "outputs": [],
      "source": [
        "# High-cost trips\n",
        "high_cost = trips_df[trips_df['trip_cost'] > 1000].copy()\n",
        "print(high_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n",
        "\n",
        "# Negative-cost trips\n",
        "neg_cost = trips_df[trips_df['trip_cost'] < 0].copy()\n",
        "print(neg_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRq1RuQcwwZT"
      },
      "outputs": [],
      "source": [
        "# Total rows\n",
        "total_rows = len(trips_df)\n",
        "\n",
        "# Define thresholds\n",
        "high_cost_threshold = 10000\n",
        "negative_cost_threshold = 0\n",
        "\n",
        "# Find outliers\n",
        "high_cost_outliers = trips_df[trips_df['trip_cost'] > high_cost_threshold]\n",
        "negative_cost_outliers = trips_df[trips_df['trip_cost'] < negative_cost_threshold]\n",
        "\n",
        "# Count\n",
        "num_high_cost = len(high_cost_outliers)\n",
        "num_negative_cost = len(negative_cost_outliers)\n",
        "total_outliers = num_high_cost + num_negative_cost\n",
        "\n",
        "# Percentages\n",
        "percent_high_cost = (num_high_cost / total_rows) * 100\n",
        "percent_negative_cost = (num_negative_cost / total_rows) * 100\n",
        "percent_total_outliers = (total_outliers / total_rows) * 100\n",
        "\n",
        "print(f\"High cost outliers: {num_high_cost} ({percent_high_cost:.2f}%)\")\n",
        "print(f\"Negative cost outliers: {num_negative_cost} ({percent_negative_cost:.2f}%)\")\n",
        "print(f\"Total outliers: {total_outliers} ({percent_total_outliers:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHli55wtyMCY"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_cost'].info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzmRagLeyj4C"
      },
      "outputs": [],
      "source": [
        "# Drop outliers by reassigning the filtered DataFrame back to df\n",
        "trips_df = trips_df[(trips_df['trip_cost'] <= high_cost_threshold) & (trips_df['trip_cost'] >= negative_cost_threshold)]\n",
        "trips_df['trip_cost'].info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUSkTeWHyuOQ"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_cost'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-inlm6X1UtR"
      },
      "source": [
        "---\n",
        "B4\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFjJEIy_04-8"
      },
      "outputs": [],
      "source": [
        "stations_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb4fPoBD1PaZ"
      },
      "outputs": [],
      "source": [
        "stations_df['CAPACITY'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THTyaahb2BmM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Basic histogram using Plotly\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Distribution of Station Capacity')\n",
        "fig.update_layout(xaxis_title='Capacity', yaxis_title='Count', bargap=0.1)\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBzKGS5A4g2K"
      },
      "outputs": [],
      "source": [
        "# Drop NaNs\n",
        "capacity_data = stations_df['CAPACITY'].dropna()\n",
        "# Histogram\n",
        "hist_data = go.Histogram(x=capacity_data, nbinsx=30, name='Histogram', opacity=0.6)\n",
        "# Density Curve\n",
        "kde = gaussian_kde(capacity_data)\n",
        "x_vals = np.linspace(capacity_data.min(), capacity_data.max(), 1000)\n",
        "kde_data = go.Scatter(x=x_vals, y=kde(x_vals) * len(capacity_data) * (x_vals[1] - x_vals[0]),\n",
        "                      mode='lines', name='KDE Curve')\n",
        "\n",
        "# Plot both\n",
        "fig = go.Figure(data=[hist_data, kde_data])\n",
        "fig.update_layout(title='Capacity Distribution with KDE',\n",
        "                  xaxis_title='Capacity', yaxis_title='Count')\n",
        "# Example thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.30)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "print(low_thresh,high_thresh)\n",
        "fig.add_vline(x=low_thresh, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=high_thresh, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULXZ605f4sqp"
      },
      "outputs": [],
      "source": [
        "# Calculate the thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.33)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "\n",
        "def classify_capacity(cap):\n",
        "    if cap <= low_thresh:\n",
        "        return 'Small'\n",
        "    elif cap <= high_thresh:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "stations_df['STATION_SIZE'].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LssiAiXc6shB"
      },
      "outputs": [],
      "source": [
        "def classify_capacity(cap):\n",
        "    if cap <= 15:\n",
        "        return 'Small'\n",
        "    elif cap <= 25:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "print(stations_df['STATION_SIZE'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5_IiBmU7Npw"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Station Capacity Distribution')\n",
        "fig.add_vline(x=15, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=25, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMBlw701c9vN"
      },
      "source": [
        "---\n",
        "B5\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fylKxgKMecgw"
      },
      "outputs": [],
      "source": [
        "Shuttle_Bus_Stops=pd.read_csv(\"Homework/data/Shuttle_Bus_Stops.csv\")\n",
        "Metro_Bus_Stops =pd.read_csv(\"Homework/data/Metro_Bus_Stops.csv\")\n",
        "Shuttle_Bus_Stops.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIUA0TNYe2RO"
      },
      "outputs": [],
      "source": [
        "Metro_Bus_Stops['BSTP_LAT'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_buZ1x_CAVFQ"
      },
      "source": [
        "\n",
        "Approaches\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Approach                    | Time Complexity | Vectorized | Fast    |\n",
        "| --------------------------- | --------------- | ---------- | ------- |\n",
        "| Brute Force (Your original) | O(N × M)        | ❌ No       | 🐌 Slow |\n",
        "| BallTree (New)              | O(N log M)      | ✅ Yes      | ⚡ Fast  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_i2yvv2O8QX"
      },
      "source": [
        "Project all your coordinates to EPSG:6933\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEGPg81slsNc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create start and end point geometries\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "\n",
        "# Create GeoDataFrames\n",
        "gdf_start = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "gdf_end = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "\n",
        "# Add x/y columns\n",
        "trips_df['start_x'] = gdf_start.geometry.x\n",
        "trips_df['start_y'] = gdf_start.geometry.y\n",
        "trips_df['end_x'] = gdf_end.geometry.x\n",
        "trips_df['end_y'] = gdf_end.geometry.y\n",
        "\n",
        "\n",
        "# projecting   metro and shuttle station coordinates:\n",
        "\n",
        "# Convert station lat/lng to projected coordinates\n",
        "def project_coords(coords_list):\n",
        "    gdf = gpd.GeoDataFrame(geometry=[Point(lon, lat) for lat, lon in coords_list], crs='EPSG:4326')\n",
        "    gdf = gdf.to_crs(epsg=6933)\n",
        "    return np.array([(geom.x, geom.y) for geom in gdf.geometry])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfj75bOenaPz"
      },
      "outputs": [],
      "source": [
        "# coords\n",
        "# Metro stop coordinates\n",
        "metro_coords = Metro_Bus_Stops[['BSTP_LAT', 'BSTP_LON']].dropna().values\n",
        "\n",
        "# Shuttle stop coordinates\n",
        "shuttle_coords = Shuttle_Bus_Stops[['LATITUDE', 'LONGITUDE']].dropna().values\n",
        "\n",
        "metro_coords_projected = project_coords(metro_coords)\n",
        "shuttle_coords_projected = project_coords(shuttle_coords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN48kQYPl5A0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def euclidean_tree_batch(source_df, stop_coords, x_col, y_col, batch_size=10000):\n",
        "    tree = BallTree(stop_coords, metric='euclidean')\n",
        "\n",
        "    distances = []\n",
        "    n = len(source_df)\n",
        "    tqdm.pandas(desc=f\"Computing distances for {x_col}\")\n",
        "\n",
        "    for i in tqdm(range(0, n, batch_size), desc=\"Batch processing\", unit=\"batch\"):\n",
        "        batch = source_df.iloc[i:i+batch_size]\n",
        "        batch_points = batch[[x_col, y_col]].values\n",
        "\n",
        "        dists, _ = tree.query(batch_points, k=1)\n",
        "        distances.extend(dists.flatten().tolist())\n",
        "\n",
        "    return distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvjyWh64l7Qd"
      },
      "outputs": [],
      "source": [
        "# Start → Metro\n",
        "trips_df['start_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End → Metro\n",
        "trips_df['end_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'end_x', 'end_y'\n",
        ")\n",
        "\n",
        "# Start → Shuttle\n",
        "trips_df['start_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End → Shuttle\n",
        "trips_df['end_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'end_x', 'end_y'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4bgJlGMAlPE"
      },
      "outputs": [],
      "source": [
        "trips_df['start_nearest_metro_distance'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU88I8yln-bs"
      },
      "outputs": [],
      "source": [
        "trips_df['end_nearest_metro_distance'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZJ6s55lFYa_"
      },
      "outputs": [],
      "source": [
        "trips_df['start_nearest_shuttle_distance'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF7cr7yTFYSV"
      },
      "outputs": [],
      "source": [
        "trips_df['end_nearest_shuttle_distance'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8yEENQZHLaS"
      },
      "source": [
        "we will drop outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbLirzQrOwwC"
      },
      "outputs": [],
      "source": [
        "start_nearest_metro_distance_thr=1550\n",
        "end_nearest_metro_distance_thr=1600\n",
        "start_nearest_shuttle_distance_thr=23000\n",
        "end_nearest_shuttle_distance_thr=23200\n",
        "outliers=[]\n",
        "outliers.append(trips_df[trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr])\n",
        "outliers.append(trips_df[trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr])\n",
        "outliers.append(trips_df[trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr])\n",
        "outliers.append(trips_df[trips_df['end_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr])\n",
        "for i in outliers :\n",
        "  print(\"Outliers:\", len(i))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJME-JoxPGdW"
      },
      "outputs": [],
      "source": [
        "trips_df = trips_df[\n",
        "    (trips_df['start_nearest_metro_distance'] < start_nearest_metro_distance_thr) &\n",
        "    (trips_df['end_nearest_metro_distance'] < end_nearest_metro_distance_thr) &\n",
        "    (trips_df['start_nearest_shuttle_distance'] < start_nearest_shuttle_distance_thr) &\n",
        "    (trips_df['end_nearest_shuttle_distance'] < start_nearest_shuttle_distance_thr)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "oYitubZ64qKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3_w8Db_YRhh"
      },
      "outputs": [],
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "\n",
        "cols = ['start_nearest_metro_distance', 'end_nearest_metro_distance',\n",
        "        'start_nearest_shuttle_distance', 'end_nearest_shuttle_distance']\n",
        "\n",
        "for col in cols:\n",
        "    fig = go.Figure(\n",
        "        data=[go.Histogram(\n",
        "            x=sampled_df[col],\n",
        "            nbinsx=100,\n",
        "            marker=dict(color='skyblue'),\n",
        "            opacity=0.75\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=col,\n",
        "        xaxis_title=col,\n",
        "        yaxis_title='Count (Log Scale)',\n",
        "        yaxis_type='log',\n",
        "        bargap=0.1,\n",
        "        width=800,\n",
        "        height=400\n",
        "    )\n",
        "    fig.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smMf9ipl0yav"
      },
      "source": [
        "---\n",
        "B6\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lXEKzGY2Lhs"
      },
      "outputs": [],
      "source": [
        "print(trips_df['start_point'].iloc[0], type(trips_df['start_point'].iloc[0]))\n",
        "print(trips_df['end_point'].iloc[0], type(trips_df['end_point'].iloc[0]))\n",
        "print(type(cbd_polygon))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOAsnPfBp6Py"
      },
      "outputs": [],
      "source": [
        "# STEP 0: Make sure the CBD polygon is projected correctly\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.iloc[0]  # assuming a single polygon\n",
        "# STEP 1: Create a GeoDataFrame from the trip points (start and end)\n",
        "# start_gdf = gpd.GeoDataFrame(trips_df, geometry=trips_df['start_point'], crs=\"EPSG:4326\")\n",
        "# end_gdf   = gpd.GeoDataFrame(trips_df, geometry=trips_df['end_point'], crs=\"EPSG:4326\")\n",
        "\n",
        "# Rebuild the point geometries from lat/lng in EPSG:4326\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "\n",
        "# Project everything to EPSG:6933\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "start_gdf = start_gdf.to_crs(epsg=6933)\n",
        "end_gdf = end_gdf.to_crs(epsg=6933)\n",
        "\n",
        "# CBD polygon (in same projection)\n",
        "cbd_polygon = CBD.geometry.unary_union\n",
        "# Check containment\n",
        "trips_df['start_in_cbd'] = start_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "trips_df['end_in_cbd']   = end_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "\n",
        "# Final result\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df['in_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M_uabQt6Bza"
      },
      "source": [
        "---\n",
        "B7\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oxDKxPlsm3r"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Compute the CBD centroid (already in EPSG:6933)\n",
        "cbd_centroid = cbd_polygon.centroid  # geometry in meters (EPSG:6933)\n",
        "\n",
        "# --- Step 2: Recreate end point GeoDataFrame and project to EPSG:6933\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=6933)\n",
        "\n",
        "# --- Step 3: Compute Euclidean distance in meters\n",
        "trips_df['distance_to_cbd_m'] = end_gdf.geometry.distance(cbd_centroid)\n",
        "\n",
        "# --- Step 4: Set distance to None where start AND end are in the CBD\n",
        "mask = trips_df['start_in_cbd'] & trips_df['end_in_cbd']\n",
        "trips_df.loc[mask, 'distance_to_cbd_m'] = None\n",
        "\n",
        "# --- Step 5: Inspect result\n",
        "trips_df['distance_to_cbd_m'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szoai97X63Qj"
      },
      "source": [
        "\n",
        "\n",
        "**Threasholding strategies**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wnOOXiY7I-C"
      },
      "source": [
        "elbow method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "# Extract the data\n",
        "data = sampled_df['distance_to_cbd_m'].dropna()\n",
        "\n",
        "# Create histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=100,\n",
        "    name='Histogram',\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75\n",
        ")\n",
        "\n",
        "# Create KDE line (manual since Plotly doesn’t support KDE directly)\n",
        "kde = gaussian_kde(data)\n",
        "x_vals = np.linspace(data.min(), data.max(), 1000)\n",
        "kde_vals = kde(x_vals) * len(data) * (x_vals[1] - x_vals[0])  # scale to match histogram\n",
        "\n",
        "kde_trace = go.Scatter(\n",
        "    x=x_vals,\n",
        "    y=kde_vals,\n",
        "    mode='lines',\n",
        "    name='KDE',\n",
        "    line=dict(color='darkblue')\n",
        ")\n",
        "\n",
        "# Vertical reference lines\n",
        "vline1 = go.Scatter(\n",
        "    x=[2000, 2000],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='2km Threshold',\n",
        "    line=dict(color='red', dash='dash')\n",
        ")\n",
        "\n",
        "vline2 = go.Scatter(\n",
        "    x=[2764, 2764],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='Median',\n",
        "    line=dict(color='green', dash='dash')\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=[hist, kde_trace, vline1, vline2])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distance to CBD at End of Trip',\n",
        "    xaxis_title='distance_to_cbd_m',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    legend=dict(x=0.7, y=0.95)\n",
        ")\n",
        "\n",
        "fig.show( config={'staticPlot':True})\n"
      ],
      "metadata": {
        "id": "XPKO880k6_k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvBXxgrW7IIf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "i will choose this beacause looking at the histogram we can see the counts drops\n",
        "\"\"\"\n",
        "threshold = 2764\n",
        "# Apply binary classification\n",
        "trips_df['close_to_cbd'] = trips_df['distance_to_cbd_m'].apply(\n",
        "    lambda d: None if pd.isna(d) else d <= threshold\n",
        ")\n",
        "trips_df['close_to_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68oomfV78m6-"
      },
      "outputs": [],
      "source": [
        "print(trips_df['close_to_cbd'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ellq2QJVzAe3"
      },
      "source": [
        "---\n",
        "B8\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZVkChHv0YkX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Washington, D.C. is roughly:\n",
        "\n",
        "~16 km (north-south)\n",
        "\n",
        "~13 km (east-west)\n",
        "\n",
        "So, a geohash precision of 5–8 is appropriate.\n",
        "\"\"\"\n",
        "def encode_geohashes(df, lat_col, lon_col, precisions):\n",
        "    for p in precisions:\n",
        "        col_name = f'geohash_p{p}'\n",
        "        df[col_name] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lon_col], p), axis=1)\n",
        "    return df\n",
        "\n",
        "# Try precisions from 5 to 8\n",
        "precisions_to_test = [5, 6, 7, 8]\n",
        "trips_df = encode_geohashes(trips_df, 'start_lat', 'start_lng', precisions_to_test)\n",
        "for p in precisions_to_test:\n",
        "    print(f\"Precision {p}: {trips_df[f'geohash_p{p}'].nunique()} unique regions\")\n",
        "\"\"\"\n",
        "If the number is too small → you're over-aggregating.\n",
        "\n",
        "If it's too big (e.g. thousands) → too fine → hard to summarize meaningfully.\n",
        "\"\"\"\n",
        "\n",
        "for p in precisions_to_test:\n",
        "    counts = trips_df[f'geohash_p{p}'].value_counts()\n",
        "    print(f\"Precision {p} → median trips per geohash: {counts.median()}\")\n",
        "\"\"\"\n",
        "This tells you how balanced the spatial bins are.\n",
        "\n",
        "You ideally want 50–500 trips per cell.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShObmS0z4FJ1"
      },
      "source": [
        "| Precision | Median Trips per Geohash | Interpretation                                                     |\n",
        "| --------- | ------------------------ | ------------------------------------------------------------------ |\n",
        "| **5**     | 1761                     | ⚠️ Too coarse — merges many neighborhoods into one.                |\n",
        "| **6**     | 196                      | ✅ Good balance — each area has enough trips for reliable analysis. |\n",
        "| **7**     | 7                        | ⚠️ Very fine — may be too sparse for most practical summaries.     |\n",
        "| **8**     | 2                        | 🚫 Too sparse — most areas will be noise or empty.                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3yKdEI63oaL"
      },
      "outputs": [],
      "source": [
        "# we will choose 6\n",
        "trips_df['geohash_sector'] = trips_df['geohash_p6']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRZhifmX5KRC"
      },
      "source": [
        "---\n",
        "\n",
        "B9\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpEGkPhx4RQ5"
      },
      "outputs": [],
      "source": [
        "# Group by Sector and Date\n",
        "# Assume you have a 'date' column (convert if needed)\n",
        "trips_df['date'] = pd.to_datetime(trips_df['date'])\n",
        "\n",
        "# Count trips per day per sector\n",
        "daily_counts = trips_df.groupby(['geohash_p6', 'date']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Now compute average daily trips per geohash sector\n",
        "avg_daily_trips = daily_counts.groupby('geohash_p6')['trip_count'].mean().reset_index()\n",
        "avg_daily_trips.rename(columns={'trip_count': 'avg_daily_trips'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctAtFgF5rwM"
      },
      "source": [
        "Choose Segmentation Method (for Red / Yellow / Gray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfhDqYSj5nJs"
      },
      "source": [
        "\n",
        "| Method                         | Description                          | Pros             | Use Case             |\n",
        "| ------------------------------ | ------------------------------------ | ---------------- | -------------------- |\n",
        "| **Quantiles** (e.g., tertiles) | Divide into 3 equal-sized groups     | Simple, fair     | Balanced datasets    |\n",
        "| **Natural Breaks (Jenks)**     | Optimize separation between clusters | Data-aware       | Uneven distributions |\n",
        "| **KMeans Clustering (k=3)**    | Machine learning-based segmentation  | Optimal grouping | Large datasets       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g__UfD5W5a6n"
      },
      "outputs": [],
      "source": [
        "# quantiles :\n",
        "# Assign labels based on quantiles\n",
        "quantiles = avg_daily_trips['avg_daily_trips'].quantile([1/3, 2/3])\n",
        "low_thresh = quantiles.iloc[0]\n",
        "high_thresh = quantiles.iloc[1]\n",
        "\n",
        "def classify_volume(val):\n",
        "    if val < low_thresh:\n",
        "        return 'gray'   # Low volume\n",
        "    elif val < high_thresh:\n",
        "        return 'yellow' # Medium volume\n",
        "    else:\n",
        "        return 'red'    # High volume\n",
        "\n",
        "avg_daily_trips['volume_segment'] = avg_daily_trips['avg_daily_trips'].apply(classify_volume)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract the data\n",
        "data = avg_daily_trips['avg_daily_trips'].dropna()\n",
        "\n",
        "# Histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=30,\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75,\n",
        "    name='Avg Daily Trips'\n",
        ")\n",
        "\n",
        "# Vertical threshold lines\n",
        "vline_low = go.Scatter(\n",
        "    x=[low_thresh, low_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='Low Threshold',\n",
        "    line=dict(color='gray', dash='dash')\n",
        ")\n",
        "\n",
        "vline_high = go.Scatter(\n",
        "    x=[high_thresh, high_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='High Threshold',\n",
        "    line=dict(color='orange', dash='dash')\n",
        ")\n",
        "\n",
        "# Combine into figure\n",
        "fig = go.Figure(data=[hist, vline_low, vline_high])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distribution of Avg Daily Trips per Geohash Sector',\n",
        "    xaxis_title='Avg Daily Trips',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    bargap=0.1\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot':True})\n"
      ],
      "metadata": {
        "id": "SAYD3Ls1-Rpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYWaSyEE569w"
      },
      "outputs": [],
      "source": [
        "X = avg_daily_trips[['avg_daily_trips']].values\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "avg_daily_trips['kmeans_label'] = kmeans.labels_\n",
        "\n",
        "# Map to red/yellow/gray using sorted cluster means\n",
        "label_map = dict(zip(\n",
        "    np.argsort(kmeans.cluster_centers_.flatten()),\n",
        "    ['gray', 'yellow', 'red']\n",
        "))\n",
        "avg_daily_trips['kmeans_segment'] = avg_daily_trips['kmeans_label'].map(label_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaBBJvFnZ81X"
      },
      "outputs": [],
      "source": [
        "avg_daily_trips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2epWDpMGakGT"
      },
      "outputs": [],
      "source": [
        "trips_df['geohash_p6'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONedOBCPcTl-"
      },
      "outputs": [],
      "source": [
        "# Merge segments into trips_df\n",
        "trips_df = trips_df.merge(\n",
        "    avg_daily_trips[['geohash_p6','volume_segment','kmeans_segment']],\n",
        "    on='geohash_p6',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDOdnzYxceA1"
      },
      "outputs": [],
      "source": [
        "trips_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_gCbWosdaUt"
      },
      "outputs": [],
      "source": [
        "comparison = pd.crosstab(avg_daily_trips['volume_segment'], avg_daily_trips['kmeans_segment'])\n",
        "comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eK2NYQkgpue"
      },
      "outputs": [],
      "source": [
        "trips_df['kmeans_segment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WSRw0iZhLjR"
      },
      "outputs": [],
      "source": [
        "trips_df['volume_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1Nby-lsEf9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "B10\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKSxW3W-dWCx"
      },
      "outputs": [],
      "source": [
        "trips_df['conditions'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh1m4kfBfY8I"
      },
      "outputs": [],
      "source": [
        "def classify_weather(condition):\n",
        "    condition = condition.lower()  # lowercase for safety\n",
        "    if 'rain' in condition or 'snow' in condition:\n",
        "        return 'rainy'\n",
        "    elif 'overcast' in condition or 'cloudy' in condition:\n",
        "        return 'cloudy'\n",
        "    elif 'clear' in condition:\n",
        "        return 'sunny'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "# Apply binning\n",
        "trips_df['weather_segment'] = trips_df['conditions'].apply(classify_weather)\n",
        "trips_df['weather_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep9rr_N9s9v_"
      },
      "source": [
        "---\n",
        "\n",
        "B11\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_f0gqxJNym1"
      },
      "outputs": [],
      "source": [
        "sorted_ended_at_df = trips_df[['ended_at']].sort_values(by='ended_at')\n",
        "print(\"--- Sorted 'ended_at' DataFrame (first 5 rows) ---\")\n",
        "print(sorted_ended_at_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Find the earliest and latest dates ---\n",
        "earliest_date = sorted_ended_at_df['ended_at'].min()\n",
        "latest_date = sorted_ended_at_df['ended_at'].max()\n",
        "\n",
        "print(f\"The earliest date in 'ended_at' is: {earliest_date}\")\n",
        "print(f\"The latest date in 'ended_at' is: {latest_date}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37IY3SeYr1C1"
      },
      "outputs": [],
      "source": [
        "# Make sure 'ended_at' is datetime\n",
        "# trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'])\n",
        "trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'], format='mixed', errors='coerce')\n",
        "\n",
        "\n",
        "# Extract just the date (without time)\n",
        "trips_df['end_date'] = trips_df['ended_at'].dt.date\n",
        "daily_income_weather = trips_df.groupby(['end_date', 'weather_segment'])['trip_cost'].sum().reset_index()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxfbTLjmtBDz"
      },
      "outputs": [],
      "source": [
        "# convert\n",
        "# Make sure end_date is datetime\n",
        "daily_income_weather['end_date'] = pd.to_datetime(daily_income_weather['end_date'])\n",
        "\n",
        "fig_long = px.line(\n",
        "    daily_income_weather,\n",
        "    x='end_date',\n",
        "    y='trip_cost',\n",
        "    color='weather_segment',\n",
        "    title='Daily Total Trip Cost by Weather Condition (Long Format)',\n",
        "    labels={'end_date': 'Date', 'trip_cost': 'Total Income', 'weather_segment': 'Weather'}\n",
        ")\n",
        "\n",
        "fig_long.update_layout(xaxis_title='Date', yaxis_title='Trip Cost', hovermode='x unified')\n",
        "fig_long.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6WfXRDRtHwQ"
      },
      "outputs": [],
      "source": [
        "# Pivot to wide format\n",
        "wide_df = daily_income_weather.pivot(index='end_date', columns='weather_segment', values='trip_cost').fillna(0)\n",
        "wide_df = wide_df.sort_index()\n",
        "\n",
        "# Build traces\n",
        "fig_wide = go.Figure()\n",
        "\n",
        "for condition in wide_df.columns:\n",
        "    fig_wide.add_trace(go.Scatter(\n",
        "        x=wide_df.index,\n",
        "        y=wide_df[condition],\n",
        "        mode='lines',\n",
        "        name=condition\n",
        "    ))\n",
        "\n",
        "fig_wide.update_layout(\n",
        "    title='Daily Total Trip Cost by Weather Condition (Wide Format)',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Trip Cost',\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white',\n",
        "    legend_title='Weather'\n",
        ")\n",
        "\n",
        "fig_wide.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMDgJxs8OXx_"
      },
      "source": [
        "Which one is better for our problem  ?\n",
        "answer here :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBXs3HzQO-wt"
      },
      "source": [
        "---\n",
        "B12\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN5x_9uTt2RV"
      },
      "outputs": [],
      "source": [
        "# feature 1 : rush_hour\n",
        "# Indicates if the ride occurred during typical commuting hours (7–10 AM or 4–7 PM).\n",
        "trips_df['start_time'] = pd.to_datetime(trips_df['start_time'], errors='coerce')\n",
        "\n",
        "trips_df['rush_hour'] = (\n",
        "    trips_df['start_time'].dt.hour.between(7, 10) |\n",
        "    trips_df['start_time'].dt.hour.between(16, 19)\n",
        ").astype(int)\n",
        "trips_df['rush_hour'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQtxwZZnPxnT"
      },
      "outputs": [],
      "source": [
        "# feature 2 : hour_segment\n",
        "# Categorize ride start times into broader buckets.\n",
        "def get_hour_segment(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Midday'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "trips_df['hour_segment'] = trips_df['start_time'].dt.hour.apply(get_hour_segment)\n",
        "trips_df['hour_segment'].value_counts()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiVY5Od0QKAe"
      },
      "outputs": [],
      "source": [
        "# feature 3 : is_weekend\n",
        "# Helps spot usage patterns on weekends vs weekdays.\n",
        "trips_df['is_weekend'] = trips_df['start_time'].dt.dayofweek >= 5\n",
        "trips_df['is_weekend'] = trips_df['is_weekend'].astype(int)\n",
        "trips_df['is_weekend'].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARUwuaWYQWnr"
      },
      "outputs": [],
      "source": [
        "# feature 4 : ride_density_zone\n",
        "# Based on start location’s proximity to popular stations (e.g., CBD or metro/shuttle stations).\n",
        "trips_df['ride_density_zone'] = np.where(\n",
        "    trips_df['start_nearest_metro_distance'] < 0.5, 'High Density', 'Low Density'\n",
        ")\n",
        "trips_df['ride_density_zone'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKjV0usaQvj8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Save the stations_df DataFrame to a CSV file\n",
        "# Define the path in your Google Drive\n",
        "output_path = '/content/drive/My Drive/BikeShare/trips_df_9-6.csv'\n",
        "\n",
        "# Ensure the directory exists (optional, but good practice)\n",
        "import os\n",
        "output_dir = os.path.dirname(output_path)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Save the DataFrame\n",
        "trips_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"trips_df successfully saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49oBoftHUUaG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**EDA**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Sampling the data\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tTOhRmjguH-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full filtered data stats\n",
        "# print(\"Full Data:\")\n",
        "# print(trips_df['trip_duration_minutes'].describe())\n",
        "\n",
        "# Sampled data stats\n",
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "# print(\"\\nSampled Data:\")\"\"\n",
        "# print(sampled_df['trip_duration_minutes'].describe())\n"
      ],
      "metadata": {
        "id": "ABgwYinmuLkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxL-4ceXUYhn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# A )\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cwZZ30VTEp_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4GFM4egUk3o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# B)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK_yKo3mUtU4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task 1\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_mATqblU0nY"
      },
      "source": [
        "| Method                     | Formula                         | Notes                               |\n",
        "| -------------------------- | ------------------------------- | ----------------------------------- |\n",
        "| **Sturges’ Rule**          | `bins = ceil(log2(n) + 1)`      | Good for small to medium-sized data |\n",
        "| **Freedman–Diaconis Rule** | `bin_width = 2 * IQR / n^(1/3)` | Good for skewed data or outliers    |\n",
        "| **Square Root Rule**       | `bins = sqrt(n)`                | Simple and often a good baseline    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8HlhefNL3gb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedman–Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "# Static histogram\n",
        "fig = go.Figure(\n",
        "    data=[go.Histogram(\n",
        "        x=durations,\n",
        "        nbinsx=bin_count,\n",
        "        marker_color='blue',\n",
        "        opacity=1.0\n",
        "    )]\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Distribution of Trip Duration (in Minutes)\",\n",
        "    xaxis_title=\"Trip Duration (minutes)\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    bargap=0.05,\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG-L58E8ESel"
      },
      "source": [
        "test without outliers :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88-AjkyzEU6m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Choose your cutoff (in minutes)\n",
        "cutoff = 1440  # Modify as needed\n",
        "\n",
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedman–Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "# Create the histogram\n",
        "fig = go.Figure()\n",
        "\n",
        "# Histogram of durations\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=durations,\n",
        "    nbinsx=bin_count,\n",
        "    marker_color='blue',\n",
        "    opacity=1.0,\n",
        "    name=\"Trip Durations\"\n",
        "))\n",
        "\n",
        "# Vertical cutoff line\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[cutoff, cutoff],\n",
        "    y=[0, durations.value_counts().max()],\n",
        "    mode=\"lines\",\n",
        "    line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
        "    name=f\"Cutoff = {cutoff} min\"\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Distribution of Trip Duration (in Minutes) with Cutoff\",\n",
        "    xaxis_title=\"Trip Duration (minutes)\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    bargap=0.05,\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n",
        "\n",
        "# Count how many trips exceed the cutoff\n",
        "sampled_exceed = (sampled_df['trip_duration_minutes'] > cutoff).sum()\n",
        "full_exceed = (trips_df['trip_duration_minutes'] > cutoff).sum()\n",
        "\n",
        "print(f\"Trips in sampled_df exceeding {cutoff} minutes: {sampled_exceed}\")\n",
        "print(f\"Trips in trips_df exceeding {cutoff} minutes: {full_exceed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78CKfIqMzVr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCw3707wXM-6"
      },
      "outputs": [],
      "source": [
        "# Use the original (not divided) trip durations\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "types = sampled_df['rideable_type']\n",
        "\n",
        "# Build the box plot grouped by rideable_type\n",
        "fig = go.Figure()\n",
        "\n",
        "# Loop through each rideable type and add a box\n",
        "for bike_type in sampled_df['rideable_type'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['rideable_type'] == bike_type]['trip_duration_minutes'],\n",
        "        name=bike_type,\n",
        "        boxpoints='outliers',  # show outliers only\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Rideable Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Rideable Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ5QqIOIM4mz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1ZEPTa6MwH-"
      },
      "outputs": [],
      "source": [
        "trips_df['member_casual'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpLjgXVjMiwf"
      },
      "outputs": [],
      "source": [
        "# Use the original (not divided) trip durations\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "types = sampled_df['member_casual']\n",
        "\n",
        "# Build the box plot grouped by rideable_type\n",
        "fig = go.Figure()\n",
        "\n",
        "# Loop through each rideable type and add a box\n",
        "for bike_type in sampled_df['rideable_type'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['rideable_type'] == bike_type]['trip_duration_minutes'],\n",
        "        name=bike_type,\n",
        "        boxpoints='outliers',  # show outliers only\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Rideable Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Rideable Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeo_Z_xBDA9A"
      },
      "source": [
        "dealing with outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzYIZTBODD3w"
      },
      "outputs": [],
      "source": [
        "# Compute IQR\n",
        "Q1 = sampled_df['trip_duration_minutes'].quantile(0.25)\n",
        "Q3 = sampled_df['trip_duration_minutes'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define outlier bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Lower Bound: {lower_bound}\")\n",
        "print(f\"Upper Bound: {upper_bound}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0wC5GyObKO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFx42HGM6BK"
      },
      "outputs": [],
      "source": [
        "# Count Trips Longer Than One Day\n",
        "\n",
        "# Define threshold: 1 day = 1440 minutes\n",
        "one_day_minutes = 1440\n",
        "\n",
        "# Filter trips longer than 1 day\n",
        "long_trips_df = sampled_df[sampled_df['trip_duration_minutes'] > one_day_minutes]\n",
        "long_sampled_df = sampled_df[sampled_df['trip_duration_minutes'] > one_day_minutes]\n",
        "# Show how many there are\n",
        "print(f\"Total number of trips longer than 1 day in full data: {len(long_trips_df)}\")\n",
        "print(f\"Total number of trips longer than 1 day in sampled data: {len(long_sampled_df)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Combine start and end station counts for long trips\n",
        "\n",
        "\n",
        "start_counts = long_trips_df['start_station_id'].value_counts()\n",
        "end_counts = long_trips_df['end_station_id'].value_counts()\n",
        "\n",
        "# Combine them into a single Series\n",
        "total_counts = start_counts.add(end_counts, fill_value=0).astype(int)\n",
        "\n",
        "# Get station info: name and location\n",
        "stations = sampled_df[['start_station_id', 'start_station_name', 'start_lat', 'start_lng']].drop_duplicates()\n",
        "stations = stations.rename(columns={\n",
        "    'start_station_id': 'station_id',\n",
        "    'start_station_name': 'station_name',\n",
        "    'start_lat': 'lat',\n",
        "    'start_lng': 'lng'\n",
        "})\n",
        "\n",
        "# Merge with counts\n",
        "stations['long_trip_count'] = stations['station_id'].map(total_counts).fillna(0).astype(int)\n",
        "\n",
        "# Filter stations with at least 1 long trip\n",
        "stations = stations[stations['long_trip_count'] > 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUi2VkSdPsTG"
      },
      "outputs": [],
      "source": [
        "stations['long_trip_count']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVJdaV5bO0sH"
      },
      "outputs": [],
      "source": [
        "# Center the map on Washington DC\n",
        "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Optional: cluster points\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add stations to the map\n",
        "for _, row in stations.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['lat'], row['lng']],\n",
        "        radius=3 + row['long_trip_count']**0.5,  # scale marker size\n",
        "        color='darkred',\n",
        "        fill=True,\n",
        "        fill_color='crimson',\n",
        "        fill_opacity=0.7,\n",
        "        popup=f\"{row['station_name']}<br>Trips > 1 day: {row['long_trip_count']}\"\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xzP7hhWPPbI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehZahS1cJPcY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# C)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task1\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oNXnXa997gP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(sampled_df['trip_cost'].unique())\n",
        "\n",
        "sampled_df['start_time'] = pd.to_datetime(sampled_df['start_time'])"
      ],
      "metadata": {
        "id": "hAYaUcZQ7rRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# cost Histogram\n",
        "fig = px.histogram(sampled_df, x='trip_cost', nbins=141, title='distrupation of trips cost')\n",
        "fig.show()\n",
        "\n",
        "# cost Boxplot\n",
        "fig = px.box(sampled_df, y='trip_cost', title='Boxplot of trips cost')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "3Av2vDQ_Cw6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- نلاحظ ان اغلب الداتا متوزعة بين ال0 - وال10 دولار بكثرة وان القمة بين 3.5 و4 وهذا يدل على انه يوجد الكثير من الناس مشتركة واغلب الرحل لا تتجاوز ال45 دقيقة\n",
        "\n",
        "- وايضا يوجد قيم اكبر صحيح انها نادرة ولكنها متوزعة وهذا يدل انه يوجد اشخاص تاخذها لمسافات كبيرة ولكنها قليلة  \n",
        "- غالبا الرحل ذات تكلفة العالية اشخاص غير مشتركين بالاضافة الى انهم قد يكونون مرة واحدة فقط يستخدمون الدراجات ولا يعودون الى استخدامها بعد تجربة الخدمة ورؤية السعر\n"
      ],
      "metadata": {
        "id": "VAoAZJ7nSAaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "task2\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F-Memz3ealR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(sampled_df, x='trip_duration_minutes', y='trip_cost', trendline='ols',title='the realtion between duration and cost')\n",
        "fig.show()\n",
        "# lowess', 'rolling', 'ewm', 'expanding', 'ols'"
      ],
      "metadata": {
        "id": "IEWmiWYslF82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   النقاط التي قيمتها قريبة من الصفر كوقت هي تمثل الاعضاء التي لديهم اشتراك ولم يتجاوزوا ال45 دقيقة وكما نلاحظ هم كثر\n",
        "*   ولدينا ثلاث توزعات للنقاط وذلك يعود بسبب الاشتراك او عدمه وحتى مروره بالمنطقة التجارية\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cYQFqMY09kTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uuKHRBScZugs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(sampled_df, x='temp', y='trip_cost', color='member_casual',\n",
        "                 title='cost vs temperatur ')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "SVDDsUqjHqkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   اغلب الرحلات تكون بين 5 درجات وال20 درجة\n",
        "*   عندما تكون درجة الحرارة فوق ال20 نلاحظ ان عدد الرحلات قليل\n",
        "* كما نلاحظ اغلب رحل المشتركين الكلفة غالبا اقل من 10 دولار\n",
        "* نلاحظ ان اغلب الكلف العالية من الغير المشتركين\n",
        "* لايوجد علاقة واضحة بين درجة الحرارة والتكلفة لكن يمكن الفول ان بين ال 5 -15 يمكن للناس ان تذهب برحلات أطول\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TziBT-sqW833"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "323sHquUZsBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_rev = sampled_df.groupby(sampled_df['start_time'].dt.date)['trip_cost'].sum().reset_index(name='revenue')\n",
        "fig = px.line(daily_rev, x='start_time', y='revenue', title='daily incomes')\n",
        "fig.show()\n",
        "\n",
        "sampled_df['week'] = sampled_df['start_time'].dt.isocalendar().week\n",
        "weekly_rev = sampled_df.groupby('week')['trip_cost'].sum().reset_index(name='revenue')\n",
        "fig = px.line(weekly_rev, x='week', y='revenue', title='weekly incomes')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "DqwZvFevaXbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   بالنسبة للايرادات اليومبة نلاحظ وجود بين هبوط وصعود ومع استمرار الايام نلاحظ زيادة بالدخل ونلاحظ تناوب بين صعود وهبوط في الايام ويعود هذا الامر اتوقع انو شخص يلي بيركب يوم بريح اليوم يلي بعدو\n",
        "\n",
        "*   لدينا بشهر april هبوط واضح في الربح السبب قد يعود الى عدم وجود داتا كافية في هذا الشهر\n",
        "\n",
        "* بالنسبة للايرادات الاسبوعية ملاحظ انه بشكل عام الامور نحو زيادة حيث ان هذا التذبذب راح بسبب انو الاسبوعي عطانا الشكل العام بالاسبوع فاصبح خط  اكثر انسيابية\n",
        "\n",
        "* بشكل عام يوجد مشكلة في شهر april\n",
        "\n"
      ],
      "metadata": {
        "id": "etFjRM408uj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task5\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5eD0_i2uIAbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_rev = sampled_df.groupby('start_month')['trip_cost'].mean().reset_index(name='avg_revenue')\n",
        "fig = px.line(monthly_rev, x='start_month', y='avg_revenue', title='average month income')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "m7UREu3XITUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* يبين لنا المخطط تطور متوسط تكلفة الرحلة الواحدة خلال شهر الاول كان متوسط تكلفة الرحلة ما يقارب 3.78 دولار مع دخول الشهر الثاني نلاحظ ارتفاع طفيف ويستمر الارتفاع بشكل طفيف حتى الشهر الثالث هذا النمو التدريجي يوحي بان شيئاً ما كان يتغير ببطء وثبات ربما كان المستخدمون يميلون لأخذ رحلات أطول قليلًا، أو أن هناك زيادة طفيفة في استخدام الدراجات ذات التكلفة الأعلى، أو ربما كان هناك تزايد في الرحلات التي تتخطى الحدود الزمنية المجانية للمشتركين وتتحمل رسومًا إضافية. هذه الزيادة، وإن كانت صغيرة، تشير إلى أن قيمة الرحلة الواحدة كانت في ازدياد\n",
        "* ثم نصل الى شهر الرابع نلاحظ قفزة في متوسط الرحلة الواحدة بشكل ملحوظ حيث وصل ال4 دولار مسجل اعلى متوسط خلال هذه الفترة قد يبدو للحظة ان الامر جيد ولكن مع النظر الى مخطط اليومي والاسبوعي فنحدد شهدنا هبوط في هذا الشهر وقد يعود سبب الهبوط في رفع سعر الرحلة مما ادى الانهيار الخدمة انهياراً كارثياً\n",
        "\n",
        "* وايضا ممكن هذا الارتفاع اتى بما انه عدد الرحلات الاجمالية في الشهر الرابع قليلة فوجود قيم شاذة او مرتفعة كما شهدنا في مخطط كلف الرحل سيرفع متوسط كلفة الرحلة بهذا الشكل\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **الخلاصة**\n",
        "\n",
        "\n",
        "*   كانت خدمة مشاركة الدراجات تشهد نموًا مستمرًا في إجمالي إيراداتها وفي قيمة الرحلة الواحدة من يناير وحتى منتصف مارس.\n",
        "*   مع ذلك، في أواخر مارس/أوائل أبريل، قد تكون الشركة تعرضت لحدث جسيم (إما إغلاق، أو تعليق، أو عطل كبير في النظام) أدى إلى توقف شبه كامل لجميع الرحلات والإيرادات, او قد يكون بسبب رفع رسوم الرحلة\n",
        "* القفزة في متوسط تكلفة الرحلة في أبريل، على الرغم من أنها تبدو إيجابية في هذا الرسم البياني بمفرده، هي في الواقع مجرد انعكاس لحقيقة أن الرحلات القليلة جدًا المتبقية كانت هي الأكثر تكلفة، مما يلقي الضوء على الوضع الكارثي للخدمة في هذا الشهر.\n",
        "\n",
        "* قد يكون سبب اخذ قرار الشركة برفع انها كانت تحاول رفع الرسوم في الاشهر الاولى ولكن بشكل طفيف وعندما وجدت ان المبيعات تزاد قامت بهذه الرفعة ظنا منها انه اصبح لديها قاعدة جماهيرية كبيرة وان المستخدمين بازدياد لتفاجئ بحصول عكس ذلك تماما\n",
        "* كل هذه الامور هي مجرد تفسيرات ممكنة\n",
        "\n",
        "* قد يكون سبب الارتفاع هو وجود تضخم\n"
      ],
      "metadata": {
        "id": "3OjvmEDuMWMX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOP_nkSnJTVM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# D)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWjBMMLQz1C"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task1\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8hGIoisQ9_F"
      },
      "outputs": [],
      "source": [
        "#Loading Residential and Visitor Parking Zones\n",
        "Residential_Visitor_Parking_Zones  = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-fDXw62JRJl"
      },
      "outputs": [],
      "source": [
        "# Step 0: Load residential zones GeoDataFrame (assuming it's already loaded)\n",
        "res_zones = Residential_Visitor_Parking_Zones\n",
        "res_zones = res_zones.to_crs(epsg=4326)  # make sure it matches trip coordinates\n",
        "\n",
        "# Step 1: Create GeoDataFrames for start and end points\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['start_lng'], sampled_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['end_lng'], sampled_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "# Step 2: Spatial join to check which points fall inside residential zones\n",
        "start_in_res = gpd.sjoin(start_gdf, res_zones, predicate='within', how='inner')\n",
        "end_in_res = gpd.sjoin(end_gdf, res_zones, predicate='within', how='inner')\n",
        "\n",
        "# Step 3: Extract lat/lon of trips touching residential zones\n",
        "res_start_points = start_in_res[['start_lat', 'start_lng']].rename(columns={'start_lat': 'lat', 'start_lng': 'lon'})\n",
        "res_end_points = end_in_res[['end_lat', 'end_lng']].rename(columns={'end_lat': 'lat', 'end_lng': 'lon'})\n",
        "\n",
        "# Combine both\n",
        "res_points = pd.concat([res_start_points, res_end_points], ignore_index=True)\n",
        "\n",
        "# Step 4: Count total trips that are outside residential zones (neither start nor end matched)\n",
        "trip_ids_with_res = set(start_in_res['ride_id']).union(set(end_in_res['ride_id']))\n",
        "non_res_trip_count = sampled_df[~sampled_df['ride_id'].isin(trip_ids_with_res)].shape[0]\n",
        "\n",
        "# Step 5: Plot heatmap with Plotly\n",
        "fig = px.density_mapbox(\n",
        "    res_points,\n",
        "    lat='lat',\n",
        "    lon='lon',\n",
        "    radius=10,\n",
        "    center=dict(lat=res_points['lat'].mean(), lon=res_points['lon'].mean()),\n",
        "    zoom=11,\n",
        "    mapbox_style='carto-positron',\n",
        "    title='Geographic Heatmap of Trips to Residential Zones'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
        "    dragmode=False\n",
        ")\n",
        "\n",
        "fig.show(config={\"staticPlot\": True})  # disables all interactivity\n",
        "\n",
        "# Step 6: Print number of trips outside residential zones\n",
        "print(f\"Total number of trips outside residential zones: {non_res_trip_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZTM_ZuT-zD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY_LY76_J5iH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Count trips per geohash sector\n",
        "geohash_counts = sampled_df['geohash_p6'].value_counts().reset_index()\n",
        "geohash_counts.columns = ['geohash_p6', 'trip_count']\n",
        "\n",
        "# Optional: sort alphabetically or by count\n",
        "geohash_counts = geohash_counts.sort_values(by='trip_count', ascending=False)\n",
        "\n",
        "# Step 2: Plot\n",
        "fig = px.bar(\n",
        "    geohash_counts,\n",
        "    x='geohash_p6',\n",
        "    y='trip_count',\n",
        "    title='Distribution of Trips by Geographic Sector (Geohash_p6)',\n",
        "    labels={'geohash_p6': 'Geographic Sector', 'trip_count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "# Step 3: Turn off interactivity\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ZXZV3XVa7n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EINWUb4QUIeO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Distance to CBD\n",
        "fig1 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='distance_to_cbd_m',\n",
        "    nbins=40,\n",
        "    title='Distribution of Distance to CBD (m)',\n",
        "    labels={'distance_to_cbd_m': 'Distance to CBD (meters)'}\n",
        ")\n",
        "fig1.show(config={'staticPlot': True})\n",
        "\n",
        "# 2. Closest Metro Station Distance\n",
        "fig2 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_metro_distance',\n",
        "    nbins=30,\n",
        "    title='Distribution of Distance to Nearest Metro Station',\n",
        "    labels={'start_nearest_metro_distance': 'Distance to Metro (meters)'}\n",
        ")\n",
        "fig2.show(config={'staticPlot': True})\n",
        "\n",
        "# 3. Closest Shuttle Station Distance\n",
        "fig3 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_shuttle_distance',\n",
        "    nbins=30,\n",
        "    title='Distribution of Distance to Nearest Shuttle Station',\n",
        "    labels={'start_nearest_shuttle_distance': 'Distance to Shuttle (meters)'}\n",
        ")\n",
        "fig3.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6Zxf96lZssHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Categorize trips\n",
        "def classify_trip(row):\n",
        "    if row['start_in_cbd'] == 1 and row['end_in_cbd'] == 1:\n",
        "        return 'Fully in CBD'\n",
        "    else:\n",
        "        return 'Outside CBD'\n",
        "\n",
        "# Apply classification\n",
        "sampled_df['cbd_trip_type'] = sampled_df.apply(classify_trip, axis=1)\n",
        "\n",
        "# Count\n",
        "trip_cbd_counts = sampled_df['cbd_trip_type'].value_counts().reset_index()\n",
        "trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    trip_cbd_counts,\n",
        "    x='Trip Type',\n",
        "    y='Count',\n",
        "    title='Trips Fully in CBD vs Outside',\n",
        "    text='Count',\n",
        "    labels={'Count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(yaxis_title='Number of Trips', xaxis_title='Trip Category')\n",
        "fig.show(config={'staticPlot': True})\n"
      ],
      "metadata": {
        "id": "63xLkRdqrujr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppoVrQwUsq63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trips_df['cbd_trip_type'] = trips_df.apply(classify_trip, axis=1)\n",
        "full_trip_cbd_counts = trips_df['cbd_trip_type'].value_counts().reset_index()\n",
        "full_trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "full_trip_cbd_counts['Percentage'] = (full_trip_cbd_counts['Count'] / full_trip_cbd_counts['Count'].sum()) * 100\n",
        "full_trip_cbd_counts"
      ],
      "metadata": {
        "id": "IoWwMMOttvP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task5\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xbWLXpMNuCVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filter trips that passed through CBD\n",
        "cbd_passed_df = sampled_df[\n",
        "    (sampled_df['start_in_cbd'] == 1) | (sampled_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    grouped,\n",
        "    x='rideable_type',\n",
        "    y='trip_count',\n",
        "    color='member_casual',\n",
        "    barmode='group',\n",
        "    title='Trips That Passed Through CBD by Rideable Type and Membership',\n",
        "    labels={'trip_count': 'Number of Trips', 'rideable_type': 'Bike Type'}\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Rideable Type',\n",
        "    yaxis_title='Number of Trips'\n",
        ")\n",
        "fig.show(config={'staticPlot': True})\n"
      ],
      "metadata": {
        "id": "re3aHjZmtmpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbd_passed_df_trips_df=trips_df[\n",
        "    (trips_df['start_in_cbd'] == 1) | (trips_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df_trips_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "print(f\"Length of cbd_passed_df_trips_df: {len(cbd_passed_df_trips_df)}\")\n",
        "print(f\"Length of trips_df: {len(trips_df)}\")\n",
        "\n",
        "percentage = (len(cbd_passed_df_trips_df) / len(trips_df)) * 100\n",
        "print(f\"Percentage of cbd_passed_df_trips_df compared to trips_df: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "bE6Ah0JBuxCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task6\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SUGoOn14yROg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a contingency table\n",
        "# (Counts of each combination)\n",
        "\n",
        "# Make sure we’re using categorical data\n",
        "contingency_table = pd.crosstab(trips_df['close_to_cbd'], trips_df['member_casual'])\n",
        "contingency_table\n"
      ],
      "metadata": {
        "id": "YmcXeVxLv7yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi2 Statistic:\", chi2)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"P-value:\", p)\n",
        "# interpretion based on the p-value:\n",
        "\n",
        "if p < 0.05:\n",
        "    print(\"✅ There is a significant correlation between distance to CBD segments and membership type.\")\n",
        "else:\n",
        "    print(\"❌ No significant correlation found between distance to CBD segments and membership type.\")\n"
      ],
      "metadata": {
        "id": "00075kXbydNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| α Value  | Interpretation                                                                |\n",
        "| -------- | ----------------------------------------------------------------------------- |\n",
        "| **0.05** | Most common — means you're willing to accept a 5% chance of a false positive. |\n",
        "| 0.01     | Stricter — used in more critical fields (medicine, etc.).                     |\n",
        "| 0.10     | Looser — sometimes used in exploratory research.                              |\n"
      ],
      "metadata": {
        "id": "IIDvh1wlz_gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Member trips are more common outside the CBD (proportionally).\n",
        "\n",
        "Casual riders are slightly more concentrated inside the CBD, which makes sense:\n",
        "\n",
        "Casuals may be tourists or occasional users.\n",
        "\n",
        "Members might be commuting or local residents going to/from suburban areas.\n"
      ],
      "metadata": {
        "id": "K0oXDOroztOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# E)\n",
        "---\n"
      ],
      "metadata": {
        "id": "tTmCrroEvV9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Task 1\n",
        "---"
      ],
      "metadata": {
        "id": "M7foeLjdv92S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df.columns"
      ],
      "metadata": {
        "id": "Q612ysGB081J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df['rideable_type'].unique()"
      ],
      "metadata": {
        "id": "k6KBbAssmo01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_weather_avg = sampled_df.groupby('date')[['temp', 'humidity', 'windspeed']].mean().reset_index()\n",
        "daily_weather_avg = daily_weather_avg.rename(columns={\n",
        "    'temp': 'Average Temperature',\n",
        "    'humidity': 'Average Humidity',\n",
        "    'windspeed': 'Average Wind Speed'\n",
        "})\n",
        "fig = px.line(\n",
        "    daily_weather_avg,\n",
        "    x='date',\n",
        "    y=['Average Temperature', 'Average Humidity', 'Average Wind Speed'], # List of columns for y-axis\n",
        "    title='Average Daily Weather Conditions (Temperature, Humidity, Wind Speed)',\n",
        "    labels={\n",
        "        'date': 'Date',\n",
        "        'value': 'Average Value', # Default label for the combined y-axis values\n",
        "        'variable': 'Metric'     # Default label for the legend (which variable is which line)\n",
        "    }\n",
        ")\n",
        "fig.update_layout(hovermode=\"x unified\") # Enhances hover tooltips for multiple lines\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "shZBWnGCvY91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---"
      ],
      "metadata": {
        "id": "si1pOnizmFy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_weather_cond = sampled_df.groupby('date')['weather_segment'].first().reset_index()\n",
        "\n",
        "daily_rev = sampled_df.groupby(sampled_df['date'])['trip_cost'].sum().reset_index(name='revenue')\n",
        "\n",
        "merged_df = pd.merge(daily_rev, daily_weather_cond, on='date', how='left')\n",
        "fig = px.box(\n",
        "    merged_df,\n",
        "    x='weather_segment',  # Categorical variable on x-axis\n",
        "    y='revenue',      # Numerical variable on y-axis\n",
        "    title='Daily Revenue by Weather Condition',\n",
        "    labels={\n",
        "        'weather_condition': 'Weather Condition',\n",
        "        'daily_revenue': 'Daily Revenue ($)'\n",
        "    },\n",
        "    category_orders={\"weather_condition\": [\"Sunny\", \"Cloudy\", \"Rainy\"]} # Optional: ensure specific order\n",
        ")\n",
        "fig.update_traces(boxpoints='all', jitter=0.3) # Show individual points for more detail\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Jezx6JnlOeVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   نلاحظ بالنسبة للايام الماطرة يقع وسطيا الايرادات عند مايقارب 650 دولار وهو  اقل  متوسط من جميع حالات طقس رغم وجود عدد كبير من الايام ماطرة ما يقارب 55 بالمئة من الأيام هي ماطرة ونلاحظ مدى توسع الصندوق وهذا يشير الى تقلب كبير في الايرادات في الأيام الماطرة وملاحظ هذا حيث لدينا ايام الايرادات تقارب الصفر وبعض متجاوزة الالف واعتقد يعود السبب الى القيم القريبة الى الصفر هي الايام ذو امطار شديدة وهذا منطقي من الصعب عندها ركوب الدراجات اما بالنسبة للقيم العالية وارد ان بعض الايام الممطرة تكون مقبولة وهذا يعود الى بعض انواع المستخدمين التي تستمع في ذلك او بسبب الحاجة العاجلة للدراجة بدل الانتظار\n",
        "*   نلاحظ بالنسبة للايام الغائمة مرتفع وسطي الايرادات لما يقارب 800 دولار اكثر من الايام الماطرة ونلاحظ انه يوجد استقرار وليس تقلب بالايرادات وايضا الاتجاه الايرادات في ايام الغائمة اما بازدياد او استقرار ونلاحظ قفزات عالية جدا في الايرادات وارد ذلك عند درجات الحرارة المعتدلة اما بالنسبة للقيم المتدنية جدا فهي اما بشهر الرابع او انها كانت ايام عطل\n",
        "\n",
        "* رغم قلة الايام المشمسة الا اننا نجد ان الناس تتجه لاستخدام الدراجات وهي اعلى متوسط دخل وملاحظ ان الناس في الايام المشمسة تميل الى استخدام الدراجات وقد يعود ذلك بسبب قلة الايام المشمسة الموجود فالناس تحب التعرض للشمس لذلك تفضل عندها استخدام الدراجات بالاضافة ان الجو يكون جيد\n",
        "\n",
        "* حيث نستنتج تأثير الطقس على سلوك الركاب يظهر بوضوح كيف تؤثر حالة الطقس بشكل مباشر على الإيرادات اليومية، حيث يفضل الناس استخدام الدراجات في الطقس المعتدل والمشمس، مما يؤدي إلى زيادة الإيرادات، في حين أن الأيام الممطرة العكس اقل مستخدمين وايرادات اقل\n",
        "\n",
        "* لكن وجود القيم الشاذة في جميع الفئات يدل على أن هناك دائمًا بعض الأيام التي لا تتبع النمط العام للطقس، سواء كانت جيدة بشكل استثنائي أو سيئة بشكل استثنائي حيث عندها اتوقع يوجد امور اخرى  تدخل عندها\n",
        "\n"
      ],
      "metadata": {
        "id": "bRyT5G-y2asD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Task3\n",
        "---"
      ],
      "metadata": {
        "id": "igR_Nc1J9JMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lowess', 'rolling', 'ewm', 'expanding', 'ols'\n",
        "# --- Apply Min-Max Normalization to 'daily_revenue' ---\n",
        "# xi-xmin /xmax-xmin\n",
        "\n",
        "# min_revenue = daily_rev['revenue'].min()\n",
        "# max_revenue = daily_rev['revenue'].max()\n",
        "# daily_rev['normalized_daily_revenue'] = (daily_rev['revenue'] - min_revenue) / (max_revenue - min_revenue)\n",
        "\n",
        "merg = pd.merge(daily_weather_avg,daily_rev,on='date',how='left')\n",
        "\n",
        "cols_to_normalize = ['revenue', 'Average Temperature', 'Average Humidity']\n",
        "for col in cols_to_normalize:\n",
        "    min_val = merg[col].min()\n",
        "    max_val = merg[col].max()\n",
        "    # Avoid division by zero if all values are the same\n",
        "    if (max_val - min_val) != 0:\n",
        "        merg[f'normalized_{col}'] = (merg[col] - min_val) / (max_val - min_val)\n",
        "    else: # If all values are the same, normalized value is 0 (or 1, depends on convention)\n",
        "        merg[f'normalized_{col}'] = 0.0\n",
        "\n",
        "fig1 = px.scatter(merg,x='normalized_Average Temperature',y='normalized_revenue',\n",
        "                 title=\"relationship between daily income and temperature\",trendline='ols',\n",
        "                 labels={\n",
        "                   'Temperature': 'normalized_Average Daily Temperature',\n",
        "                   'daily_revenue': 'Daily Revenue ($)' }\n",
        "                 )\n",
        "fig1.show()\n",
        "\n",
        "\n",
        "fig2 = px.scatter(merg,x='normalized_Average Humidity',y='normalized_revenue',\n",
        "                 title=\"relationship between daily income and Humidity\",trendline='ols',\n",
        "                 labels={\n",
        "                   'Humidity': 'normalized_Average Daily humidity',\n",
        "                   'daily_revenue': 'Daily Revenue ($)' }\n",
        "                 )\n",
        "fig2.show()\n"
      ],
      "metadata": {
        "id": "g0BuFBl0sec0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* بالنسبة للعلاقة بين الايرادات اليومية ودرجة الحرارة نلاحظ وجود علافة ارتباط خطي ايجابية حيث في درجات الحرارة المنخفضة (-5 - 3) نرى انخفاض في الايرادات ثم مع ازدياد درجة الحرارة نلاحظ انها تزداد الايردات الى ان تصل الى حد معين ثم تبدء بالنزول حيث ازدياد درجة الحرارة الى درجة ما وهي 16 يؤدي ازدياد الايرادات ولكن بعدها نرى ان ازدياد درجة الحرارة سيؤدي الى انخفاض في الايرادات\n",
        "\n",
        "\n",
        "* بالنسبة للارتباط الخطي بين الايرادات والرطوبة لا يوجد علاقة ارتباط خطي حيث نلاحظ عند رطوبة منخقضة لدينا ايردادات مرتفعة وايرادات ومنخفضة والامر على القيم اي عندما تكون الرطوبة متوسطة او حتى عالية لدينا الايرادات مرات تكون منخفضة ومرات تكون عالية"
      ],
      "metadata": {
        "id": "mh3X6Ti7GuF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task4"
      ],
      "metadata": {
        "id": "u45Frt3lyl5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # 1. Create the Contingency Table\n",
        "# This table shows the observed frequencies (counts) of each unique combination\n",
        "# of weather segment and ride type.\n",
        "# Rows: weather_segment\n",
        "# Columns: rideable_type\n",
        "contingency_table = pd.crosstab(sampled_df['weather_segment'], sampled_df['rideable_type'])\n",
        "print(\"Contingency Table (Observed Frequencies):\")\n",
        "print(contingency_table)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Visual separator in output\n",
        "\n",
        "# 2. Perform the Chi-Square Test\n",
        "# The chi2_contingency function performs the statistical calculations.\n",
        "# It returns four values:\n",
        "#   - chi2: The calculated Chi-Square statistic.\n",
        "#   - p_value: The probability value (most important for interpretation).\n",
        "#   - dof: Degrees of freedom.\n",
        "#   - expected_frequencies: A 2D array of expected frequencies if the variables were independent.\n",
        "chi2, p_value, dof, expected_frequencies = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi2 Statistic: {chi2:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(\"\\nExpected Frequencies Table:\")\n",
        "\n",
        "# Display the expected frequencies array as a DataFrame for better readability,\n",
        "# using the same indices (rows) and columns as the observed contingency table.\n",
        "print(pd.DataFrame(expected_frequencies, index=contingency_table.index, columns=contingency_table.columns))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\") # Another visual separator\n",
        "\n",
        "\n",
        "# 3. Interpret the Results\n",
        "# Define the significance level (alpha), which is the threshold for comparing the p_value.\n",
        "# A common alpha level is 0.05 (or 5%).\n",
        "alpha = 0.05\n",
        "print(\"Interpretation of Results:\")\n",
        "if p_value < alpha:\n",
        "    # If the p-value is less than alpha, we reject the null hypothesis.\n",
        "    # The null hypothesis (H0) here is: There is no relationship between weather condition and ride type.\n",
        "    print(f\"Since the P-value ({p_value:.4f}) is less than the significance level (alpha = {alpha}),\")\n",
        "    print(\"we reject the null hypothesis (H0).\")\n",
        "    print(\"Conclusion: There is strong statistical evidence of a significant relationship between weather condition and ride type.\")\n",
        "    print(\"In other words, it appears that the distribution of ride types (or bike types) differs depending on the weather condition.\")\n",
        "    print(\"\\n* To understand this relationship further, compare the observed frequencies with the expected frequencies to identify which categories contribute most to the association.\")\n",
        "else:\n",
        "    # If the p-value is greater than or equal to alpha, we fail to reject the null hypothesis.\n",
        "    print(f\"Since the P-value ({p_value:.4f}) is greater than or equal to the significance level (alpha = {alpha}),\")\n",
        "    print(\"we fail to reject the null hypothesis (H0).\")\n",
        "    print(\"Conclusion: There is no sufficient statistical evidence to claim a significant relationship between weather condition and ride type.\")\n",
        "    print(\"In other words, it appears that the choice of ride type (or bike type) is not significantly affected by the weather condition, or any observed differences could be due to random chance.\")\n",
        "\n",
        "\n",
        "df_plot = contingency_table.reset_index().melt(id_vars='weather_segment', var_name='rideable_type', value_name='Count')\n",
        "\n",
        "# 2. Draw a Grouped Bar Chart\n",
        "fig = px.bar(\n",
        "    df_plot,\n",
        "    x='weather_segment',  # X-axis will be weather conditions\n",
        "    y='Count',            # Y-axis will be the number of rides\n",
        "    color='rideable_type',    # Different bars for each ride type within each weather condition\n",
        "    barmode='group',      # This makes the bars for each ride_type stand side-by-side\n",
        "    title='Ride Type Distribution by Weather Condition',\n",
        "    labels={\n",
        "        'weather_segment': 'Weather Condition',\n",
        "        'Count': 'Number of Rides',\n",
        "        'Ride Type': 'Ride Type'\n",
        "    },\n",
        "    category_orders={\"weather_segment\": [\"Sunny\", \"Cloudy\", \"Rainy\"]} # Optional: ensure specific order\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_title=\"Weather Condition\", yaxis_title=\"Number of Rides\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "iQtcFMU1Kxgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vrGtM8NTTRjF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bZTfRsiq6CQa",
        "Nxjq-QmE5b9K",
        "SL1oTCjr6OFa",
        "zRUlFc1tO8QN",
        "e6HWUwZfBbaR",
        "3MC8yVG5pmu6",
        "6-inlm6X1UtR",
        "3M_uabQt6Bza",
        "Ellq2QJVzAe3",
        "PRZhifmX5KRC",
        "Fo1Nby-lsEf9",
        "ep9rr_N9s9v_"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}