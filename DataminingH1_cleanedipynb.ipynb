{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF3CEiZb4GtS"
      },
      "source": [
        "# **Loading libraries**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xs-YTsgO8QA",
        "outputId": "e8e5a540-c263-4e5c-ba78-e77b7360dd83"
      },
      "outputs": [],
      "source": [
        "%pip install gdown\n",
        "%pip install tqdm scikit-learn\n",
        "%pip install geopandas\n",
        "%pip install geohash2\n",
        "%pip install folium\n",
        "%pip install python-geohash\n",
        "%pip install statsmodels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import gdown\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "from scipy.stats import gaussian_kde\n",
        "from google.colab import drive\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "from tqdm import tqdm\n",
        "import geohash2\n",
        "from sklearn.cluster import KMeans\n",
        "import json\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "from scipy.stats import chi2_contingency\n",
        "import geohash as gh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I05pgLlnO8QA",
        "outputId": "a660808d-3b70-4dd3-f898-fcc09f5d2b10"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTfRsiq6CQa"
      },
      "source": [
        "# **Loading the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9lConBSO8QB"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "downloading the dataset\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkCADdlJzQGc"
      },
      "outputs": [],
      "source": [
        "folder_id = '1O3w5OKnS__hzlL8kTSfGCUc_iX8XNjEN'\n",
        "output_dir = 'Homework'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "print(f\"Attempting to download content from folder ID: {folder_id} into {output_dir}\")\n",
        "try:\n",
        "    gdown.download_folder(id=folder_id, output=output_dir, quiet=False, use_cookies=False)\n",
        "    print(f\"\\nSuccessfully downloaded content to: /content/{output_dir}\")\n",
        "    print(\"You can now find the downloaded content in the 'downloaded_external_folder' directory in your Colab files browser.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during download: {e}\")\n",
        "    print(\"Please ensure the Google Drive folder is publicly accessible or shared with 'Anyone with the link can view'.\")\n",
        "\n",
        "stations_info=pd.read_csv(\"Homework/data/Capital_Bikeshare_Locations.csv\")\n",
        "#\n",
        "# Load tabular data\n",
        "weather_df = pd.read_csv(\"Homework/data/Washington,DC,USA 2024-01-01 to 2024-12-31.csv\")\n",
        "trips_df = pd.read_parquet('Homework/data/daily-rent.parquet')\n",
        "\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "\n",
        "stations_df = pd.read_csv(\"Homework/data/Capital_Bikeshare_Locations.csv\")\n",
        "# Load spatial parking zones\n",
        "parking_zones_gdf = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "Shuttle_Bus_Stops=pd.read_csv(\"Homework/data/Shuttle_Bus_Stops.csv\")\n",
        "Metro_Bus_Stops =pd.read_csv(\"Homework/data/Metro_Bus_Stops.csv\")\n",
        "#Loading Residential and Visitor Parking Zones\n",
        "Residential_Visitor_Parking_Zones  = gpd.read_file('Homework/data/Residential_and_Visitor_Parking_Zones.geojson')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjq-QmE5b9K"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Downloading the combined and modified dataset (for ease of use )\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWTtq7nB0Q0T",
        "outputId": "ee703192-0b1f-4348-fb33-65fc176de944"
      },
      "outputs": [],
      "source": [
        "# prompt: i want to get a file from my drive its called trips_df_tempdelete.csv\n",
        "\n",
        "# Define the path to the file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/trips_df_tempdelete.csv'\n",
        "\n",
        "# Check if the file exists before attempting to read it\n",
        "if os.path.exists(file_path):\n",
        "    # Read the CSV file into a pandas DataFrame\n",
        "    temp_trips_df = pd.read_csv(file_path)\n",
        "    print(\"File loaded successfully.\")\n",
        "    # Display the first few rows and info to confirm\n",
        "    print(temp_trips_df.head())\n",
        "    print(temp_trips_df.info())\n",
        "else:\n",
        "    print(f\"File not found at: {file_path}\")\n",
        "    print(\"Please check the file path and ensure your Google Drive is mounted correctly.\")\n",
        "    print(\"You can list files in your drive to confirm the path:\")\n",
        "    !ls -R \"/content/drive/MyDrive/\"\n",
        "trips_df = pd.read_csv(file_path)\n",
        "print(f\"File downloaded to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wrlFZdU0f2Q",
        "outputId": "7f5496c1-3f35-4970-edc8-d345b2e7761c"
      },
      "outputs": [],
      "source": [
        "trips_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "nI0R9q-L5l8o",
        "outputId": "18558bfb-c7b6-4ac9-eb08-eeaff957ac25"
      },
      "outputs": [],
      "source": [
        "file_id = \"114g7JYuZ00i864przAIJQYymib_5h6Qa\"  # Replace with your actual file ID\n",
        "\n",
        "\n",
        "output_file = \"trips_df.csv\"  # You can change the output file name\n",
        "\n",
        "gdown.download(id=file_id, output=output_file, quiet=False)\n",
        "trips_df = pd.read_csv(output_file)\n",
        "print(f\"File downloaded to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_an-r3FItdCv"
      },
      "outputs": [],
      "source": [
        "trips_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL1oTCjr6OFa"
      },
      "source": [
        "# **Preprocessing , Cleaning & inspecting the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gt_DuEv4w9P"
      },
      "source": [
        "\n",
        "There is a problem with missing start/id , almost 20% of the data are nulls so we must find a way to fill these up\n",
        "\n",
        "**spatial join**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "using lang and lati we can match it to the nearest station and then assign this id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h20Hhlhg1IFk"
      },
      "outputs": [],
      "source": [
        "trips_df = trips_df.dropna(subset=['end_lat', 'end_lng'])\n",
        "\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw3RvCyR1T9h"
      },
      "outputs": [],
      "source": [
        "# EPSG:4326 = lat/lon\n",
        "trips_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "stations_gdf = gpd.GeoDataFrame(\n",
        "    stations_df,\n",
        "    geometry=gpd.points_from_xy(stations_df['LONGITUDE'], stations_df['LATITUDE']),\n",
        "    crs='EPSG:4326'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA47hcEu1ldC"
      },
      "outputs": [],
      "source": [
        "# Find nearest station to each ride\n",
        "trips_with_nearest_station = gpd.sjoin_nearest(\n",
        "    trips_gdf, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"distance\"\n",
        ")\n",
        "\n",
        "# Now we fill missing station_id with nearest one\n",
        "trips_df['start_station_id'] = trips_df['start_station_id'].fillna(\n",
        "    trips_with_nearest_station['STATION_ID']\n",
        ")\n",
        "# Creating a mapping from STATION_ID to STATION_NAME\n",
        "id_to_name = stations_df.set_index('STATION_ID')['NAME'].to_dict()\n",
        "\n",
        "# Fill in missing start_station_name using start_station_id\n",
        "trips_df['start_station_name'] = trips_df['start_station_name'].fillna(\n",
        "    trips_df['start_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df_cleaned=trips_df.drop_duplicates()\n",
        "trips_df_cleaned.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8isYZzA4yfR"
      },
      "source": [
        "Repeating the process to end id and name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIcXJiy95mU9"
      },
      "outputs": [],
      "source": [
        "trips_gdf_end = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "trips_with_nearest_end_station = gpd.sjoin_nearest(\n",
        "    trips_gdf_end, stations_gdf[['STATION_ID', 'geometry']],\n",
        "    how=\"left\", distance_col=\"end_distance\"\n",
        ")\n",
        "\n",
        "trips_df['end_station_id'] = trips_df['end_station_id'].fillna(\n",
        "    trips_with_nearest_end_station['STATION_ID']\n",
        ")\n",
        "trips_df['end_station_name'] = trips_df['end_station_name'].fillna(\n",
        "    trips_df['end_station_id'].map(id_to_name)\n",
        ")\n",
        "trips_df=trips_df.drop_duplicates()\n",
        "trips_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkda9AV_6sc4"
      },
      "source": [
        "we will continue inspecting the rest of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo2_a0EG6qbm"
      },
      "outputs": [],
      "source": [
        "stations_df=stations_df.drop_duplicates()\n",
        "stations_df.isna().sum()  # we dont need to drop null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4BwUW136uQH"
      },
      "outputs": [],
      "source": [
        "weather_df=weather_df.drop_duplicates()\n",
        "weather_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQGps8Rt62eU"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrMSX_Av6vdY"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhqxpEU267b0"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf = parking_zones_gdf.drop(columns=['CREATOR', 'CREATED','EDITOR','EDITED'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_WPYmtO8iLt"
      },
      "outputs": [],
      "source": [
        "parking_zones_gdf=parking_zones_gdf.drop_duplicates()\n",
        "parking_zones_gdf.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Laqgf-vPBIl"
      },
      "source": [
        "---\n",
        "**The outside WDC problem :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgmtRPGpPKqE"
      },
      "outputs": [],
      "source": [
        "# The bounding box method\n",
        "DC_LAT_MIN = 38.7916\n",
        "DC_LAT_MAX = 38.9955\n",
        "DC_LNG_MIN = -77.1198\n",
        "DC_LNG_MAX = -76.9094\n",
        "# Filtering  Points Outside the Bounding Box\n",
        "out_of_bounds_start = ~(\n",
        "    (trips_df['start_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &\n",
        "    (trips_df['start_lng'].between(DC_LNG_MIN, DC_LNG_MAX))\n",
        ")\n",
        "\n",
        "out_of_bounds_end = ~(\n",
        "    (trips_df['end_lat'].between(DC_LAT_MIN, DC_LAT_MAX)) &\n",
        "    (trips_df['end_lng'].between(DC_LNG_MIN, DC_LNG_MAX))\n",
        ")\n",
        "\n",
        "# Combine both to detect any trip with at least one bad coordinate\n",
        "outlier_mask = out_of_bounds_start | out_of_bounds_end\n",
        "outliers = trips_df[outlier_mask]\n",
        "\n",
        "# Inspect the Outliers\n",
        "print(f\"Number of outlier trips: {len(outliers)}\")\n",
        "outliers[['ride_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng']].head()\n",
        "# Total number of trips in the dataset\n",
        "total_trips = len(trips_df)\n",
        "\n",
        "# Number of outliers detected\n",
        "num_outliers = len(outliers)\n",
        "\n",
        "# Calculate the percentage of outliers\n",
        "percentage_outliers = (num_outliers / total_trips) * 100\n",
        "\n",
        "print(f\"Percentage of outlier trips: {percentage_outliers:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6sn_C7ASFUj"
      },
      "outputs": [],
      "source": [
        "# we will drop them\n",
        "trips_df = trips_df[~outlier_mask].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcTb9jx8S9KT"
      },
      "outputs": [],
      "source": [
        "# checking ride_id\n",
        "print(\"Duplicate ride_ids:\", trips_df['ride_id'].duplicated().sum())\n",
        "print(\"Missing ride_ids:\", trips_df['ride_id'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIRvlXxGVCng"
      },
      "source": [
        "*there an issue with dublicated ride_id so we will only keep the first occurrence*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-Dv6GvgUzWk"
      },
      "outputs": [],
      "source": [
        "# Keep first occurrence or drop based on your context:\n",
        "trips_df = trips_df.drop_duplicates(subset='ride_id', keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4RYU-8_TK-i"
      },
      "outputs": [],
      "source": [
        "print(\"Null times:\", trips_df[['started_at', 'ended_at']].isna().sum())\n",
        "print(\"Negative durations:\", (trips_df['ended_at'] < trips_df['started_at']).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY510PKnTOV9"
      },
      "outputs": [],
      "source": [
        "print(\"Missing start station:\", trips_df['start_station_id'].isna().sum())\n",
        "print(\"Missing end station:\", trips_df['end_station_id'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E87_lOcUTZ5P"
      },
      "outputs": [],
      "source": [
        "zero_coords = trips_df[(trips_df['start_lat'] == 0) | (trips_df['start_lng'] == 0)]\n",
        "print(\"Zero coordinates:\", len(zero_coords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcjBcf4NTcqH"
      },
      "outputs": [],
      "source": [
        "# checkign if rideable_type and member_casual has weird values\n",
        "print(\"Ride types:\", trips_df['rideable_type'].unique())\n",
        "print(\"Member types:\", trips_df['member_casual'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhYVVGgM9mwY"
      },
      "outputs": [],
      "source": [
        "weather_df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHgrh-3o-F1Y"
      },
      "outputs": [],
      "source": [
        "# first we make sure all the dates are in the same format (by checking the length)\n",
        "datetime_lengths = weather_df[\"datetime\"].astype(str).apply(len)\n",
        "print(datetime_lengths.value_counts())\n",
        "weather_df[\"date\"] = pd.to_datetime(weather_df[\"datetime\"])\n",
        "print(weather_df[\"date\"].dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU_XzGfm-3st"
      },
      "outputs": [],
      "source": [
        "trips_df[\"start_time\"] = pd.to_datetime(trips_df[\"started_at\"])\n",
        "trips_df[\"end_time\"] = pd.to_datetime(trips_df[\"ended_at\"])\n",
        "# ensuring that CRS is EPSG:4326\n",
        "if parking_zones_gdf.crs != \"EPSG:4326\":\n",
        "    parking_zones_gdf = parking_zones_gdf.to_crs(\"EPSG:4326\")\n",
        "# Spatial Join to Map Stations to Parking Zones\n",
        "# Spatial join: add zone info to each station\n",
        "stations_with_zone = gpd.sjoin(\n",
        "    stations_gdf,\n",
        "    parking_zones_gdf[[\"NAME\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"\n",
        ")\n",
        "# Rename column for clarity\n",
        "stations_with_zone = stations_with_zone.rename(columns={\"zone_name\": \"residential_zone\"})\n",
        "# Joining Weather Data\n",
        "# Extract date from start_time for weather join\n",
        "trips_df[\"date\"] = trips_df[\"start_time\"].dt.date\n",
        "weather_df[\"date\"] = weather_df[\"date\"].dt.date\n",
        "\n",
        "# Join weather by date\n",
        "trips_df = trips_df.merge(weather_df, on=\"date\", how=\"left\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MHwqZPyBL_t"
      },
      "outputs": [],
      "source": [
        "trips_df[['start_station_id', 'end_station_id', 'start_station_name', 'end_station_name']].isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnhDD_fQBZvY"
      },
      "outputs": [],
      "source": [
        "trips_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLYxFqxxhgQ0"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Feature engineering**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRUlFc1tO8QN"
      },
      "source": [
        "\n",
        "---\n",
        "B1\n",
        "---\n",
        "استخلاص المكونات الزمنية من الرحلات\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "48-Qa2HGpClZ",
        "outputId": "17da41bd-0ffc-44cb-c275-4a920ae435ff"
      },
      "outputs": [],
      "source": [
        "# B1\n",
        "\n",
        "# From started_at\n",
        "trips_df['start_year'] = trips_df['started_at'].dt.year\n",
        "trips_df['start_month'] = trips_df['started_at'].dt.month\n",
        "trips_df['start_day_num'] = trips_df['started_at'].dt.day\n",
        "trips_df['start_day_name'] = trips_df['started_at'].dt.day_name()\n",
        "\n",
        "# From ended_at\n",
        "trips_df['end_year'] = trips_df['ended_at'].dt.year\n",
        "trips_df['end_month'] = trips_df['ended_at'].dt.month\n",
        "trips_df['end_day_num'] = trips_df['ended_at'].dt.day\n",
        "trips_df['end_day_name'] = trips_df['ended_at'].dt.day_name()\n",
        "trips_df.head(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6HWUwZfBbaR"
      },
      "source": [
        "\n",
        "---\n",
        "B2\n",
        "---\n",
        "حساب مدة الرحلة\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7eipZArwwpS"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_duration_minutes'] = (trips_df['end_time'] - trips_df['start_time']).dt.total_seconds() / 60\n",
        "trips_df['trip_duration_minutes']=trips_df['trip_duration_minutes'].round(2)\n",
        "trips_df['trip_duration_minutes'].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN-VHZhttU30"
      },
      "source": [
        "**The trip_duration_minutes problem**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ0TSaictSZr"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_duration_minutes'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dugc51rytbJp"
      },
      "source": [
        "*we can clearly see that there is a problem with the tripd_durations, the min is a negative value and that is not right*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltwKRGertgvi"
      },
      "outputs": [],
      "source": [
        "# Show trips with negative or 0 duration\n",
        "invalid_durations = trips_df[trips_df['trip_duration_minutes'] <= 0]\n",
        "print(f\"Invalid rows: {len(invalid_durations)}\")\n",
        "invalid_durations[['ride_id', 'started_at', 'ended_at', 'trip_duration_minutes']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-wTEW_ZtjcF"
      },
      "outputs": [],
      "source": [
        "# Filter only valid trips\n",
        "trips_df = trips_df[trips_df['trip_duration_minutes'] > 0]\n",
        "trips_df['trip_duration_minutes'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MC8yVG5pmu6"
      },
      "source": [
        "\n",
        "---\n",
        "B3\n",
        "---\n",
        "حساب تكلفة الرحلة  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iMUivT3_god"
      },
      "outputs": [],
      "source": [
        "trips_df['member_casual'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy1RkWaJodJF"
      },
      "outputs": [],
      "source": [
        "trips_df['rideable_type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Q2d88_r7xx"
      },
      "outputs": [],
      "source": [
        "# Initialize base cost\n",
        "# Start with 0 cost\n",
        "trips_df['trip_cost'] = 0.0\n",
        "\n",
        "# Define fixed costs\n",
        "trips_df.loc[trips_df['member_casual'] == 'member', 'trip_cost'] = 3.95\n",
        "trips_df.loc[trips_df['member_casual'] == 'casual', 'trip_cost'] = 1.00\n",
        "\n",
        "# Add extra cost for duration\n",
        "# for members :\n",
        "# Create condition for member rides longer than 45 mins\n",
        "cond_member_extra = (trips_df['member_casual'] == 'member') & (trips_df['trip_duration_minutes'] > 45)\n",
        "\n",
        "# Electric bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'electric_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.10\n",
        "\n",
        "# Classic bike extra for members\n",
        "trips_df.loc[cond_member_extra & (trips_df['rideable_type'] == 'classic_bike'), 'trip_cost'] += \\\n",
        "    (trips_df['trip_duration_minutes'] - 45) * 0.05\n",
        "# Electric bike for casuals\n",
        "cond_casual_electric = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'electric_bike')\n",
        "trips_df.loc[cond_casual_electric, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.15\n",
        "\n",
        "# Classic bike for casuals\n",
        "cond_casual_classic = (trips_df['member_casual'] == 'casual') & (trips_df['rideable_type'] == 'classic_bike')\n",
        "trips_df.loc[cond_casual_classic, 'trip_cost'] += trips_df['trip_duration_minutes'] * 0.05\n",
        "# Add Central Business District (CBD) fee\n",
        "# Preparaing your geometry points\n",
        "# Create GeoDataFrame of start points\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "# #  Load CBD Polygon\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=4326)  # Ensures it's in WGS 84\n",
        "\n",
        "\n",
        "# Convert to GeoDataFrames with correct CRS\n",
        "start_gdf = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "end_gdf = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs('EPSG:6933')\n",
        "\n",
        "# Load CBD polygon and project to EPSG:6933\n",
        "# CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "# CBD = CBD.to_crs(epsg=6933)\n",
        "# cbd_polygon = CBD.geometry.unary_union  # Get full boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph9GeYKu2WJl"
      },
      "outputs": [],
      "source": [
        "# Load CBD polygon and project to EPSG:6933\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.unary_union  # Get full boundary\n",
        "CBD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "306f9C9Nuh0x"
      },
      "outputs": [],
      "source": [
        "# Check spatial containment in EPSG:6933\n",
        "trips_df['start_in_cbd'] = start_gdf['start_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "trips_df['end_in_cbd'] = end_gdf['end_point'].apply(lambda point: point.within(cbd_polygon))\n",
        "\n",
        "# Final condition and cost update\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df.loc[trips_df['in_cbd'], 'trip_cost'] += 0.5\n",
        "trips_df['trip_cost'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYgY5Pp2jR5W"
      },
      "outputs": [],
      "source": [
        "trips_df['trip_cost'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck8sSCqNzuXo"
      },
      "source": [
        "*we can see a clear issue in the data ,  and super high values (4.3 mil in the max ) and std is very high (4837.62) , so we must identify this outliers and deal with them*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE-WAd13voMS"
      },
      "outputs": [],
      "source": [
        "# High-cost trips\n",
        "high_cost = trips_df[trips_df['trip_cost'] > 1000].copy()\n",
        "print(high_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n",
        "\n",
        "# Negative-cost trips\n",
        "neg_cost = trips_df[trips_df['trip_cost'] < 0].copy()\n",
        "# try to only print the len\n",
        "print(neg_cost[['ride_id', 'trip_duration_minutes', 'rideable_type', 'member_casual', 'trip_cost']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRq1RuQcwwZT"
      },
      "outputs": [],
      "source": [
        "# Total rows\n",
        "total_rows = len(trips_df)\n",
        "# Define thresholds\n",
        "high_cost_threshold = 10000\n",
        "negative_cost_threshold = 0\n",
        "\n",
        "# Find outliers\n",
        "high_cost_outliers = trips_df[trips_df['trip_cost'] > high_cost_threshold]\n",
        "negative_cost_outliers = trips_df[trips_df['trip_cost'] < negative_cost_threshold]\n",
        "\n",
        "# Count\n",
        "num_high_cost = len(high_cost_outliers)\n",
        "num_negative_cost = len(negative_cost_outliers)\n",
        "total_outliers = num_high_cost + num_negative_cost\n",
        "\n",
        "# Percentages\n",
        "percent_high_cost = (num_high_cost / total_rows) * 100\n",
        "percent_negative_cost = (num_negative_cost / total_rows) * 100\n",
        "percent_total_outliers = (total_outliers / total_rows) * 100\n",
        "\n",
        "print(f\"High cost outliers: {num_high_cost} ({percent_high_cost:.2f}%)\")\n",
        "print(f\"Negative cost outliers: {num_negative_cost} ({percent_negative_cost:.2f}%)\")\n",
        "print(f\"Total outliers: {total_outliers} ({percent_total_outliers:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkFhEsEC0YA0"
      },
      "source": [
        "since they make a very small amount of the data (0.0%) they can be classifed as false data and we can drop them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzmRagLeyj4C"
      },
      "outputs": [],
      "source": [
        "# Drop outliers by reassigning the filtered DataFrame back to df\n",
        "trips_df = trips_df[(trips_df['trip_cost'] <= high_cost_threshold) & (trips_df['trip_cost'] >= negative_cost_threshold)]\n",
        "trips_df['trip_cost'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-inlm6X1UtR"
      },
      "source": [
        "---\n",
        "B4\n",
        "---\n",
        "تصنيف سعة المحطات  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb4fPoBD1PaZ"
      },
      "outputs": [],
      "source": [
        "stations_df['CAPACITY'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THTyaahb2BmM"
      },
      "outputs": [],
      "source": [
        "# Basic histogram using Plotly\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Distribution of Station Capacity')\n",
        "fig.update_layout(xaxis_title='Capacity', yaxis_title='Count', bargap=0.1)\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYG6CLLl1UHj"
      },
      "source": [
        "*Choosing the right threshold*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBzKGS5A4g2K"
      },
      "outputs": [],
      "source": [
        "# Drop NaNs\n",
        "capacity_data = stations_df['CAPACITY'].dropna()\n",
        "# Histogram\n",
        "hist_data = go.Histogram(x=capacity_data, nbinsx=30, name='Histogram', opacity=0.6)\n",
        "# Density Curve\n",
        "kde = gaussian_kde(capacity_data)\n",
        "x_vals = np.linspace(capacity_data.min(), capacity_data.max(), 1000)\n",
        "kde_data = go.Scatter(x=x_vals, y=kde(x_vals) * len(capacity_data) * (x_vals[1] - x_vals[0]),\n",
        "                      mode='lines', name='KDE Curve')\n",
        "\n",
        "# Plot both\n",
        "fig = go.Figure(data=[hist_data, kde_data])\n",
        "fig.update_layout(title='Capacity Distribution with KDE',\n",
        "                  xaxis_title='Capacity', yaxis_title='Count')\n",
        "# Example thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.30)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "print(low_thresh,high_thresh)\n",
        "fig.add_vline(x=low_thresh, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=high_thresh, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8xY7YVb1hxP"
      },
      "source": [
        "method 1 : using quantiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULXZ605f4sqp"
      },
      "outputs": [],
      "source": [
        "# Calculate the thresholds\n",
        "low_thresh = stations_df['CAPACITY'].quantile(0.33)\n",
        "high_thresh = stations_df['CAPACITY'].quantile(0.66)\n",
        "\n",
        "def classify_capacity(cap):\n",
        "    if cap <= low_thresh:\n",
        "        return 'Small'\n",
        "    elif cap <= high_thresh:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "stations_df['STATION_SIZE'].value_counts()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtJ_U9Is1qdi"
      },
      "source": [
        "method 2 : based on domain knowledge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LssiAiXc6shB"
      },
      "outputs": [],
      "source": [
        "def classify_capacity(cap):\n",
        "    if cap <= 15:\n",
        "        return 'Small'\n",
        "    elif cap <= 25:\n",
        "        return 'Average'\n",
        "    else:\n",
        "        return 'Large'\n",
        "\n",
        "stations_df['STATION_SIZE'] = stations_df['CAPACITY'].apply(classify_capacity)\n",
        "stations_df['STATION_SIZE'].value_counts()\n",
        "print(stations_df['STATION_SIZE'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsYLrpKY1vqS"
      },
      "source": [
        "combine with trips df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7vWPiUD19OW"
      },
      "source": [
        "try1 : using the station_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vouMGhoaqdxB"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create a simplified DataFrame for merging\n",
        "station_size_map = stations_df[['STATION_ID', 'STATION_SIZE']].copy()\n",
        "# Step 3: Merge for start_station_size\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_map.rename(columns={\n",
        "        'STATION_ID': 'start_station_id',\n",
        "        'STATION_SIZE': 'start_station_size'\n",
        "    }),\n",
        "    on='start_station_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Step 4: Merge for end_station_size\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_map.rename(columns={\n",
        "        'STATION_ID': 'end_station_id',\n",
        "        'STATION_SIZE': 'end_station_size'\n",
        "    }),\n",
        "    on='end_station_id',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjJnWcdNt128"
      },
      "outputs": [],
      "source": [
        "print(\"Missing start_station_size:\", trips_df['start_station_size'].isna().sum())\n",
        "print(\"Missing end_station_size:\", trips_df['end_station_size'].isna().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAxOiIeqyVaE"
      },
      "outputs": [],
      "source": [
        "# What kind of start_station_id had no match?\n",
        "print(trips_df[trips_df['start_station_size'].isna()][['start_station_id']].drop_duplicates().head(10))\n",
        "\n",
        "# Same for end_station\n",
        "print(trips_df[trips_df['end_station_size'].isna()][['end_station_id']].drop_duplicates().head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lK7oC-15RR"
      },
      "source": [
        "try2 : with names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuNfrlpwyuIk"
      },
      "outputs": [],
      "source": [
        "stations_df['NAME'] = stations_df['NAME'].str.strip().str.lower()\n",
        "trips_df['start_station_name'] = trips_df['start_station_name'].str.strip().str.lower()\n",
        "trips_df['end_station_name'] = trips_df['end_station_name'].str.strip().str.lower()\n",
        "\n",
        "# Map start station size using name\n",
        "station_size_name_map = stations_df[['NAME', 'STATION_SIZE']]\n",
        "\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_name_map.rename(columns={'NAME': 'start_station_name', 'STATION_SIZE': 'start_station_size_name'}),\n",
        "    on='start_station_name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Same for end station\n",
        "trips_df = trips_df.merge(\n",
        "    station_size_name_map.rename(columns={'NAME': 'end_station_name', 'STATION_SIZE': 'end_station_size_name'}),\n",
        "    on='end_station_name',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t904hbWnty9o"
      },
      "outputs": [],
      "source": [
        "trips_df['end_station_size'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyd6ovXSzhG7"
      },
      "outputs": [],
      "source": [
        "trips_df['start_station_size'] = trips_df['start_station_size_name']\n",
        "trips_df['end_station_size'] = trips_df['end_station_size_name']\n",
        "\n",
        "# Then drop the temp columns\n",
        "trips_df.drop(columns=['start_station_size_name', 'end_station_size_name'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbDfN8QEubCh"
      },
      "outputs": [],
      "source": [
        "print(trips_df[['start_station_size', 'end_station_size']].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7RoQ7P92IHU"
      },
      "source": [
        "using names was better but we still have some null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5_IiBmU7Npw"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.histogram(stations_df, x='CAPACITY', nbins=30, title='Station Capacity Distribution')\n",
        "fig.add_vline(x=15, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Small/Average\")\n",
        "fig.add_vline(x=25, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average/Large\")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMBlw701c9vN"
      },
      "source": [
        "---\n",
        "B5\n",
        "---\n",
        "حساب المسافة إلى أقرب محطة نقل عام\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fylKxgKMecgw"
      },
      "outputs": [],
      "source": [
        "Shuttle_Bus_Stops.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIUA0TNYe2RO"
      },
      "outputs": [],
      "source": [
        "Metro_Bus_Stops['BSTP_LAT'].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_buZ1x_CAVFQ"
      },
      "source": [
        "\n",
        "Approaches\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Approach                    | Time Complexity | Vectorized | Fast    |\n",
        "| --------------------------- | --------------- | ---------- | ------- |\n",
        "| Brute Force (Your original) | O(N × M)        | ❌ No       | 🐌 Slow |\n",
        "| BallTree (New)              | O(N log M)      | ✅ Yes      | ⚡ Fast  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_i2yvv2O8QX"
      },
      "source": [
        "Project all your coordinates to EPSG:6933\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEGPg81slsNc"
      },
      "outputs": [],
      "source": [
        "# Create start and end point geometries\n",
        "trips_df['start_point'] = trips_df.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis=1)\n",
        "trips_df['end_point'] = trips_df.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis=1)\n",
        "\n",
        "# Create GeoDataFrames\n",
        "gdf_start = gpd.GeoDataFrame(trips_df, geometry='start_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "gdf_end = gpd.GeoDataFrame(trips_df, geometry='end_point', crs='EPSG:4326').to_crs(epsg=6933)\n",
        "\n",
        "# Add x/y columns\n",
        "trips_df['start_x'] = gdf_start.geometry.x\n",
        "trips_df['start_y'] = gdf_start.geometry.y\n",
        "trips_df['end_x'] = gdf_end.geometry.x\n",
        "trips_df['end_y'] = gdf_end.geometry.y\n",
        "\n",
        "\n",
        "# projecting   metro and shuttle station coordinates:\n",
        "\n",
        "# Convert station lat/lng to projected coordinates\n",
        "def project_coords(coords_list):\n",
        "    gdf = gpd.GeoDataFrame(geometry=[Point(lon, lat) for lat, lon in coords_list], crs='EPSG:4326')\n",
        "    gdf = gdf.to_crs(epsg=6933)\n",
        "    return np.array([(geom.x, geom.y) for geom in gdf.geometry])\n",
        "# coords\n",
        "# Metro stop coordinates\n",
        "metro_coords = Metro_Bus_Stops[['BSTP_LAT', 'BSTP_LON']].dropna().values\n",
        "\n",
        "# Shuttle stop coordinates\n",
        "shuttle_coords = Shuttle_Bus_Stops[['LATITUDE', 'LONGITUDE']].dropna().values\n",
        "\n",
        "metro_coords_projected = project_coords(metro_coords)\n",
        "shuttle_coords_projected = project_coords(shuttle_coords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN48kQYPl5A0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def euclidean_tree_batch(source_df, stop_coords, x_col, y_col, batch_size=10000):\n",
        "    tree = BallTree(stop_coords, metric='euclidean')\n",
        "\n",
        "    distances = []\n",
        "    n = len(source_df)\n",
        "    tqdm.pandas(desc=f\"Computing distances for {x_col}\")\n",
        "\n",
        "    for i in tqdm(range(0, n, batch_size), desc=\"Batch processing\", unit=\"batch\"):\n",
        "        batch = source_df.iloc[i:i+batch_size]\n",
        "        batch_points = batch[[x_col, y_col]].values\n",
        "\n",
        "        dists, _ = tree.query(batch_points, k=1)\n",
        "        distances.extend(dists.flatten().tolist())\n",
        "\n",
        "    return distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvjyWh64l7Qd"
      },
      "outputs": [],
      "source": [
        "# Start → Metro\n",
        "trips_df['start_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End → Metro\n",
        "trips_df['end_nearest_metro_distance'] = euclidean_tree_batch(\n",
        "    trips_df, metro_coords_projected, 'end_x', 'end_y'\n",
        ")\n",
        "\n",
        "# Start → Shuttle\n",
        "trips_df['start_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'start_x', 'start_y'\n",
        ")\n",
        "\n",
        "# End → Shuttle\n",
        "trips_df['end_nearest_shuttle_distance'] = euclidean_tree_batch(\n",
        "    trips_df, shuttle_coords_projected, 'end_x', 'end_y'\n",
        ")\n",
        "\n",
        "# converting to from meters to km (our choice)\n",
        "trips_df['start_nearest_metro_distance'] = trips_df['start_nearest_metro_distance'] / 1000\n",
        "trips_df['end_nearest_metro_distance'] = trips_df['end_nearest_metro_distance'] / 1000\n",
        "trips_df['start_nearest_shuttle_distance'] = trips_df['start_nearest_shuttle_distance'] / 1000\n",
        "trips_df['end_nearest_shuttle_distance'] = trips_df['end_nearest_shuttle_distance'] / 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4bgJlGMAlPE"
      },
      "outputs": [],
      "source": [
        "trips_df[\n",
        "    ['start_nearest_metro_distance',\n",
        "     'end_nearest_metro_distance',\n",
        "     'start_nearest_shuttle_distance',\n",
        "     'end_nearest_shuttle_distance']\n",
        "].describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3_w8Db_YRhh"
      },
      "outputs": [],
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "cols = ['start_nearest_metro_distance', 'end_nearest_metro_distance',\n",
        "        'start_nearest_shuttle_distance', 'end_nearest_shuttle_distance']\n",
        "for col in cols:\n",
        "    fig = go.Figure(\n",
        "        data=[go.Histogram(\n",
        "            x=sampled_df[col],\n",
        "            nbinsx=100,\n",
        "            marker=dict(color='skyblue'),\n",
        "            opacity=0.75\n",
        "        )]\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=col,\n",
        "        xaxis_title=col,\n",
        "        yaxis_title='Count (Log Scale)',\n",
        "        yaxis_type='log',\n",
        "        bargap=0.1,\n",
        "        width=800,\n",
        "        height=400\n",
        "    )\n",
        "    fig.show(config={'staticPlot':True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAFaQhYwnyJ5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Re-define thresholds for clarity\n",
        "start_nearest_metro_distance_thr = 1 # meters\n",
        "end_nearest_metro_distance_thr = 1   # meters\n",
        "start_nearest_shuttle_distance_thr = 9 # meters\n",
        "end_nearest_shuttle_distance_thr = 9 # meters\n",
        "\n",
        "# --- Step 1: Identify \"far from\" trips based on current thresholds ---\n",
        "# Using the corrected end_nearest_shuttle_distance_thr for the end shuttle distance\n",
        "far_metro_start_trips = trips_df[trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr]\n",
        "far_metro_end_trips = trips_df[trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr]\n",
        "far_shuttle_start_trips = trips_df[trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr]\n",
        "far_shuttle_end_trips = trips_df[trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr]\n",
        "\n",
        "# Combine all \"far from transit\" trips for a general map (for demonstration)\n",
        "# Using a logical OR to get any trip that is far from ANY of these points\n",
        "far_from_transit_trips = trips_df[\n",
        "    (trips_df['start_nearest_metro_distance'] > start_nearest_metro_distance_thr) |\n",
        "    (trips_df['end_nearest_metro_distance'] > end_nearest_metro_distance_thr) |\n",
        "    (trips_df['start_nearest_shuttle_distance'] > start_nearest_shuttle_distance_thr) |\n",
        "    (trips_df['end_nearest_shuttle_distance'] > end_nearest_shuttle_distance_thr)\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "print(f\"Number of trips far from metro (start): {len(far_metro_start_trips)}\")\n",
        "print(f\"Number of trips far from metro (end): {len(far_metro_end_trips)}\")\n",
        "print(f\"Number of trips far from shuttle (start): {len(far_shuttle_start_trips)}\")\n",
        "print(f\"Number of trips far from shuttle (end): {len(far_shuttle_end_trips)}\")\n",
        "print(f\"Total unique trips identified as 'far from transit': {len(far_from_transit_trips)}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Create a Folium Map ---\n",
        "\n",
        "# Get the approximate center of Washington D.C. for the map's initial view\n",
        "# You can use the mean of your start/end lat/lngs or a known DC coordinate\n",
        "dc_center_lat = trips_df['start_lat'].mean() # Or a more precise known center for DC\n",
        "dc_center_lng = trips_df['start_lng'].mean() # Or a more precise known center for DC\n",
        "\n",
        "# Create a base map\n",
        "m = folium.Map(location=[dc_center_lat, dc_center_lng], zoom_start=12)\n",
        "\n",
        "# Add markers for start points of trips identified as \"far from transit\"\n",
        "# Due to the large number of potential points (40k), plotting individual markers for all\n",
        "# might be slow or make the map unreadable.\n",
        "# We'll plot a sample or use a MarkerCluster for better performance.\n",
        "# Let's start by plotting a *sample* of these points if far_from_transit_trips is very large,\n",
        "# or use MarkerCluster. For a first look, a small sample is good.\n",
        "\n",
        "# If you have too many points, consider sampling for initial visualization\n",
        "# Or, even better for density visualization, use MarkerCluster or HeatMap (if allowed for density, check project rules)\n",
        "# Since you're using Plotly for charts and Folium for maps, heatmap should be fine.\n",
        "\n",
        "# Let's just add a few to see the logic work, or use MarkerCluster for all:\n",
        "\n",
        "# OPTION A: Plotting a limited sample (good for a quick check if map gets cluttered)\n",
        "# sample_size = 1000 # Adjust as needed\n",
        "# if len(far_from_transit_trips) > sample_size:\n",
        "#     sample_to_plot = far_from_transit_trips.sample(sample_size, random_state=42)\n",
        "# else:\n",
        "#     sample_to_plot = far_from_transit_trips\n",
        "\n",
        "# for idx, row in sample_to_plot.iterrows():\n",
        "#     folium.CircleMarker(\n",
        "#         location=[row['start_lat'], row['start_lng']],\n",
        "#         radius=2, # Small radius\n",
        "#         color='red',\n",
        "#         fill=True,\n",
        "#         fill_color='red',\n",
        "#         fill_opacity=0.6,\n",
        "#         tooltip=f\"Start: {row['start_station_name']} (Far from Transit)\"\n",
        "#     ).add_to(m)\n",
        "\n",
        "# OPTION B: Using MarkerCluster for better visualization of many points\n",
        "from folium.plugins import MarkerCluster\n",
        "\n",
        "marker_cluster_start = MarkerCluster().add_to(m)\n",
        "marker_cluster_end = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add start points\n",
        "for idx, row in far_from_transit_trips.iterrows():\n",
        "    if pd.notnull(row['start_lat']) and pd.notnull(row['start_lng']):\n",
        "        folium.CircleMarker(\n",
        "            location=[row['start_lat'], row['start_lng']],\n",
        "            radius=2,\n",
        "            color='red', # Color for start points\n",
        "            fill=True,\n",
        "            fill_color='red',\n",
        "            fill_opacity=0.6,\n",
        "            tooltip=f\"Start: {row['start_station_name']} (Far from Transit)\"\n",
        "        ).add_to(marker_cluster_start)\n",
        "\n",
        "# Add end points (optional, you might want separate layers or colors if combining)\n",
        "# For now, let's just show start points to avoid overwhelming the map.\n",
        "# If you want to see end points, you could use a different color or a separate MarkerCluster\n",
        "# for idx, row in far_from_transit_trips.iterrows():\n",
        "#     if pd.notnull(row['end_lat']) and pd.notnull(row['end_lng']):\n",
        "#         folium.CircleMarker(\n",
        "#             location=[row['end_lat'], row['end_lng']],\n",
        "#             radius=2,\n",
        "#             color='blue', # Color for end points\n",
        "#             fill=True,\n",
        "#             fill_color='blue',\n",
        "#             fill_opacity=0.6,\n",
        "#             tooltip=f\"End: {row['end_station_name']} (Far from Transit)\"\n",
        "#         ).add_to(marker_cluster_end)\n",
        "\n",
        "\n",
        "# Save the map to an HTML file or display in a Jupyter Notebook\n",
        "# m.save(\"far_from_transit_trips_map.html\")\n",
        "# In a Jupyter/IPython environment, you can also just display `m` directly\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgaKHoAnoORK"
      },
      "outputs": [],
      "source": [
        "# --- Define your new thresholds based on your histogram observations ---\n",
        "# Example new thresholds (YOU WILL REPLACE THESE WITH YOUR OWN INSIGHTS)\n",
        "new_metro_start_thr = 1  # meters (e.g., if you see a clear drop after 1.2km)\n",
        "new_metro_end_thr = 1    # meters\n",
        "new_shuttle_start_thr = 9 # meters (e.g., if you see a drop after 20km)\n",
        "new_shuttle_end_thr = 9 # meters\n",
        "\n",
        "# --- Create the new boolean features ---\n",
        "trips_df['is_far_from_metro_start'] = trips_df['start_nearest_metro_distance'] > new_metro_start_thr\n",
        "trips_df['is_far_from_metro_end'] = trips_df['end_nearest_metro_distance'] > new_metro_end_thr\n",
        "trips_df['is_far_from_shuttle_start'] = trips_df['start_nearest_shuttle_distance'] > new_shuttle_start_thr\n",
        "trips_df['is_far_from_shuttle_end'] = trips_df['end_nearest_shuttle_distance'] > new_shuttle_end_thr\n",
        "\n",
        "# You can also create a combined flag for any \"far from transit\"\n",
        "trips_df['is_far_from_any_transit'] = (\n",
        "    trips_df['is_far_from_metro_start'] |\n",
        "    trips_df['is_far_from_metro_end'] |\n",
        "    trips_df['is_far_from_shuttle_start'] |\n",
        "    trips_df['is_far_from_shuttle_end']\n",
        ")\n",
        "\n",
        "# Verify the counts of the new features\n",
        "print(\"\\nCounts for new 'far from' features:\")\n",
        "print(trips_df[['is_far_from_metro_start', 'is_far_from_metro_end',\n",
        "                'is_far_from_shuttle_start', 'is_far_from_shuttle_end',\n",
        "                'is_far_from_any_transit']].sum())\n",
        "\n",
        "# Display the first few rows with the new columns to confirm\n",
        "print(\"\\nTrips DataFrame with new features:\")\n",
        "print(trips_df[['ride_id', 'start_nearest_metro_distance', 'is_far_from_metro_start',\n",
        "                'start_nearest_shuttle_distance', 'is_far_from_shuttle_start',\n",
        "                'is_far_from_any_transit']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw0kmmiTpELo"
      },
      "outputs": [],
      "source": [
        "trips_df['is_far_from_metro_start'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smMf9ipl0yav"
      },
      "source": [
        "---\n",
        "B6\n",
        "---\n",
        "تحديد ما إذا كانت الرحلة تمر بالمنطقة التجارية المركزية\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lXEKzGY2Lhs"
      },
      "outputs": [],
      "source": [
        "print(trips_df['start_point'].iloc[0], type(trips_df['start_point'].iloc[0]))\n",
        "print(trips_df['end_point'].iloc[0], type(trips_df['end_point'].iloc[0]))\n",
        "print(type(cbd_polygon))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOAsnPfBp6Py"
      },
      "outputs": [],
      "source": [
        "# STEP 0: Make sure the CBD polygon is projected correctly\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "cbd_polygon = CBD.geometry.iloc[0]  # assuming a single polygon\n",
        "# STEP 1: Create a GeoDataFrame from the trip points (start and end)\n",
        "# start_gdf = gpd.GeoDataFrame(trips_df, geometry=trips_df['start_point'], crs=\"EPSG:4326\")\n",
        "# end_gdf   = gpd.GeoDataFrame(trips_df, geometry=trips_df['end_point'], crs=\"EPSG:4326\")\n",
        "\n",
        "# Rebuild the point geometries from lat/lng in EPSG:4326\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['start_lng'], trips_df['start_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "\n",
        "# Project everything to EPSG:6933\n",
        "CBD = CBD.to_crs(epsg=6933)\n",
        "start_gdf = start_gdf.to_crs(epsg=6933)\n",
        "end_gdf = end_gdf.to_crs(epsg=6933)\n",
        "\n",
        "# CBD polygon (in same projection)\n",
        "cbd_polygon = CBD.geometry.unary_union\n",
        "# Check containment\n",
        "trips_df['start_in_cbd'] = start_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "trips_df['end_in_cbd']   = end_gdf['geometry'].apply(lambda pt: cbd_polygon.contains(pt))\n",
        "\n",
        "# Final result\n",
        "trips_df['in_cbd'] = trips_df['start_in_cbd'] | trips_df['end_in_cbd']\n",
        "trips_df['in_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M_uabQt6Bza"
      },
      "source": [
        "---\n",
        "B7\n",
        "---\n",
        "حساب المسافة إلى المنطقة التجارية المركزية (CBD) وتصنيف القرب"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oxDKxPlsm3r"
      },
      "outputs": [],
      "source": [
        "# --- Step 1: Compute the CBD centroid (already in EPSG:6933)\n",
        "cbd_centroid = cbd_polygon.centroid  # geometry in meters (EPSG:6933)\n",
        "\n",
        "# --- Step 2: Recreate end point GeoDataFrame and project to EPSG:6933\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    trips_df,\n",
        "    geometry=gpd.points_from_xy(trips_df['end_lng'], trips_df['end_lat']),\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=6933)\n",
        "\n",
        "# --- Step 3: Compute Euclidean distance in meters\n",
        "trips_df['distance_to_cbd_m'] = end_gdf.geometry.distance(cbd_centroid)\n",
        "\n",
        "# --- Step 4: Set distance to None where start AND end are in the CBD\n",
        "mask = trips_df['start_in_cbd'] & trips_df['end_in_cbd']\n",
        "trips_df.loc[mask, 'distance_to_cbd_m'] = None\n",
        "\n",
        "# --- Step 5: Inspect result\n",
        "trips_df['distance_to_cbd_m'].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szoai97X63Qj"
      },
      "source": [
        "\n",
        "\n",
        "**Threasholding strategies**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wnOOXiY7I-C"
      },
      "source": [
        "kinda of an elbow method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPKO880k6_k8"
      },
      "outputs": [],
      "source": [
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n",
        "# Extract the data\n",
        "data = sampled_df['distance_to_cbd_m'].dropna()\n",
        "\n",
        "# Create histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=100,\n",
        "    name='Histogram',\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75\n",
        ")\n",
        "\n",
        "# Create KDE line (manual since Plotly doesn’t support KDE directly)\n",
        "kde = gaussian_kde(data)\n",
        "x_vals = np.linspace(data.min(), data.max(), 1000)\n",
        "kde_vals = kde(x_vals) * len(data) * (x_vals[1] - x_vals[0])  # scale to match histogram\n",
        "\n",
        "kde_trace = go.Scatter(\n",
        "    x=x_vals,\n",
        "    y=kde_vals,\n",
        "    mode='lines',\n",
        "    name='KDE',\n",
        "    line=dict(color='darkblue')\n",
        ")\n",
        "\n",
        "# Vertical reference lines\n",
        "vline1 = go.Scatter(\n",
        "    x=[2000, 2000],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='2km Threshold',\n",
        "    line=dict(color='red', dash='dash')\n",
        ")\n",
        "\n",
        "vline2 = go.Scatter(\n",
        "    x=[2764, 2764],\n",
        "    y=[0, max(kde_vals)],\n",
        "    mode='lines',\n",
        "    name='Median',\n",
        "    line=dict(color='green', dash='dash')\n",
        ")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=[hist, kde_trace, vline1, vline2])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distance to CBD at End of Trip',\n",
        "    xaxis_title='distance_to_cbd_m',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    legend=dict(x=0.7, y=0.95)\n",
        ")\n",
        "\n",
        "fig.show( config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvBXxgrW7IIf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "i will choose the median beacause looking at the histogram we can see the counts drops\n",
        "\"\"\"\n",
        "threshold = 2764\n",
        "# Apply binary classification\n",
        "trips_df['close_to_cbd'] = trips_df['distance_to_cbd_m'].apply(\n",
        "    lambda d: None if pd.isna(d) else d <= threshold\n",
        ")\n",
        "trips_df['close_to_cbd'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68oomfV78m6-"
      },
      "outputs": [],
      "source": [
        "print(trips_df['close_to_cbd'].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ellq2QJVzAe3"
      },
      "source": [
        "---\n",
        "B8\n",
        "---\n",
        "\n",
        "تجميع المواقع الجغرافية باستخدام الهاش الجغرافي (Geohashing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZVkChHv0YkX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Washington, D.C. is roughly:\n",
        "\n",
        "~16 km (north-south)\n",
        "\n",
        "~13 km (east-west)\n",
        "\n",
        "So, a geohash precision of 5–8 is appropriate.\n",
        "\"\"\"\n",
        "def encode_geohashes(df, lat_col, lon_col, precisions):\n",
        "    for p in precisions:\n",
        "        col_name = f'geohash_p{p}'\n",
        "        df[col_name] = df.apply(lambda row: geohash2.encode(row[lat_col], row[lon_col], p), axis=1)\n",
        "    return df\n",
        "\n",
        "# Try precisions from 5 to 8\n",
        "precisions_to_test = [5, 6, 7, 8]\n",
        "trips_df = encode_geohashes(trips_df, 'start_lat', 'start_lng', precisions_to_test)\n",
        "for p in precisions_to_test:\n",
        "    print(f\"Precision {p}: {trips_df[f'geohash_p{p}'].nunique()} unique regions\")\n",
        "\"\"\"\n",
        "If the number is too small → you're over-aggregating.\n",
        "\n",
        "If it's too big (e.g. thousands) → too fine → hard to summarize meaningfully.\n",
        "\"\"\"\n",
        "\n",
        "for p in precisions_to_test:\n",
        "    counts = trips_df[f'geohash_p{p}'].value_counts()\n",
        "    print(f\"Precision {p} → median trips per geohash: {counts.median()}\")\n",
        "\"\"\"\n",
        "This tells us how balanced the spatial bins are.\n",
        "\n",
        "we ideally want 50–500 trips per cell.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShObmS0z4FJ1"
      },
      "source": [
        "| Precision | Median Trips per Geohash | Interpretation                                                     |\n",
        "| --------- | ------------------------ | ------------------------------------------------------------------ |\n",
        "| **5**     | 1761                     | ⚠️ Too coarse — merges many neighborhoods into one.                |\n",
        "| **6**     | 196                      | ✅ Good balance — each area has enough trips for reliable analysis. |\n",
        "| **7**     | 7                        | ⚠️ Very fine — may be too sparse for most practical summaries.     |\n",
        "| **8**     | 2                        | 🚫 Too sparse — most areas will be noise or empty.                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3yKdEI63oaL"
      },
      "outputs": [],
      "source": [
        "# we will choose 6t\n",
        "trips_df['geohash_sector'] = trips_df['geohash_p6']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRZhifmX5KRC"
      },
      "source": [
        "---\n",
        "B9\n",
        "---\n",
        "تقسيم القطاعات الجغرافية (Geohash Sectors) بناءً على حجم الرحلات اليومية"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpEGkPhx4RQ5"
      },
      "outputs": [],
      "source": [
        "# Group by Sector and Date\n",
        "# Assume you have a 'date' column (convert if needed)\n",
        "trips_df['date'] = pd.to_datetime(trips_df['date'])\n",
        "\n",
        "# Count trips per day per sector\n",
        "daily_counts = trips_df.groupby(['geohash_p6', 'date']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Now compute average daily trips per geohash sector\n",
        "avg_daily_trips = daily_counts.groupby('geohash_p6')['trip_count'].mean().reset_index()\n",
        "avg_daily_trips.rename(columns={'trip_count': 'avg_daily_trips'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctAtFgF5rwM"
      },
      "source": [
        "Choose Segmentation Method (for Red / Yellow / Gray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfhDqYSj5nJs"
      },
      "source": [
        "\n",
        "| Method                         | Description                          | Pros             | Use Case             |\n",
        "| ------------------------------ | ------------------------------------ | ---------------- | -------------------- |\n",
        "| **Quantiles** (e.g., tertiles) | Divide into 3 equal-sized groups     | Simple, fair     | Balanced datasets    |\n",
        "| **KMeans Clustering (k=3)**    | Machine learning-based segmentation  | Optimal grouping | Large datasets       |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g__UfD5W5a6n"
      },
      "outputs": [],
      "source": [
        "# quantiles :\n",
        "# Assign labels based on quantiles\n",
        "quantiles = avg_daily_trips['avg_daily_trips'].quantile([1/3, 2/3])\n",
        "low_thresh = quantiles.iloc[0]\n",
        "high_thresh = quantiles.iloc[1]\n",
        "\n",
        "def classify_volume(val):\n",
        "    if val < low_thresh:\n",
        "        return 'gray'   # Low volume\n",
        "    elif val < high_thresh:\n",
        "        return 'yellow' # Medium volume\n",
        "    else:\n",
        "        return 'red'    # High volume\n",
        "\n",
        "avg_daily_trips['volume_segment'] = avg_daily_trips['avg_daily_trips'].apply(classify_volume)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAYD3Ls1-Rpi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract the data\n",
        "data = avg_daily_trips['avg_daily_trips'].dropna()\n",
        "\n",
        "# Histogram trace\n",
        "hist = go.Histogram(\n",
        "    x=data,\n",
        "    nbinsx=30,\n",
        "    marker_color='lightblue',\n",
        "    opacity=0.75,\n",
        "    name='Avg Daily Trips'\n",
        ")\n",
        "\n",
        "# Vertical threshold lines\n",
        "vline_low = go.Scatter(\n",
        "    x=[low_thresh, low_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='Low Threshold',\n",
        "    line=dict(color='gray', dash='dash')\n",
        ")\n",
        "\n",
        "vline_high = go.Scatter(\n",
        "    x=[high_thresh, high_thresh],\n",
        "    y=[0, data.value_counts().max()],\n",
        "    mode='lines',\n",
        "    name='High Threshold',\n",
        "    line=dict(color='orange', dash='dash')\n",
        ")\n",
        "\n",
        "# Combine into figure\n",
        "fig = go.Figure(data=[hist, vline_low, vline_high])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Distribution of Avg Daily Trips per Geohash Sector',\n",
        "    xaxis_title='Avg Daily Trips',\n",
        "    yaxis_title='Count',\n",
        "    width=800,\n",
        "    height=500,\n",
        "    bargap=0.1\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYWaSyEE569w"
      },
      "outputs": [],
      "source": [
        "X = avg_daily_trips[['avg_daily_trips']].values\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42  , n_init=10).fit(X)\n",
        "avg_daily_trips['kmeans_label'] = kmeans.labels_\n",
        "\n",
        "# Map to red/yellow/gray using sorted cluster means\n",
        "label_map = dict(zip(\n",
        "    np.argsort(kmeans.cluster_centers_.flatten()),\n",
        "    ['gray', 'yellow', 'red']\n",
        "))\n",
        "avg_daily_trips['kmeans_segment'] = avg_daily_trips['kmeans_label'].map(label_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaBBJvFnZ81X"
      },
      "outputs": [],
      "source": [
        "avg_daily_trips.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2epWDpMGakGT"
      },
      "outputs": [],
      "source": [
        "trips_df['geohash_p6'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONedOBCPcTl-"
      },
      "outputs": [],
      "source": [
        "# Merge segments into trips_df\n",
        "trips_df = trips_df.merge(\n",
        "    avg_daily_trips[['geohash_p6','volume_segment','kmeans_segment']],\n",
        "    on='geohash_p6',\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eK2NYQkgpue"
      },
      "outputs": [],
      "source": [
        "trips_df['kmeans_segment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WSRw0iZhLjR"
      },
      "outputs": [],
      "source": [
        "trips_df['volume_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1Nby-lsEf9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "B10\n",
        "----\n",
        "تصنيف الظروف الجوية إلى فئات"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKSxW3W-dWCx"
      },
      "outputs": [],
      "source": [
        "trips_df['conditions'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh1m4kfBfY8I"
      },
      "outputs": [],
      "source": [
        "def classify_weather(condition):\n",
        "    condition = condition.lower()  # lowercase for safety\n",
        "    if 'rain' in condition or 'snow' in condition:\n",
        "        return 'rainy'\n",
        "    elif 'overcast' in condition or 'cloudy' in condition:\n",
        "        return 'cloudy'\n",
        "    elif 'clear' in condition:\n",
        "        return 'sunny'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "# Apply binning\n",
        "trips_df['weather_segment'] = trips_df['conditions'].apply(classify_weather)\n",
        "trips_df['weather_segment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep9rr_N9s9v_"
      },
      "source": [
        "---\n",
        "B11\n",
        "---\n",
        "إنشاء سلاسل زمنية للإيرادات اليومية حسب الظروف الجوية"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cd81R4R4PWY"
      },
      "source": [
        "quick inspection of the data dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_f0gqxJNym1"
      },
      "outputs": [],
      "source": [
        "sorted_ended_at_df = trips_df[['ended_at']].sort_values(by='ended_at')\n",
        "print(\"--- Sorted 'ended_at' DataFrame (first 5 rows) ---\")\n",
        "print(sorted_ended_at_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Find the earliest and latest dates ---\n",
        "earliest_date = sorted_ended_at_df['ended_at'].min()\n",
        "latest_date = sorted_ended_at_df['ended_at'].max()\n",
        "\n",
        "print(f\"The earliest date in 'ended_at' is: {earliest_date}\")\n",
        "print(f\"The latest date in 'ended_at' is: {latest_date}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0A2F38oTZ5x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Step 2: Sort the DataFrame by 'started_at' ---\n",
        "sorted_started_at_df = trips_df[['started_at']].sort_values(by='started_at')\n",
        "print(\"--- Sorted 'started_at' DataFrame (first 5 rows) ---\")\n",
        "print(sorted_started_at_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Step 3: Find the earliest and latest dates using 'started_at' ---\n",
        "earliest_date_started = sorted_started_at_df['started_at'].min()\n",
        "latest_date_started = sorted_started_at_df['started_at'].max()\n",
        "\n",
        "print(f\"The earliest date in 'started_at' is: {earliest_date_started}\")\n",
        "print(f\"The latest date in 'started_at' is: {latest_date_started}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37IY3SeYr1C1"
      },
      "outputs": [],
      "source": [
        "# Make sure 'ended_at' is datetime\n",
        "# trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'])\n",
        "trips_df['ended_at'] = pd.to_datetime(trips_df['ended_at'], format='mixed', errors='coerce')\n",
        "\n",
        "\n",
        "# Extract just the date (without time)\n",
        "trips_df['end_date'] = trips_df['ended_at'].dt.date\n",
        "daily_income_weather = trips_df.groupby(['end_date', 'weather_segment'])['trip_cost'].sum().reset_index()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxfbTLjmtBDz"
      },
      "outputs": [],
      "source": [
        "# convert\n",
        "# Make sure end_date is datetime\n",
        "daily_income_weather['end_date'] = pd.to_datetime(daily_income_weather['end_date'])\n",
        "\n",
        "fig_long = px.line(\n",
        "    daily_income_weather,\n",
        "    x='end_date',\n",
        "    y='trip_cost',\n",
        "    color='weather_segment',\n",
        "    title='Daily Total Trip Cost by Weather Condition (Long Format)',\n",
        "    labels={'end_date': 'Date', 'trip_cost': 'Total Income', 'weather_segment': 'Weather'}\n",
        ")\n",
        "\n",
        "fig_long.update_layout(xaxis_title='Date', yaxis_title='Trip Cost', hovermode='x unified')\n",
        "fig_long.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6WfXRDRtHwQ"
      },
      "outputs": [],
      "source": [
        "# Pivot to wide format\n",
        "wide_df = daily_income_weather.pivot(index='end_date', columns='weather_segment', values='trip_cost').fillna(0)\n",
        "wide_df = wide_df.sort_index()\n",
        "\n",
        "# Build traces\n",
        "fig_wide = go.Figure()\n",
        "\n",
        "for condition in wide_df.columns:\n",
        "    fig_wide.add_trace(go.Scatter(\n",
        "        x=wide_df.index,\n",
        "        y=wide_df[condition],\n",
        "        mode='lines',\n",
        "        name=condition\n",
        "    ))\n",
        "\n",
        "fig_wide.update_layout(\n",
        "    title='Daily Total Trip Cost by Weather Condition (Wide Format)',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Trip Cost',\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white',\n",
        "    legend_title='Weather'\n",
        ")\n",
        "\n",
        "fig_wide.show(config={'staticPlot':True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMDgJxs8OXx_"
      },
      "source": [
        "Which one is better for our problem  ?<br>\n",
        "the Long Format is the most suitable and effective,This is because it allows for direct visual comparison of revenue trends across different weather types over time on a single graph, making it easier to spot patterns and seasonal impacts. The long format is also considered more intuitive for time-series visualization. Conversely, the \"wide format\" is deemed less clear due to potential visual clutter when many categories are present.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBXs3HzQO-wt"
      },
      "source": [
        "---\n",
        "B12\n",
        "---\n",
        "إنشاء سمات مساعدة للتحليل الاستكشافي للبيانات (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruD09zYz5E68"
      },
      "source": [
        "Feature 1 : rush_hour\n",
        "<br> Indicates if the ride occurred during typical commuting hours (7–10 AM or 4–7 PM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "bN5x_9uTt2RV",
        "outputId": "dafbebaa-792f-4c91-c76f-33649fdb794f"
      },
      "outputs": [],
      "source": [
        "\n",
        "trips_df['start_time'] = pd.to_datetime(trips_df['start_time'], errors='coerce')\n",
        "\n",
        "trips_df['rush_hour'] = (\n",
        "    trips_df['start_time'].dt.hour.between(7, 10) |\n",
        "    trips_df['start_time'].dt.hour.between(16, 19)\n",
        ").astype(int)\n",
        "trips_df['rush_hour'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCxcP4_M5MK-"
      },
      "source": [
        "Feature 2 : hour_segment <br>\n",
        "Categorize ride start times into broader buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "kQtxwZZnPxnT",
        "outputId": "83dcd13e-c6a2-4575-bf54-9097336c7c8a"
      },
      "outputs": [],
      "source": [
        "def get_hour_segment(hour):\n",
        "    if 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Midday'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "trips_df['hour_segment'] = trips_df['start_time'].dt.hour.apply(get_hour_segment)\n",
        "trips_df['hour_segment'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGwHHc2n5ThH"
      },
      "source": [
        "Feature 3 : is_weekend<br>\n",
        "Helps spot usage patterns on weekends vs weekdays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ZiVY5Od0QKAe",
        "outputId": "bc6e1867-4c6d-4e81-ad27-3d7f4fcfd88a"
      },
      "outputs": [],
      "source": [
        "trips_df['is_weekend'] = trips_df['start_time'].dt.dayofweek >= 5\n",
        "trips_df['is_weekend'] = trips_df['is_weekend'].astype(int)\n",
        "trips_df['is_weekend'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49oBoftHUUaG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**EDA**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTOhRmjguH-c"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Sampling the data\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABgwYinmuLkR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sampled data stats\n",
        "sampled_df = trips_df.sample(n=20000, random_state=50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxL-4ceXUYhn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# A )\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cwZZ30VTEp_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4GFM4egUk3o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# B)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK_yKo3mUtU4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task 1\n",
        "---\n",
        "رسم مخطط Histogram لتوزيع مدة الرحلة بالدقائق\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_mATqblU0nY"
      },
      "source": [
        "| Method                     | Formula                         | Notes                               |\n",
        "| -------------------------- | ------------------------------- | ----------------------------------- |\n",
        "| **Sturges’ Rule**          | `bins = ceil(log2(n) + 1)`      | Good for small to medium-sized data |\n",
        "| **Freedman–Diaconis Rule** | `bin_width = 2 * IQR / n^(1/3)` | Good for skewed data or outliers    |\n",
        "| **Square Root Rule**       | `bins = sqrt(n)`                | Simple and often a good baseline    |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8HlhefNL3gb"
      },
      "outputs": [],
      "source": [
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedman–Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "# Static histogram\n",
        "fig = go.Figure(\n",
        "    data=[go.Histogram(\n",
        "        x=durations,\n",
        "        nbinsx=bin_count,\n",
        "        marker_color='blue',\n",
        "        opacity=1.0\n",
        "    )]\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Distribution of Trip Duration (in Minutes)\",\n",
        "    xaxis_title=\"Trip Duration (minutes)\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    bargap=0.05,\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG-L58E8ESel"
      },
      "source": [
        "How many trips took more then a day ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88-AjkyzEU6m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Choose your cutoff (in minutes)\n",
        "cutoff = 1440  # Modify as needed\n",
        "\n",
        "# Use the sampled dataframe to avoid memory issues\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "\n",
        "# Freedman–Diaconis rule for bin width\n",
        "q25, q75 = np.percentile(durations, [25, 75])\n",
        "iqr = q75 - q25\n",
        "n = len(durations)\n",
        "bin_width = 2 * iqr / (n ** (1/3))\n",
        "bin_count = int(np.ceil((durations.max() - durations.min()) / bin_width))\n",
        "\n",
        "print(f\"Suggested bin count: {bin_count}\")\n",
        "\n",
        "\n",
        "# Count how many trips exceed the cutoff\n",
        "sampled_exceed = (sampled_df['trip_duration_minutes'] > cutoff).sum()\n",
        "full_exceed = (trips_df['trip_duration_minutes'] > cutoff).sum()\n",
        "\n",
        "print(f\"Trips in sampled_df exceeding {cutoff} minutes: {sampled_exceed}\")\n",
        "print(f\"Trips in trips_df exceeding {cutoff} minutes: {full_exceed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36wNkRedY3sL"
      },
      "source": [
        "insights :\n",
        "1. The massive bar near 0-20 minutes clearly shows that most bike trips are very short. This is typical for bike-sharing systems, often used for short commutes or quick errands.\n",
        "2. The presence of bars, even if very short, extending all the way to 1440 minutes shoes that some trips in the data did take more then a day , the number of trips is 361\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78CKfIqMzVr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task2\n",
        "---\n",
        "\n",
        "رسم مخطط صندوقي  لتوزيع مدة الرحلة حسب نوع الدراجة"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCw3707wXM-6"
      },
      "outputs": [],
      "source": [
        "# Use the original (not divided) trip durations\n",
        "durations = sampled_df['trip_duration_minutes']\n",
        "types = sampled_df['rideable_type']\n",
        "\n",
        "# Build the box plot grouped by rideable_type\n",
        "fig = go.Figure()\n",
        "\n",
        "# Loop through each rideable type and add a box\n",
        "for bike_type in sampled_df['rideable_type'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['rideable_type'] == bike_type]['trip_duration_minutes'],\n",
        "        name=bike_type,\n",
        "        boxpoints='outliers',  # show outliers only\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Rideable Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Rideable Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlLYvhqMbi93"
      },
      "source": [
        "insights :\n",
        "1. both types show a very compact box near 20 minutes, indicating that the vast majority of trips for both bike types are quite short.\n",
        "2. The median line is very close to the bottom of the box, confirming heavy right-skewness, this means almost all of the middle 50% of data is concentrated very close to the lower end, and the remaining data (up to Q3) is more spread out.\n",
        "3. The green dots above the whiskers clearly represent the longer  trips with some of them above the 1440 line\n",
        "4. Electric bikes seem to have a slightly tighter distribution,This suggests that while both have short typical trips, classic bikes might have a slightly wider range of trips durations\n",
        "5. Both bike types exhibit very long duration \"outliers,\" with classic bikes potentially having more extreme longer-duration outliers , It suggests that while electric bikes facilitate shorter, perhaps faster trips, classic bikes are used for the most extended journeys.\n",
        "This could be due to factors like cost , battery limits, or simply user preference ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ5QqIOIM4mz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "رسم مخطط صندوقي  لتوزيع مدة الرحلة حسب نوع العضوية\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1ZEPTa6MwH-"
      },
      "outputs": [],
      "source": [
        "trips_df['member_casual'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpLjgXVjMiwf"
      },
      "outputs": [],
      "source": [
        "urations = sampled_df['trip_duration_minutes']\n",
        "types_to_group_by = sampled_df['member_casual']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for member_type in sampled_df['member_casual'].unique():\n",
        "    fig.add_trace(go.Box(\n",
        "        y=sampled_df[sampled_df['member_casual'] == member_type]['trip_duration_minutes'],\n",
        "        name=member_type,\n",
        "        boxpoints='outliers',\n",
        "        marker_color='green',\n",
        "        line_color='black',\n",
        "        opacity=0.8\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Box Plot of Trip Duration by Member Type\",\n",
        "    yaxis_title=\"Trip Duration (minutes)\",\n",
        "    xaxis_title=\"Member Type\",\n",
        "    template='simple_white'\n",
        ")\n",
        "\n",
        "# Render statically to avoid Colab issues\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MadmH7TDdwd1"
      },
      "source": [
        "insights :\n",
        "1.\tانحراف حاد نحو اليمين لكلا الفئتين: على غرار الرسوم البيانية السابقة لمدة الرحلة، يُظهر مخطط الصندوق لكلا فئتي \"المستخدمين العاديين\" (casual) و \"الأعضاء\" (member) انحرافًا قوياً للغاية نحو اليمين, صندوق المخطط لكلتا الفئتين مضغوط للغاية ويقع في الجزء السفلي من الرسم (قريبًا من 0 دقيقة)، ويقع خط الوسيط عملياً فوق الربع الأول (Q1) , هذا يؤكد أن الغالبية العظمى من الرحلات لكلا النوعين من المستخدمين قصيرة جداً.\n",
        "2. \tمدة رحلة أطول قليلاً للمستخدمين العاديين: على الرغم من أن مدة الرحلات النموذجية قصيرة لكلا الفئتين، إلا أن الوسيط (والربيع الأول/الثالث) للمستخدمين العاديين يبدو أعلى قليلاً (أو على الأقل، الصندوق أقل انضغاطاً بشكل هامشي) من تلك الخاصة بالأعضاء, هذا يشير إلى أن الرحلة \"النموذجية\" للمستخدم العابر أطول قليلاً من رحلة العضو، حتى لو كانت كلتاهما قصيرتين نسبياً.\n",
        "3. \tوجود قيم متطرفة للرحلات الطويلة لكلا الفئتين:  تظهر كلتا الفئتين بوضوح عدداً كبيراً من الرحلات \"المتطرفة\" (النقاط الخضراء) التي تمتد إلى ما هو أبعد بكثير من الصندوق والشعيرات الرئيسية، مما يشير إلى أن الرحلات الطويلة جداً تحدث لكلا النوعين من المستخدمين.\n",
        "4. \tمدى أوسع للرحلات النموذجية للمستخدمين العاديين: يبدو أن صندوق  )المدى الربيعي (IQR -  للمستخدمين العاديين أوسع بشكل هامشي من ذلك الخاص بالأعضاء, كما أن الشعيرات للمستخدمين العاديين تبدو أطول قليلاً هذا يعني أن المستخدمين العاديين يظهرون مدى أكبر في مدة الرحلات النموذجية مقارنة بالأعضاء.\n",
        "5.\tكثافة أعلى للرحلات المتطرفة الطويلة للمستخدمين العاديين: يمتلك المستخدمون العاديين كثافة أعلى بكثير من الرحلات المتطرفة الطويلة جداً, بينما يمتلك الأعضاء أيضاً رحلات متطرفة طويلة، إلا أنها أقل عدداً وبشكل عام لا تصل إلى نفس المدد القصوى التي يصل إليها المستخدمون العاديين, هذا يشير إلى أن المستخدمين العاديين قد يستخدمون الدراجات لرحلات أطول بشكل متقطع، ربما لأغراض ترفيهية أو سياحية، في حين أن الأعضاء يفضلون الرحلات القصيرة والمنتظمة التي تتناسب مع الاشتراكات.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs0wC5GyObKO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "تعيين الرحلات التي تجاوزا يوم كامل\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFx42HGM6BK"
      },
      "outputs": [],
      "source": [
        "# Counting Trips Longer Than One Day\n",
        "# Define threshold: 1 day = 1440 minutes\n",
        "one_day_minutes = 1440\n",
        "# Filter trips longer than 1 day\n",
        "long_trips_df = trips_df[trips_df['trip_duration_minutes'] > one_day_minutes]\n",
        "long_sampled_df = sampled_df[sampled_df['trip_duration_minutes'] > one_day_minutes]\n",
        "# Show how many there are\n",
        "print(f\"Total number of trips longer than 1 day in full data: {len(long_trips_df)}\")\n",
        "print(f\"Total number of trips longer than 1 day in sampled data: {len(long_sampled_df)}\")\n",
        "# Combine start and end station counts for long trips\n",
        "start_counts = long_trips_df['start_station_id'].value_counts()\n",
        "end_counts = long_trips_df['end_station_id'].value_counts()\n",
        "\n",
        "# Combine them into a single Series\n",
        "total_counts = start_counts.add(end_counts, fill_value=0).astype(int)\n",
        "\n",
        "# Get station info: name and location\n",
        "stations = sampled_df[['start_station_id', 'start_station_name', 'start_lat', 'start_lng']].drop_duplicates()\n",
        "stations = stations.rename(columns={\n",
        "    'start_station_id': 'station_id',\n",
        "    'start_station_name': 'station_name',\n",
        "    'start_lat': 'lat',\n",
        "    'start_lng': 'lng'\n",
        "})\n",
        "\n",
        "# Merge with counts\n",
        "stations['long_trip_count'] = stations['station_id'].map(total_counts).fillna(0).astype(int)\n",
        "\n",
        "# Filter stations with at least 1 long trip\n",
        "stations = stations[stations['long_trip_count'] > 0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVJdaV5bO0sH"
      },
      "outputs": [],
      "source": [
        "# Center the map on Washington DC\n",
        "m = folium.Map(location=[38.9072, -77.0369], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Optional: cluster points\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "\n",
        "# Add stations to the map\n",
        "for _, row in stations.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['lat'], row['lng']],\n",
        "        radius=3 + row['long_trip_count']**0.5,  # scale marker size\n",
        "        color='darkred',\n",
        "        fill=True,\n",
        "        fill_color='crimson',\n",
        "        fill_opacity=0.7,\n",
        "        popup=f\"{row['station_name']}<br>Trips > 1 day: {row['long_trip_count']}\"\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "# Show the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sab-rkVxVoJ"
      },
      "source": [
        "insights :\n",
        "1.  primary observation is that stations associated with long-duration trips are particularly concentrated around the central part WDC map ,This suggests that while very long trips are rare overall, they are not uniformly distributed but rather originate from or end in specific zones.\n",
        "2. The 2496 is the count of unique station IDs that appear as either a start_station_id OR an end_station_id within those 361 trips.\n",
        "This is a significant number of stations involved, considering that the total number of such trips was only 361. This means these long trips are spread across a wide variety of stations, rather than being concentrated at just a few specific locations.\n",
        "3. This highlights areas where the system might need to adapt to different user behaviors like offering specific \"long-term rental\" options or different pricing for these stations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehZahS1cJPcY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# C)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOP_nkSnJTVM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# D)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eWjBMMLQz1C"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task1\n",
        "---\n",
        "عرض مخطط حراري جغرافي لعدد الرحلات\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnB_ydw33K-z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- (Your existing data preparation for res_points) ---\n",
        "# Assuming 'sampled_df' and 'Residential_Visitor_Parking_Zones' are loaded\n",
        "\n",
        "# Step 0: Load residential zones GeoDataFrame\n",
        "res_zones = Residential_Visitor_Parking_Zones\n",
        "res_zones = res_zones.to_crs(epsg=4326)\n",
        "\n",
        "# Step 1: Create GeoDataFrames for start and end points (using sampled_df)\n",
        "start_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['start_lng'], sampled_df['start_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "end_gdf = gpd.GeoDataFrame(\n",
        "    sampled_df,\n",
        "    geometry=gpd.points_from_xy(sampled_df['end_lng'], sampled_df['end_lat']),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "# Step 2: Spatial join to check which points fall inside residential zones\n",
        "start_in_res = gpd.sjoin(start_gdf, res_zones, predicate='within', how='inner')\n",
        "end_in_res = gpd.sjoin(end_gdf, res_zones, predicate='within', how='inner')\n",
        "\n",
        "# Step 3: Extract lat/lon of trips touching residential zones\n",
        "res_start_points = start_in_res[['start_lat', 'start_lng']].rename(columns={'start_lat': 'lat', 'start_lng': 'lon'})\n",
        "res_end_points = end_in_res[['end_lat', 'end_lng']].rename(columns={'end_lat': 'lat', 'end_lng': 'lon'})\n",
        "res_points = pd.concat([res_start_points, res_end_points], ignore_index=True)\n",
        "\n",
        "# --- Load CBD Data and ensure correct CRS (EPSG:4326) ---\n",
        "CBD = gpd.read_file('Homework/data/DDOT_Central_Business_District.geojson')\n",
        "CBD = CBD.to_crs(epsg=4326) # Ensure CBD is also in EPSG:4326\n",
        "\n",
        "# --- Create the Plotly Graph Object Figure ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the Heatmap Trace\n",
        "fig.add_trace(go.Densitymapbox(\n",
        "    lat=res_points['lat'],\n",
        "    lon=res_points['lon'],\n",
        "    radius=10, # Adjust radius as needed for visual effect\n",
        "    colorscale=\"Viridis\", # Or \"Jet\", \"Hot\", \"Portland\", etc. for heatmap colors\n",
        "    hoverinfo=\"skip\"\n",
        "))\n",
        "\n",
        "# Add Residential Zones as a GeoJSON layer\n",
        "fig.update_layout(\n",
        "    mapbox_layers=[\n",
        "        {\n",
        "            \"below\": 'traces',\n",
        "            \"sourcetype\": \"geojson\",\n",
        "            \"source\": json.loads(res_zones.to_json()),\n",
        "            \"type\": \"line\",\n",
        "            # \"line\": {\"width\": 1, \"color\": \"blue\"} # Corrected: used \"color\" within \"line\" object\n",
        "        },\n",
        "        # Add CBD as another GeoJSON layer\n",
        "        {\n",
        "            \"below\": 'traces',\n",
        "            \"sourcetype\": \"geojson\",\n",
        "            \"source\": json.loads(CBD.to_json()),\n",
        "            \"type\": \"line\",\n",
        "            # \"line\": {\"width\": 2, \"color\": \"green\"} # Corrected: used \"color\" within \"line\" object\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Update layout for map style, center, zoom, and title\n",
        "fig.update_layout(\n",
        "    title_text='Geographic Heatmap of Trips to Residential Zones with Borders',\n",
        "    mapbox_style=\"carto-positron\",\n",
        "    mapbox_zoom=10,\n",
        "    mapbox_center = {\"lat\": res_points['lat'].mean(), \"lon\": res_points['lon'].mean()},\n",
        "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
        ")\n",
        "\n",
        "# Render statically (Plotly handles this directly when fig.show() is called in a notebook)\n",
        "fig.show(config={'staticPlot': True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdhQF2Fi7lup"
      },
      "source": [
        "insights :\n",
        "1. The most prominent insight is the very high density (bright yellow areas) of bike trips that overwhelmingly coincides with and largely respects the boundaries of the residential zones.\n",
        " This indicates that the bike-sharing system is heavily used for purposes directly related to residential areas, whether starting from, ending in, or traversing through them.\n",
        "2. The highest concentration of trips within residential zones appears to be in the central and northern parts of Washington D.C., This suggests these are the most active residential areas for bike-sharing usage.\n",
        "3. While the overall pattern is dense,These could indicate areas with fewer stations, different demographic profiles, or less need for bike-sharing, which could be further explored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oZTM_ZuT-zD"
      },
      "source": [
        "---\n",
        "Task2\n",
        "---\n",
        "عرض مخطط شريطي لتوزع فئات القطاعات الجغرافية (Geohash Sectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY_LY76_J5iH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Count trips per geohash sector\n",
        "geohash_counts = sampled_df['geohash_p6'].value_counts().reset_index()\n",
        "geohash_counts.columns = ['geohash_p6', 'trip_count']\n",
        "\n",
        "# Optional: sort alphabetically or by count\n",
        "geohash_counts = geohash_counts.sort_values(by='trip_count', ascending=False)\n",
        "\n",
        "# Step 2: Plot\n",
        "fig = px.bar(\n",
        "    geohash_counts,\n",
        "    x='geohash_p6',\n",
        "    y='trip_count',\n",
        "    title='Distribution of Trips by Geographic Sector (Geohash_p6)',\n",
        "    labels={'geohash_p6': 'Geographic Sector', 'trip_count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "# Step 3: Turn off interactivity\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEbLkSru_yc6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- Your Function to Convert Geohash to Polygon ---\n",
        "def geohash_to_polygon(geohash_str):\n",
        "    \"\"\"\n",
        "    Converts a geohash string to a shapely Polygon representing its bounding box.\n",
        "    \"\"\"\n",
        "    lat, lon, lat_err, lon_err = gh.decode_exactly(geohash_str)\n",
        "    min_lat = lat - lat_err\n",
        "    max_lat = lat + lat_err\n",
        "    min_lon = lon - lon_err\n",
        "    max_lon = lon + lon_err\n",
        "    coords = [\n",
        "        (min_lon, min_lat),\n",
        "        (max_lon, min_lat),\n",
        "        (max_lon, max_lat),\n",
        "        (min_lon, max_lat),\n",
        "        (min_lon, min_lat)  # Close the polygon\n",
        "    ]\n",
        "    return Polygon(coords)\n",
        "\n",
        "# --- Sample Input: Top 5 Geohashes (replace with your real data) ---\n",
        "top_5_geohashes = geohash_counts.head(5)['geohash_p6'].tolist()\n",
        "geohash_polygons = [geohash_to_polygon(g) for g in top_5_geohashes]\n",
        "\n",
        "# --- Convert to GeoDataFrame for easier plotting ---\n",
        "gdf = gpd.GeoDataFrame(geometry=geohash_polygons, crs='EPSG:4326')\n",
        "\n",
        "# --- Plot using Plotly ---\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add each polygon as a filled scattermapbox trace\n",
        "for poly in gdf.geometry:\n",
        "    lons, lats = poly.exterior.coords.xy\n",
        "    fig.add_trace(go.Scattermapbox(\n",
        "        lon=list(lons),\n",
        "        lat=list(lats),\n",
        "\n",
        "        mode='lines',\n",
        "        fill='toself',\n",
        "        fillcolor='rgba(255, 0, 0, 0.4)',  # Semi-transparent red\n",
        "        line=dict(width=1, color='red'),\n",
        "        hoverinfo='skip',\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "# Map settings\n",
        "fig.update_layout(\n",
        "    mapbox_style='carto-positron',\n",
        "    mapbox_zoom=12,\n",
        "    mapbox_center={\"lat\": gdf.geometry.centroid.y.mean(), \"lon\": gdf.geometry.centroid.x.mean()},\n",
        "    margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\n",
        ")\n",
        "\n",
        "# Static rendering (no interactions)\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE98VnXv-ww7"
      },
      "source": [
        "insights :\n",
        "1.  The first few bars on the left are significantly taller than the rest, with the tallest bar approaching 800 trips, while many others have less than 50. This indicates that bike-sharing activity is heavily concentrated in a few high-demand areas.\n",
        "2. the plot allows for quick identification of the highest-demand geographic sectors. These are likely to correspond to areas with high population density, major transit hubs, popular attractions, or dense commercial/office districts\n",
        "3. we ploted these top sectors and we compared it the heatmap in task1 , and we are now sure that they are in hotspots for trips in residential zone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ZXZV3XVa7n"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task3\n",
        "---\n",
        "عرض مخططات Histogram لتوزيع المسافة عن المنطقة التجارية الرئيسية (CBD)، وأقرب محطة مترو، وأقرب محطة حافلات\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbmejUVrdzbO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def freedman_diaconis_bins(data):\n",
        "    data = data.dropna()\n",
        "    q75, q25 = np.percentile(data, [75 ,25])\n",
        "    iqr = q75 - q25\n",
        "    n = len(data)\n",
        "    if n == 0 or iqr == 0:\n",
        "        return 10  # fallback if data is empty or IQR is zero\n",
        "    bin_width = 2 * iqr / (n ** (1/3))\n",
        "    if bin_width == 0:\n",
        "        return 10\n",
        "    bin_count = int(np.ceil((data.max() - data.min()) / bin_width))\n",
        "    print(\"Bin count : \", bin_count)\n",
        "    return bin_count\n",
        "\n",
        "\n",
        "# Calculate bins for each variable\n",
        "bins_cbd = freedman_diaconis_bins(sampled_df['distance_to_cbd_m'])\n",
        "bins_metro = freedman_diaconis_bins(sampled_df['start_nearest_metro_distance'])\n",
        "bins_shuttle = freedman_diaconis_bins(sampled_df['start_nearest_shuttle_distance'])\n",
        "\n",
        "sampled_df['distance_to_cbd_km'] = sampled_df['distance_to_cbd_m'] / 1000\n",
        "\n",
        "# 1. Distance to CBD\n",
        "fig1 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='distance_to_cbd_km',\n",
        "    nbins=bins_cbd,\n",
        "    title='Distribution of Distance to CBD (km)',\n",
        "    labels={'distance_to_cbd_km': 'Distance to CBD (km)'}\n",
        ")\n",
        "fig1.show(config={'staticPlot': True})\n",
        "\n",
        "# 2. Closest Metro Station Distance\n",
        "fig2 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_metro_distance',\n",
        "    nbins=bins_metro,\n",
        "    title='Distribution of Distance to Nearest Metro Station',\n",
        "    labels={'start_nearest_metro_distance': 'Distance to Metro (km)'}\n",
        ")\n",
        "fig2.show(config={'staticPlot': True})\n",
        "\n",
        "# 3. Closest Shuttle Station Distance\n",
        "fig3 = px.histogram(\n",
        "    sampled_df,\n",
        "    x='start_nearest_shuttle_distance',\n",
        "    nbins=bins_shuttle,\n",
        "    title='Distribution of Distance to Nearest Shuttle Station',\n",
        "    labels={'start_nearest_shuttle_distance': 'Distance to Shuttle (km)'}\n",
        ")\n",
        "fig3.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B_a704MjIGH"
      },
      "source": [
        "insights :\n",
        "1. The overwhelming majority of trips (as indicated by the highest bars) occur within approximately 0 to 4 kilometers of the Central Business District. The peak frequency appears to be around 2.2 kilometers, This reinforces the CBD's role as a major hub for bike-sharing activity.\n",
        "2. this strongly supports the idea that the bike-sharing system primarily serves \"last-mile\" transportation, short-distance commutes, or intra-CBD movement.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. The histogram displays a highly skewed distribution, with an extremely high frequency of trips originating or ending very close to Metro stations. The counts drop off sharply as the distance increases.\n",
        "2. This insight highlights the importance of placing bike-sharing stations in very close proximity to Metro entrances to maximize their integration with the public transit network and serve commuter needs effectively.\n",
        "\n",
        "\n",
        "---\n",
        "1.  Similar to the Metro station plot, this histogram shows a highly skewed distribution, with the highest frequency of trips occurring very close to shuttle stations.\n",
        "2. This suggests a consistent user behavior of using bike-sharing to bridge short gaps to fixed public transport points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zxf96lZssHE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task4\n",
        "---\n",
        "عرض مخطط شريطي لتوزيع الرحلات بناءً على ما إذا كانت داخل المنطقة التجارية المركزية (CBD) بالكامل أم خارجها\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63xLkRdqrujr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Categorize trips\n",
        "def classify_trip(row):\n",
        "    if row['start_in_cbd'] == 1 and row['end_in_cbd'] == 1:\n",
        "        return 'Fully in CBD'\n",
        "    else:\n",
        "        return 'Outside CBD'\n",
        "\n",
        "# Apply classification\n",
        "sampled_df['cbd_trip_type'] = sampled_df.apply(classify_trip, axis=1)\n",
        "\n",
        "# Count\n",
        "trip_cbd_counts = sampled_df['cbd_trip_type'].value_counts().reset_index()\n",
        "trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    trip_cbd_counts,\n",
        "    x='Trip Type',\n",
        "    y='Count',\n",
        "    title='Trips Fully in CBD vs Outside',\n",
        "    text='Count',\n",
        "    labels={'Count': 'Number of Trips'}\n",
        ")\n",
        "\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(yaxis_title='Number of Trips', xaxis_title='Trip Category')\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoWwMMOttvP4"
      },
      "outputs": [],
      "source": [
        "trips_df['cbd_trip_type'] = trips_df.apply(classify_trip, axis=1)\n",
        "full_trip_cbd_counts = trips_df['cbd_trip_type'].value_counts().reset_index()\n",
        "full_trip_cbd_counts.columns = ['Trip Type', 'Count']\n",
        "full_trip_cbd_counts['Percentage'] = (full_trip_cbd_counts['Count'] / full_trip_cbd_counts['Count'].sum()) * 100\n",
        "full_trip_cbd_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEClq4S-lzUe"
      },
      "source": [
        "insights :\n",
        "1. The vast majority of trips  fall into the \"Outside CBD\" category, This indicates that the bike-sharing system primarily serves a broader geographical area beyond just the core CBD, or facilitates connections into/out of it rather than exclusively within it.\n",
        "3.  A significantly smaller number of trips (1,243 trips) were classified as \"Fully in CBD\". This suggests that while there is some intra-CBD movement, it represents a much smaller proportion\n",
        "4. This distribution implies that the bike-sharing service's primary function might not be solely for quick hops within the CBD itself, but rather for facilitating commutes, errands, or leisure travel that connects to or traverses the CBD from other parts of the city. This aligns with the \"first/last mile\" insights from the distance histograms, where trips are often connecting to major transit points or residential areas that might lie outside the strict CBD boundaries.\n",
        "5.  The fact that over 93% of all the trips involve areas outside the CBD strongly reinforces the idea that bike-sharing is primarily used as a \"first/last mile\" solution, connecting users to or from transit hubs (like Metro and Shuttle stations) or residential areas that often lie outside the strict CBD boundaries. These trips are unlikely to be exclusively within the CBD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbWLXpMNuCVn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task5\n",
        "---\n",
        "•\tعرض مخطط شريطي لتوزيع الرحلات التي مرت عبر المنطقة التجارية المركزية (CBD) حسب نوع الدراجة ونوع العضوية\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re3aHjZmtmpv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter trips that passed through CBD\n",
        "cbd_passed_df = sampled_df[\n",
        "    (sampled_df['start_in_cbd'] == 1) | (sampled_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "# Plot\n",
        "fig = px.bar(\n",
        "    grouped,\n",
        "    x='rideable_type',\n",
        "    y='trip_count',\n",
        "    color='member_casual',\n",
        "    barmode='group',\n",
        "    title='Trips That Passed Through CBD by Rideable Type and Membership',\n",
        "    labels={'trip_count': 'Number of Trips', 'rideable_type': 'Bike Type'}\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Rideable Type',\n",
        "    yaxis_title='Number of Trips'\n",
        ")\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE6Ah0JBuxCX"
      },
      "outputs": [],
      "source": [
        "cbd_passed_df_trips_df=trips_df[\n",
        "    (trips_df['start_in_cbd'] == 1) | (trips_df['end_in_cbd'] == 1)\n",
        "]\n",
        "\n",
        "# Group by rideable_type and member_casual\n",
        "grouped = cbd_passed_df_trips_df.groupby(['rideable_type', 'member_casual']).size().reset_index(name='trip_count')\n",
        "\n",
        "print(f\"Length of cbd_passed_df_trips_df: {len(cbd_passed_df_trips_df)}\")\n",
        "print(f\"Length of trips_df: {len(trips_df)}\")\n",
        "\n",
        "percentage = (len(cbd_passed_df_trips_df) / len(trips_df)) * 100\n",
        "print(f\"Percentage of cbd_passed_df_trips_df compared to trips_df: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH3oXctYnxyk"
      },
      "source": [
        "insights :\n",
        "1. the extra exploration shows that 29.85% of all bike-sharing trips pass through the CBD, This is a substantial portion, indicating the CBD's critical role as an origin, destination, or transit point for a large segment of bike-sharing users.\n",
        "2. For both classic_bike and electric_bike categories, members (orange/red bars) consistently account for a significantly higher number of trips that pass through the CBD compared to casual riders (blue bars). This suggests that regular commuters or frequent users (members) are more likely to utilize bike-sharing for travel involving the city's central business district.\n",
        "3. Both casual and member riders use classic_bike more frequently for CBD-involved trips than electric_bike\n",
        "4. Since members are the primary users for CBD-involved trips, strategies to retain and grow membership, and ensure sufficient bike availability in and around the CBD, are crucial.\n",
        "5. The demand for both classic and electric bikes for CBD-involved trips means that fleet management should consider a balanced distribution to meet the preferences of both member and casual riders.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUGoOn14yROg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Task6\n",
        "---\n",
        "دراسة الارتباط بين فئة البعد عن المنطقة التجارية الرئيسية (CBD) ونوع العضوية باستخدام تحليل  Chi-Square\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmcXeVxLv7yx"
      },
      "outputs": [],
      "source": [
        "# Create a contingency table\n",
        "# (Counts of each combination)\n",
        "# Make sure we’re using categorical data\n",
        "contingency_table = pd.crosstab(trips_df['close_to_cbd'], trips_df['member_casual'])\n",
        "contingency_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00075kXbydNE"
      },
      "outputs": [],
      "source": [
        "# Run chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "print(\"Chi2 Statistic:\", chi2)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"P-value:\", p)\n",
        "# interpretion based on the p-value:\n",
        "if p < 0.05:\n",
        "    print(\" There is a significant correlation between distance to CBD segments and membership type.\")\n",
        "else:\n",
        "    print(\" No significant correlation found between distance to CBD segments and membership type.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIDvh1wlz_gr"
      },
      "source": [
        "| α Value  | Interpretation                                                                |\n",
        "| -------- | ----------------------------------------------------------------------------- |\n",
        "| **0.05** | Most common — means you're willing to accept a 5% chance of a false positive. |\n",
        "| 0.01     | Stricter — used in more critical fields (medicine, etc.).                     |\n",
        "| 0.10     | Looser — sometimes used in exploratory research.                              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0oXDOroztOm"
      },
      "source": [
        "insights:\n",
        "1. The p-value of 0.0, which is much less than the conventional significance level of 0.05 leads us to believe there is a statistically significant relationship between whether a trip is close to the CBD and the user's membership type.\n",
        "2. While the Chi-square test indicates strong statistical significance (due to the very large sample size), the proportional difference between casual and member trips being close_to_cbd is relatively small,This suggests thatBoth member and casual riders have roughly half their trips originating or ending close to the CBD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhUkV6rYuQpv"
      },
      "source": [
        "# Secret mission\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o6DO1xSyC5G"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Knowledge Discovery & Storytelling**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc8aqNzByLDh"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "1. How do peak hours and days (rush hour, day of week, weekend) vary for different rideable_type and member_casual groups?\n",
        "<br>\n",
        "Answer here :\n",
        "2. Are there distinct usage patterns for electric_bike vs. classic_bike during different hour_segment and start_day_name, and does this differ between member_casual types?\n",
        "<br>\n",
        "Answer here :\n",
        "3. How do specific weather_segment impact trip_duration_minutes and trip_cost for different rideable_type?\n",
        "<br>\n",
        "Answer here :\n",
        "4. Do start_station_name and end_station_name popularity, or overall trip density (geohash_sector), show significant differences between is_weekend and weekdays?\n",
        "<br>\n",
        "Answer here :\n",
        "5. Which stations act as major transfer hubs (high start_station_name and end_station_name volume)? Do they primarily serve specific member_casual types or rideable_type?\n",
        "<br>\n",
        "Answer here :\n",
        "6. Can we identify common \"commute routes\" or \"regular paths\" for users based on frequent start_station_name and end_station_name pairs?\n",
        "<br>\n",
        "Answer here :\n",
        "7. Given the April revenue issue, can we pinpoint a potential cause by analyzing weather_segment, changes in member_casual behavior, or rideable_type availability/usage during that specific month?\n",
        "<br>\n",
        "Answer here :\n",
        "8. Is there any correlation between a station's volume_segment and its performance during rush_hour or is_weekend, and does this performance vary by weather_segment?\n",
        "<br>\n",
        "Answer here :\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgmqv9vPuVKg"
      },
      "outputs": [],
      "source": [
        "trips_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMVUH2Se0uNA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "َQ1\n",
        "---\n",
        "How do peak hours and days (rush hour, day of week, weekend) vary for different rideable_type and member_casual groups?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRRmS4bq2B7a"
      },
      "outputs": [],
      "source": [
        "def aggregate_trip_counts(df, group_col):\n",
        "    return (\n",
        "        df\n",
        "        .groupby(['member_casual', 'rideable_type', group_col])\n",
        "        .size()\n",
        "        .reset_index(name='ride_count')\n",
        "    )\n",
        "\n",
        "def plot_static_trip_bar(df, x_col, title, x_label):\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x=x_col,\n",
        "        y='ride_count',\n",
        "        color='rideable_type',\n",
        "        barmode='group',\n",
        "        facet_col='member_casual',\n",
        "        title=title,\n",
        "        labels={x_col: x_label, 'ride_count': 'Number of Rides'}\n",
        "    )\n",
        "    fig.update_layout(showlegend=True)\n",
        "    fig.show(config={'staticPlot': True})  # Static rendering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTouPzoJ2zO_"
      },
      "outputs": [],
      "source": [
        "# for rush hour\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_trip_counts, group_col='rush_hour')\n",
        "    .pipe(plot_static_trip_bar, x_col='rush_hour', title='Rush Hour Usage by Type & Member', x_label='Rush Hour (1=True, 0=False)')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cLdbGNe4Bgf"
      },
      "source": [
        "____________\n",
        "casual  \n",
        "1. Significantly fewer rides during rush hour\n",
        "2.  during non-rush hour, electric bikes are slightly preferred over classic bikes.\n",
        "\n",
        "members\n",
        "1. Substantially higher number of rides during non-rush hour\n",
        "2. Electric bikes are also preferred during non-rush hour.\n",
        "_______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xmT8T-N21Ha"
      },
      "outputs": [],
      "source": [
        "#  for start day name\n",
        "trips_df['start_day_name'] = pd.Categorical(\n",
        "    trips_df['start_day_name'],\n",
        "    categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_trip_counts, group_col='start_day_name')\n",
        "    .sort_values('start_day_name')\n",
        "    .pipe(plot_static_trip_bar, x_col='start_day_name', title='daily Usage Patterns by Type & Member', x_label='Day of the Week')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pZTfTf_5JRR"
      },
      "source": [
        "____________\n",
        "casual  \n",
        "1. Usage gradually increases throughout the weekdays, peaking on weekends (saturday and sunday), and then slightly higher on weekends (Saturday being highest).\n",
        "2.  Electric bikes are consistently more popular than classic bikes across all days of the week.\n",
        "\n",
        "members\n",
        "1. Usage is highest during weekdays (Monday to Friday), indicating regular commuting patterns.\n",
        "2. Usage drops significantly on weekends (Saturday and Sunday), which is the opposite of the casual.\n",
        "3. Electric bikes are consistently and significantly more popular than classic ,On weekends, electric bike usage remains higher but the gap between electric and classic bikes narrows compared to weekdays.\n",
        "\n",
        "\n",
        "_______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiwcBObA25cV"
      },
      "outputs": [],
      "source": [
        "# for  is weekend\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_trip_counts, group_col='is_weekend')\n",
        "    .pipe(plot_static_trip_bar, x_col='is_weekend', title='Weekend vs Weekday Rides by Type & Member', x_label='Is Weekend (1=True, 0=False)')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0zhbWKe37nS"
      },
      "source": [
        "____________\n",
        "casual  \n",
        "1. Electric bikes are more popular than classic bikes for both weekdays and weekends, but especially during weekdays.\n",
        "\n",
        "\n",
        "members\n",
        "1. On weekends, classic bike usage kinda equal electric bike usage\n",
        "_______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK2pQkQA64pY"
      },
      "source": [
        "**Final Answer :**\n",
        "\n",
        "تختلف أوقات الذروة وأيام الاستخدام بشكل كبير بين فئات المستخدمين وأنواع الدراجات, يظهر الأعضاء أنماط استخدام قوية في أيام الأسبوع وساعات غير الذروة، ويفضلون الدراجات الكهربائية بشكل عام، باستثناء ساعات الذروة وعطلات نهاية الأسبوع حيث يكون هناك استخدام ملحوظ للدراجات الكلاسيكية. في المقابل، يميل المستخدمون العابرون إلى زيادة استخدامهم نحو نهاية الأسبوع ويفضلون الدراجات الكهربائية."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Cv1tu0Doum"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "َQ2\n",
        "---\n",
        " Are there distinct usage patterns for electric_bike vs. classic_bike during different hour_segment and start_day_name, and does this differ between member_casual types?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZooXfs7k63Is"
      },
      "outputs": [],
      "source": [
        "# for hour segments\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_trip_counts, group_col='hour_segment')\n",
        "    .pipe(plot_static_trip_bar, x_col='hour_segment', title='Usage by Hour Segment and Ride Type', x_label='Hour Segment')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCaLaJ1eGhoz"
      },
      "source": [
        "________\n",
        "1. Electric Bike Dominant in night time while classis is slightly more in the the evening\n",
        "\n",
        "2. both casual and members usage peakes at night time segment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bCsTsF0HYWC"
      },
      "source": [
        "**Final Answer :**<br>\n",
        "Yes, there are distinct usage patterns for electric vs. classic bikes, and these patterns vary between member and casual user types across different hourly segments and days of the week. Electric bikes show a general and dominant preference across both user types  during the \"Night\" segment\n",
        "what does this tells us ?\n",
        "it could be the following\n",
        "1. Late-Night Commuting/Work Shifts\n",
        "2.  During the \"Night\" segment, public transportation options (buses, metro) often become less frequent or stop running, making electric bikes a convenient and perhaps faster alternative for members needing to travel.\n",
        "3. Convenience and Effort Reduction\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNIcGNleKvaF"
      },
      "source": [
        "---\n",
        "Q3\n",
        "---\n",
        "How do specific weather_segment or conditions (e.g., \"Light Rain,\" \"Clear\") impact trip_duration_minutes and trip_cost for different rideable_type?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0VryxuwEfYk"
      },
      "outputs": [],
      "source": [
        "def aggregate_weather_impact(df, group_col):\n",
        "    return (\n",
        "        df\n",
        "        .groupby(['rideable_type', group_col])\n",
        "        .agg(\n",
        "            avg_duration=('trip_duration_minutes', 'mean'),\n",
        "            median_duration=('trip_duration_minutes', 'median'),\n",
        "            avg_cost=('trip_cost', 'mean'),\n",
        "            median_cost=('trip_cost', 'median'),\n",
        "            trip_count=('trip_duration_minutes', 'count')\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "def plot_avg_duration(df, x_col, title):\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x=x_col,\n",
        "        y='avg_duration',\n",
        "        color='rideable_type',\n",
        "        barmode='group',\n",
        "        title=title,\n",
        "        labels={'avg_duration': 'Avg Trip Duration (min)', x_col: x_col}\n",
        "    )\n",
        "    fig.update_layout(showlegend=True)\n",
        "    fig.show(config={'staticPlot': True})\n",
        "\n",
        "# average trip cost\n",
        "def plot_avg_cost(df, x_col, title):\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x=x_col,\n",
        "        y='avg_cost',\n",
        "        color='rideable_type',\n",
        "        barmode='group',\n",
        "        title=title,\n",
        "        labels={'avg_cost': 'Avg Trip Cost ($)', x_col: x_col}\n",
        "    )\n",
        "    fig.update_layout(showlegend=True)\n",
        "    fig.show(config={'staticPlot': True})\n",
        "# Duration by weather segment\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_weather_impact, group_col='weather_segment')\n",
        "    .pipe(plot_avg_duration, x_col='weather_segment', title='Avg Trip Duration by Weather Segment and Ride Type')\n",
        ")\n",
        "\n",
        "# Cost by weather segment\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_weather_impact, group_col='weather_segment')\n",
        "    .pipe(plot_avg_cost, x_col='weather_segment', title='Avg Trip Cost by Weather Segment and Ride Type')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OjDFavmL4jC"
      },
      "source": [
        "1.  Across all weather conditions (Cloudy, Rainy, Sunny), classic bikes consistently show a significantly higher average trip duration compared to electric bikes.\n",
        "2. For classic bikes, the average duration is slightly higher on sunny and cloudy days (around 17-18 minutes), and decreases slightly on rainy days (just over 15 minutes), suggesting users might shorten their rides slightly in the rain.\n",
        "3. For electric bikes, the average trip duration remains relatively stable and around 12-13 minutes across all weather conditions, indicating their use for quick and efficient trips regardless of the weather.\n",
        "4. both bike types avg trip cost is stable in every condition , this incdicates general weather condition  has a very minimal, almost negligible, impact on the average trip cost for both types of bikes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdIR5usUNMDL"
      },
      "source": [
        "**Final Answer :**<br>\n",
        "Weather conditions have a small impact on the average trip duration and average trip cost,  Classic bikes consistently record a longer average trip duration than electric bikes across all weather conditions, with a slight decrease on rainy days. In contrast, electric bikes are used for shorter average trips and have a consistently slightly higher average cost, but their duration and cost are not  affected by changes in general weather conditions. This suggests that the primary factors influencing trip duration and cost are related to the bike type itself rather than the overarching weather segments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhRGJSFUODU9"
      },
      "source": [
        "---\n",
        "Q4\n",
        "---\n",
        "Do start_station_name and end_station_name popularity, or overall trip density (geohash_sector), show significant differences between is_weekend and weekdays?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Hc2okSKi0U"
      },
      "outputs": [],
      "source": [
        "def aggregate_station_or_zone_avg_per_day(df, group_col, top_n=15):\n",
        "    grouped = (\n",
        "        df\n",
        "        .groupby([group_col, 'is_weekend'])\n",
        "        .size()\n",
        "        .reset_index(name='ride_count')\n",
        "    )\n",
        "\n",
        "    # Normalize: divide by number of days in each category\n",
        "    grouped['avg_per_day'] = grouped.apply(\n",
        "        lambda row: row['ride_count'] / 2 if row['is_weekend'] == 1 else row['ride_count'] / 5,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Get top N based on total ride counts (still for filtering)\n",
        "    top_items = (\n",
        "        grouped\n",
        "        .groupby(group_col)['ride_count']\n",
        "        .sum()\n",
        "        .nlargest(top_n)\n",
        "        .index\n",
        "    )\n",
        "\n",
        "    return grouped[grouped[group_col].isin(top_items)]\n",
        "def plot_static_avg_bar(df, x_col, title):\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x=x_col,\n",
        "        y='avg_per_day',\n",
        "        color='is_weekend',\n",
        "        barmode='group',\n",
        "        title=title,\n",
        "        labels={x_col: x_col.replace(\"_\", \" \").title(), 'avg_per_day': 'Avg Rides per Day', 'is_weekend': 'Is Weekend'}\n",
        "    )\n",
        "    fig.update_layout(showlegend=True)\n",
        "    fig.show(config={'staticPlot': True})\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_station_or_zone_avg_per_day, group_col='start_station_name', top_n=15)\n",
        "    .pipe(plot_static_avg_bar, x_col='start_station_name', title='Avg Daily Rides from Top 15 Start Stations (Weekday vs Weekend)')\n",
        ")\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_station_or_zone_avg_per_day, group_col='end_station_name', top_n=15)\n",
        "    .pipe(plot_static_avg_bar, x_col='end_station_name', title='Avg Daily Rides to Top 15 End Stations (Weekday vs Weekend)')\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAkeBvIDPgd8"
      },
      "source": [
        "1. Weekday Dominance, For nearly all of the top 15 stations, the vast majority of rides originate on weekdays\n",
        "2. \"Park Rd & Holmead Pl NW\" stands out as the most popular starting and end station\n",
        "3. The consistent weekday dominance across these high-density zones suggests that the core bike-sharing activity in the city's busiest areas is predominantly driven by weekday behaviors, such as commuting or running errands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYGwwAcJQFC2"
      },
      "source": [
        "**Final answer**<br>\n",
        "Yes, the popularity of both start_station_name and end_station_name, along with overall trip density as measured by geohash_sector, exhibit significant differences between weekdays and weekends. weekday usage consistently and heavily outweighs weekend usage. This indicates that the primary demand and busiest areas for bike-sharing are concentrated during the workweek, suggesting a strong influence of commuting and routine daily activities. While weekend rides do occur, they form a notably smaller proportion of total activity in these popular hubs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHE9Nm7-QhvK"
      },
      "source": [
        "---\n",
        "Q5\n",
        "---\n",
        "Which stations act as major transfer hubs (high start_station_name and end_station_name volume)? Do they primarily serve specific member_casual types or rideable_type?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gJSQf30OPWg"
      },
      "outputs": [],
      "source": [
        "def get_top_transfer_stations(df, top_n=15):\n",
        "    # Count start and end occurrences separately\n",
        "    starts = df['start_station_name'].value_counts()\n",
        "    ends = df['end_station_name'].value_counts()\n",
        "\n",
        "    # Sum to get total transfer volume\n",
        "    total_volume = starts.add(ends, fill_value=0).sort_values(ascending=False)\n",
        "\n",
        "    top_stations = total_volume.head(top_n).index.tolist()\n",
        "    return top_stations\n",
        "\n",
        "def aggregate_station_transfer_profiles(df, station_list):\n",
        "    filtered = df[\n",
        "        df['start_station_name'].isin(station_list) |\n",
        "        df['end_station_name'].isin(station_list)\n",
        "    ].copy()\n",
        "\n",
        "    # Normalize to one column: station_name\n",
        "    filtered['station_name'] = filtered.apply(\n",
        "        lambda row: row['start_station_name'] if row['start_station_name'] in station_list else row['end_station_name'],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Group by station, member_casual, rideable_type\n",
        "    return (\n",
        "        filtered\n",
        "        .groupby(['station_name', 'member_casual', 'rideable_type'])\n",
        "        .size()\n",
        "        .reset_index(name='ride_count')\n",
        "    )\n",
        "\n",
        "def plot_station_transfer_breakdown(df, title):\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x='station_name',\n",
        "        y='ride_count',\n",
        "        color='rideable_type',\n",
        "        barmode='group',\n",
        "        facet_col='member_casual',\n",
        "        title=title,\n",
        "        labels={\n",
        "            'station_name': 'Station Name',\n",
        "            'ride_count': 'Ride Count',\n",
        "            'rideable_type': 'Bike Type',\n",
        "            'member_casual': 'User Type'\n",
        "        }\n",
        "    )\n",
        "    fig.update_layout(showlegend=True, xaxis_tickangle=-45)\n",
        "    fig.show(config={'staticPlot': True})\n",
        "top_stations = get_top_transfer_stations(trips_df, top_n=15)\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_station_transfer_profiles, station_list=top_stations)\n",
        "    .pipe(plot_station_transfer_breakdown, title='Top 15 Transfer Hubs by User and Bike Type')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbIYeB5KRYLM"
      },
      "source": [
        "1. The plot clearly indicates that \"Park Rd & Holmead Pl NW\" is the most significant transfer hub\n",
        "2. These major transfer hubs overwhelmingly serve member users\n",
        "3. For both casual and member users, electric bikes  are consistently more utilized than classic bikes  across almost all of the top 15 transfer stations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oUSvbnmRp37"
      },
      "source": [
        "**Final answer** <br>\n",
        "The station acting as the primary major transfer hub is \"Park Rd & Holmead Pl NW\", followed by stations such as \"14th St & New York Ave NW,\" \"17th & P St NW,\" and \"14th & Belmont St NW\". These hubs overwhelmingly serve member users, who exhibit significantly higher ride volumes than casual users at these locations. Furthermore, these transfer hubs primarily cater to the use of electric bikes, which are consistently and notably more popular than classic bikes for both member and casual riders at these high-volume stations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfPEvZ8lR_oO"
      },
      "source": [
        "---\n",
        "Q6\n",
        "---\n",
        "Can we identify common \"commute routes\" or \"regular paths\" for  users based on frequent start_station_name and end_station_name pairs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWtYp0tESLe5"
      },
      "outputs": [],
      "source": [
        "def aggregate_top_routes_with_cbd(df, top_n=15):\n",
        "    # Filter to classic/electric only\n",
        "    filtered_df = df[df['rideable_type'].isin(['classic_bike', 'electric_bike'])].copy()\n",
        "\n",
        "    # Create route string\n",
        "    filtered_df['route'] = filtered_df['start_station_name'] + ' → ' + filtered_df['end_station_name']\n",
        "\n",
        "    # Group by user, route, and in_cbd\n",
        "    route_counts = (\n",
        "        filtered_df\n",
        "        .groupby(['member_casual', 'route', 'in_cbd'])\n",
        "        .size()\n",
        "        .reset_index(name='ride_count')\n",
        "    )\n",
        "\n",
        "    # Get top N routes *per user*, regardless of CBD\n",
        "    top_routes = (\n",
        "        route_counts\n",
        "        .groupby('member_casual', group_keys=False)\n",
        "        .apply(lambda g: g.groupby('route')['ride_count'].sum().nlargest(top_n))\n",
        "        .reset_index()\n",
        "        .rename(columns={0: 'total_rides'})\n",
        "    )\n",
        "\n",
        "    # Merge back to get `in_cbd` split per route\n",
        "    final_df = route_counts.merge(top_routes[['route']], on='route', how='inner')\n",
        "\n",
        "    return final_df\n",
        "def plot_top_routes_cbd(df, title):\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x='ride_count',\n",
        "        y='route',\n",
        "        color='in_cbd',\n",
        "        facet_col='member_casual',\n",
        "        orientation='h',\n",
        "        title=title,\n",
        "        labels={\n",
        "            'route': 'Route (Start → End)',\n",
        "            'ride_count': 'Number of Rides',\n",
        "            'in_cbd': 'Passed CBD',\n",
        "            'member_casual': 'User Type'\n",
        "        },\n",
        "        color_discrete_map={0: '#636EFA', 1: '#EF553B'}\n",
        "    )\n",
        "    fig.update_layout(showlegend=True)\n",
        "    fig.update_yaxes(categoryorder=\"total ascending\")\n",
        "    fig.show(config={'staticPlot': True})\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(aggregate_top_routes_with_cbd, top_n=15)\n",
        "    .pipe(plot_top_routes_cbd, title='Top 15 Routes by User Type and CBD Inclusion')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub_adGnCU1CS"
      },
      "source": [
        "1. There are clearly a few routes that are significantly more popular than others,The self-loop routes (start and end at the same station) are very prominent.\n",
        "2.  For many of the top routes, a significant portion of trips seem to involve the CBD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMa7vz6qVVOY"
      },
      "source": [
        "**Final answer**<br>\n",
        "Yes, we can identify common \"regular paths\" for users based on frequent start_station_name and end_station_name pairs. The analysis of the \"Top 15 Routes\" plot reveals distinct popular paths. While several prominent routes are self-looping , a considerable portion of these top routes also show involvement with the Central Business District (in_cbd). This in_cbd presence suggests that many of these frequent paths, likely serve as actual \"commute routes\" or regular travel paths for users, especially those traveling to, from, or through the city's commercial core.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLORVRIiVvwU"
      },
      "source": [
        "---\n",
        "Q7\n",
        "---\n",
        "Given the April revenue issue, can we pinpoint a potential cause by analyzing weather_segment, changes in member_casual behavior, or rideable_type availability/usage during that specific month?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTF4dh1JWbiS"
      },
      "outputs": [],
      "source": [
        "trips_df['start_month'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwC4ZMECUHr1"
      },
      "outputs": [],
      "source": [
        "def filter_april(df):\n",
        "    return df[df['start_month'] == 4].copy()\n",
        "def aggregate_april_by_user(df):\n",
        "    return (\n",
        "        df\n",
        "        .groupby('member_casual')\n",
        "        .size()\n",
        "        .reset_index(name='ride_count')\n",
        "    )\n",
        "\n",
        "def aggregate_april_by_bike(df):\n",
        "    return (\n",
        "        df\n",
        "        .groupby('rideable_type')\n",
        "        .size()\n",
        "        .reset_index(name='ride_count')\n",
        "    )\n",
        "\n",
        "def aggregate_april_by_weather(df):\n",
        "    return (\n",
        "        df\n",
        "        .groupby('weather_segment')\n",
        "        .size()\n",
        "        .reset_index(name='ride_count')\n",
        "    )\n",
        "\n",
        "def aggregate_april_cost_stats(df):\n",
        "    return (\n",
        "        df\n",
        "        .groupby(['rideable_type', 'member_casual', 'weather_segment'])\n",
        "        .agg(\n",
        "            total_rides=('ride_id', 'count'),\n",
        "            avg_trip_cost=('trip_cost', 'mean'),\n",
        "            total_revenue=('trip_cost', 'sum')\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "def plot_april_bar(df, x, y, title, color=None):\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x=x,\n",
        "        y=y,\n",
        "        color=color,\n",
        "        title=title,\n",
        "        labels={x: x.replace(\"_\", \" \").title(), y: y.replace(\"_\", \" \").title()}\n",
        "    )\n",
        "    fig.update_layout(showlegend=True)\n",
        "    fig.show(config={'staticPlot': True})\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(filter_april)\n",
        "    .pipe(aggregate_april_by_user)\n",
        "    .pipe(lambda df: plot_april_bar(df, x='member_casual', y='ride_count', title='April Ride Volume by User Type'))\n",
        ")\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(filter_april)\n",
        "    .pipe(aggregate_april_by_bike)\n",
        "    .pipe(lambda df: plot_april_bar(df, x='rideable_type', y='ride_count', title='April Ride Volume by Bike Type'))\n",
        ")\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(filter_april)\n",
        "    .pipe(aggregate_april_by_weather)\n",
        "    .pipe(lambda df: plot_april_bar(df, x='weather_segment', y='ride_count', title='April Ride Volume by Weather Segment'))\n",
        ")\n",
        "\n",
        "(\n",
        "    trips_df\n",
        "    .pipe(filter_april)\n",
        "    .pipe(aggregate_april_cost_stats)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igRIqtFjzXsh"
      },
      "source": [
        "# Even deeper analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "CdEyMmaoWP_C",
        "outputId": "d3211d0d-3c23-42f6-d080-f75e4a122350"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Step 1: Prepare weekly aggregated DataFrame\n",
        "weekly_stats = (\n",
        "    trips_df\n",
        "    .assign(date=pd.to_datetime(trips_df['date']))\n",
        "    .groupby(pd.Grouper(key='date', freq='W-MON'))  # Weekly data, starting Mondays\n",
        "    .agg(\n",
        "        total_rides=('ride_id', 'count'),\n",
        "        total_revenue=('trip_cost', 'sum')\n",
        "    )\n",
        "    .reset_index()\n",
        "    .sort_values('date')\n",
        "    .set_index('date')\n",
        ")\n",
        "\n",
        "# Step 2: Apply STL decomposition to total_rides\n",
        "stl_result = STL(weekly_stats['total_rides'], period=52).fit()  # period=52 for yearly seasonality in weekly data\n",
        "\n",
        "# Step 3: Add residuals and z-scores to the DataFrame\n",
        "weekly_stats = (\n",
        "    weekly_stats\n",
        "    .assign(\n",
        "        trend=stl_result.trend,\n",
        "        seasonal=stl_result.seasonal,\n",
        "        resid=stl_result.resid,\n",
        "        resid_z=zscore(stl_result.resid)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 4: Detect anomalies (z-score > 2)\n",
        "anomalies = weekly_stats[weekly_stats['resid_z'].abs() > 2]\n",
        "\n",
        "# Step 5: Plot total rides and anomalies\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=weekly_stats.index, y=weekly_stats['total_rides'], name='Total Weekly Rides'))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=anomalies.index, y=anomalies['total_rides'],\n",
        "    mode='markers', name='Anomalies',\n",
        "    marker=dict(color='red', size=8)\n",
        "))\n",
        "fig.update_layout(title='Weekly Total Rides with Anomalies', xaxis_title='Week', yaxis_title='Total Rides')\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF0n7WXZ2kl9",
        "outputId": "9cf02cdf-96f6-4a4c-f675-7dca63faec11"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'trips_df' is your main DataFrame (the one with all trip data)\n",
        "# and 'anomalies' is the DataFrame you created in the previous step\n",
        "# (containing the detected anomalous weeks, likely indexed by date).\n",
        "\n",
        "# --- Step 1: Identify the exact week of the anomaly ---\n",
        "# We'll take the first anomalous week detected.\n",
        "# If you have multiple anomalies and want to explore a specific one,\n",
        "# adjust 'anomalies.index[0]' accordingly (e.g., anomalies.index[1] for the second).\n",
        "anomalous_week_start_date = anomalies.index[0]\n",
        "\n",
        "print(f\"Exploring the anomalous week starting: {anomalous_week_start_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Define the end of the anomalous week (7 days after the start)\n",
        "anomalous_week_end_date = anomalous_week_start_date + pd.Timedelta(days=6)\n",
        "print(f\"Week ends on: {anomalous_week_end_date.strftime('%Y-%m-%d')}\\n\")\n",
        "trips_df['date'] = pd.to_datetime(trips_df['date'])\n",
        "# --- Step 2: Filter trips_df for that specific week ---\n",
        "trips_of_anomalous_week = trips_df[\n",
        "    (trips_df['date'] >= anomalous_week_start_date) &\n",
        "    (trips_df['date'] <= anomalous_week_end_date)\n",
        "].copy() # .copy() helps prevent SettingWithCopyWarning in future operations\n",
        "\n",
        "print(f\"Total trips in the anomalous week: {len(trips_of_anomalous_week)}\")\n",
        "print(f\"Total revenue in the anomalous week: ${trips_of_anomalous_week['trip_cost'].sum():.2f}\\n\")\n",
        "\n",
        "# --- Step 3: Perform detailed EDA for that week ---\n",
        "\n",
        "print(\"--- Breakdown by Member/Casual Type (Anomalous Week) ---\")\n",
        "member_casual_breakdown = trips_of_anomalous_week.groupby('member_casual').agg(\n",
        "    ride_count=('ride_id', 'count'),\n",
        "    total_revenue=('trip_cost', 'sum'),\n",
        "    avg_trip_cost=('trip_cost', 'mean'),\n",
        "    avg_trip_duration_minutes=('trip_duration_minutes', 'mean')\n",
        ").round(2)\n",
        "print(member_casual_breakdown)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"--- Breakdown by Rideable Type (Anomalous Week) ---\")\n",
        "rideable_type_breakdown = trips_of_anomalous_week.groupby('rideable_type').agg(\n",
        "    ride_count=('ride_id', 'count'),\n",
        "    total_revenue=('trip_cost', 'sum'),\n",
        "    avg_trip_cost=('trip_cost', 'mean'),\n",
        "    avg_trip_duration_minutes=('trip_duration_minutes', 'mean')\n",
        ").round(2)\n",
        "print(rideable_type_breakdown)\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- Check for 'weather_segment' column before breaking down by it ---\n",
        "if 'weather_segment' in trips_of_anomalous_week.columns:\n",
        "    print(\"--- Breakdown by Weather Segment (Anomalous Week) ---\")\n",
        "    weather_breakdown = trips_of_anomalous_week.groupby('weather_segment').agg(\n",
        "        ride_count=('ride_id', 'count'),\n",
        "        total_revenue=('trip_cost', 'sum'),\n",
        "        avg_trip_cost=('trip_cost', 'mean'),\n",
        "        avg_trip_duration_minutes=('trip_duration_minutes', 'mean')\n",
        "    ).round(2)\n",
        "    print(weather_breakdown)\n",
        "    print(\"\\n\")\n",
        "else:\n",
        "    print(\"Weather segment data not available for direct breakdown in trips_df for this week.\\n\")\n",
        "\n",
        "\n",
        "print(\"--- Breakdown by Day of Week (Anomalous Week) ---\")\n",
        "# Ensure consistency in day names; 'start_day_name' should ideally be string names (e.g., 'Monday')\n",
        "day_of_week_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "# Using .reindex to ensure all days appear, even if count is 0\n",
        "day_of_week_breakdown = trips_of_anomalous_week['start_day_name'].value_counts().reindex(day_of_week_order).fillna(0).astype(int)\n",
        "print(day_of_week_breakdown)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"--- Breakdown by Hour Segment (Anomalous Week) ---\")\n",
        "# Assuming 'hour_segment' has 'Morning', 'Midday', 'Evening', 'Night'\n",
        "hour_segment_order = ['Morning', 'Midday', 'Evening', 'Night']\n",
        "hour_segment_breakdown = trips_of_anomalous_week['hour_segment'].value_counts().reindex(hour_segment_order).fillna(0).astype(int)\n",
        "print(hour_segment_breakdown)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"--- Top 5 Start Stations in Anomalous Week ---\")\n",
        "print(trips_of_anomalous_week['start_station_name'].value_counts().head(5))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"--- Top 5 End Stations in Anomalous Week ---\")\n",
        "print(trips_of_anomalous_week['end_station_name'].value_counts().head(5))\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dANETm8a7ONA"
      },
      "source": [
        "When a point in a time series, like your \"Total Weekly Rides\" or \"Total Weekly Revenue,\" is detected as an anomaly (a red dot on your plot), it means that particular data point deviates significantly from the pattern that the time series model (like STL decomposition) expects, given its learned trend and seasonality.\n",
        "\n",
        "Even if a point doesn't look dramatically different to the naked eye on the raw plot, the statistical model has identified it as an outlier after factoring in the usual ebb and flow of the data.\n",
        "\n",
        "Here are the general categories of problems or situations that could lead to such a point being flagged as an anomaly in a bike-sharing dataset:\n",
        "<br>\n",
        "1. Data Quality Issues\n",
        "2. External Events and Unforeseen Circumstances\n",
        "3. Sudden System or Operational Changes\n",
        "4. Changes in User Behavior/Trends:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcnmhGD2-1IJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDzAW7fB-vqg"
      },
      "source": [
        "---\n",
        "Season\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUe6jepN6q_g"
      },
      "outputs": [],
      "source": [
        "trips_df = trips_df.assign(\n",
        "    is_round_trip=lambda df: df['start_station_id'] == df['end_station_id']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "F9PsFp5z9eo-",
        "outputId": "c40c08a7-4c05-4778-c148-9a4057f7b9cf"
      },
      "outputs": [],
      "source": [
        "trips_df['is_round_trip'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8C2FBlp7--o0",
        "outputId": "f0dbdb6b-34e2-4b0e-e0c6-463758ede7c8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Function to plot seasonal bar chart\n",
        "def plot_seasonal_bar(data, y_col, title):\n",
        "    fig = px.bar(\n",
        "        data,\n",
        "        x='season',\n",
        "        y=y_col,\n",
        "        color='season',\n",
        "        title=title,\n",
        "        text=y_col\n",
        "    )\n",
        "    fig.update_traces(texttemplate='%{text:,}', textposition='outside')\n",
        "    fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
        "    fig.show(config={'staticPlot': True})\n",
        "\n",
        "# Season mapping dictionary\n",
        "season_map = {\n",
        "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
        "    3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "    9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
        "}\n",
        "\n",
        "# Process with method chaining / pipes\n",
        "seasonal_stats = (\n",
        "    trips_df\n",
        "    .assign(season=lambda df: df['start_month'].map(season_map))\n",
        "    .groupby('season')\n",
        "    .agg(\n",
        "        total_trips=('ride_id', 'count'),\n",
        "        total_revenue=('trip_cost', 'sum')\n",
        "    )\n",
        "    .reset_index()\n",
        "    .assign(season=lambda df: pd.Categorical(df['season'], categories=['Winter', 'Spring', 'Summer', 'Autumn'], ordered=True))\n",
        "    .sort_values('season')\n",
        ")\n",
        "\n",
        "# Plot Total Trips\n",
        "plot_seasonal_bar(seasonal_stats, 'total_trips', 'Total Trips per Season (2024)')\n",
        "\n",
        "# Plot Total Revenue\n",
        "plot_seasonal_bar(seasonal_stats, 'total_revenue', 'Total Revenue per Season (2024)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "-lWkaaqhAoap",
        "outputId": "9671f9ac-3edf-4767-8bab-a38d7bccfab1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Group by week and assign season\n",
        "season_map = {\n",
        "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
        "    3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
        "    9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
        "}\n",
        "\n",
        "# Prepare the time series\n",
        "seasonal_time_series = (\n",
        "    trips_df\n",
        "    .assign(\n",
        "        date=pd.to_datetime(trips_df['date']),\n",
        "        week_start=lambda df: df['date'] - pd.to_timedelta(df['date'].dt.weekday, unit='D'),\n",
        "        season=lambda df: df['date'].dt.month.map(season_map)\n",
        "    )\n",
        "    .query(\"date.dt.year == 2024\")\n",
        "    .groupby(['week_start'])\n",
        "    .agg(\n",
        "        total_trips=('ride_id', 'count'),\n",
        "        season=('season', 'first')  # Take the dominant season for each week\n",
        "    )\n",
        "    .reset_index()\n",
        "    .assign(\n",
        "        season=lambda df: pd.Categorical(df['season'], categories=['Winter', 'Spring', 'Summer', 'Autumn'], ordered=True)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Detect season change points\n",
        "season_changes = seasonal_time_series[\n",
        "    seasonal_time_series['season'] != seasonal_time_series['season'].shift(1)\n",
        "]\n",
        "\n",
        "# Plot with gray continuous line + colored season markers\n",
        "fig = go.Figure()\n",
        "\n",
        "# Full continuous line (gray)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=seasonal_time_series['week_start'],\n",
        "    y=seasonal_time_series['total_trips'],\n",
        "    mode='lines',\n",
        "    name='Total Trips',\n",
        "    line=dict(color='gray'),\n",
        "    showlegend=False\n",
        "))\n",
        "\n",
        "# Add colored points at season start positions\n",
        "for season_name, color in zip(['Winter', 'Spring', 'Summer', 'Autumn'], ['blue', 'green', 'orange', 'brown']):\n",
        "    season_points = season_changes[season_changes['season'] == season_name]\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=season_points['week_start'],\n",
        "        y=season_points['total_trips'],\n",
        "        mode='markers+text',\n",
        "        name=season_name,\n",
        "        text=season_name,\n",
        "        textposition='top center',\n",
        "        marker=dict(size=10, color=color),\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Total Weekly Trips in 2024 (Colored by Season Start)',\n",
        "    xaxis_title='Week',\n",
        "    yaxis_title='Total Trips',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show(config={'staticPlot': True})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7WX_PF8C1Tw"
      },
      "source": [
        "insights:\n",
        "1. Autumn generated the highest revenue and number of trips, followed by Summer, then Spring, and Winter had the lowest revenue.\n",
        "2. at the end of autumn (november) the number of trips drop significantly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfV2jGhDEn80"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Feature selection\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RDWPLpNDCcJn",
        "outputId": "80520d66-59b2-476a-d96d-53d7e5d89411"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make sure 'date' and 'started_at' are datetime objects\n",
        "# Assuming 'started_at' is the full timestamp and 'date' is derived as just the date part.\n",
        "# trips_df['started_at'] = pd.to_datetime(trips_df['started_at'])\n",
        "# Fix datetime parsing with flexible format\n",
        "trips_df['started_at'] = pd.to_datetime(trips_df['started_at'], errors='coerce', infer_datetime_format=True)\n",
        "# Ensure 'date' is datetime64 dtype (not Python date)\n",
        "trips_df['date'] = pd.to_datetime(trips_df['started_at'], errors='coerce')\n",
        "\n",
        "# Drop rows where 'date' could not be parsed (NaT)\n",
        "trips_df = trips_df.dropna(subset=['date'])\n",
        "\n",
        "# Now create week_start_date safely using vectorized operations\n",
        "trips_df['week_start_date'] = trips_df['date'] - pd.to_timedelta(trips_df['date'].dt.weekday, unit='d')\n",
        "\n",
        "trips_df['week_start_date'] = pd.to_datetime(trips_df['week_start_date']) # Ensure it's datetime\n",
        "trips_df['week_start_date'] = trips_df['date'] - pd.to_timedelta(trips_df['date'].dt.weekday, unit='d')\n",
        "trips_df['week_start_date'] = trips_df['week_start_date'].dt.normalize()\n",
        "\n",
        "# Define numerical features for averaging/summing, and categorical for modes/counts\n",
        "# These are the original and engineered features from your column list\n",
        "numerical_features_to_avg = [\n",
        "    'trip_duration_minutes', 'trip_cost', 'temp', 'humidity', 'windspeedmean',\n",
        "    'trip_distance_km', 'trip_avg_speed_kmh', 'distance_to_metro_connection'\n",
        "]\n",
        "\n",
        "boolean_features_to_sum_as_count_or_prop = [\n",
        "    'is_round_trip', 'is_adverse_weather_trip', 'is_peak_season',\n",
        "    'is_cbd_trip', 'is_long_duration_trip', 'is_weekend', 'rush_hour',\n",
        "    'start_in_cbd', 'end_in_cbd', 'is_far_from_metro_start', 'is_far_from_metro_end',\n",
        "    'is_far_from_shuttle_start', 'is_far_from_shuttle_end', 'is_far_from_any_transit'\n",
        "]\n",
        "\n",
        "categorical_features_to_mode = [\n",
        "    'season', 'rideable_type', 'member_casual', 'conditions',\n",
        "    'start_day_name', 'hour_segment' # Added hour_segment based on your list\n",
        "]\n",
        "\n",
        "# Ensure numeric types where expected for aggregation\n",
        "for col in numerical_features_to_avg + ['trip_cost']: # trip_cost also used for total revenue\n",
        "    if col in trips_df.columns and not pd.api.types.is_numeric_dtype(trips_df[col]):\n",
        "        trips_df[col] = pd.to_numeric(trips_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "for col in boolean_features_to_sum_as_count_or_prop:\n",
        "    if col in trips_df.columns and not pd.api.types.is_numeric_dtype(trips_df[col]):\n",
        "        trips_df[col] = trips_df[col].astype(int)\n",
        "\n",
        "\n",
        "# Aggregate data weekly\n",
        "weekly_data = trips_df.groupby('week_start_date').agg(\n",
        "    total_trips=('ride_id', 'count'), # Our target variable\n",
        "    total_revenue=('trip_cost', 'sum'), # Another potential target\n",
        "\n",
        "    # Averages for numerical features\n",
        "    **{f'avg_{col}': (col, 'mean') for col in numerical_features_to_avg if col in trips_df.columns},\n",
        "\n",
        "    # Sums/Counts for boolean flags (representing total occurrences in the week)\n",
        "    **{f'total_{col}': (col, 'sum') for col in boolean_features_to_sum_as_count_or_prop if col in trips_df.columns},\n",
        "\n",
        "    # Proportions of specific categories (e.g., proportion of electric bikes, members)\n",
        "    prop_electric_bike=('rideable_type', lambda x: (x == 'electric_bike').sum() / len(x) if len(x) > 0 else 0),\n",
        "    prop_classic_bike=('rideable_type', lambda x: (x == 'classic_bike').sum() / len(x) if len(x) > 0 else 0),\n",
        "    prop_member=('member_casual', lambda x: (x == 'member').sum() / len(x) if len(x) > 0 else 0),\n",
        "    prop_casual=('member_casual', lambda x: (x == 'casual').sum() / len(x) if len(x) > 0 else 0),\n",
        "\n",
        "    # Mode/most frequent for categorical features (e.g., most common weather condition for the week)\n",
        "    # Be careful with mode - it returns a Series if multiple modes exist. Let's get the first mode.\n",
        "    # We'll use start_day_name from the first day of the week, or the most common season\n",
        "    weekly_season=('season', lambda x: x.mode()[0] if not x.empty else pd.NA), # For weekly seasonality\n",
        "    weekly_conditions=('conditions', lambda x: x.mode()[0] if not x.empty else pd.NA),\n",
        "    # Add other categorical features as needed, using .mode()[0]\n",
        ").reset_index()\n",
        "\n",
        "# Drop rows where total_trips is 0 (no activity for that week) as they won't provide meaningful feature insights\n",
        "weekly_data = weekly_data[weekly_data['total_trips'] > 0]\n",
        "\n",
        "print(\"Weekly Aggregated Data Head:\")\n",
        "print(weekly_data.head())\n",
        "print(\"\\nWeekly Aggregated Data Info:\")\n",
        "print(weekly_data.info())\n",
        "\n",
        "\n",
        "# --- Prepare Data for Tree-based Feature Importance ---\n",
        "\n",
        "# Identify features (X) and target (y)\n",
        "# Let's exclude week_start_date itself and the other target (total_revenue)\n",
        "X = weekly_data.drop(columns=['week_start_date', 'total_trips', 'total_revenue'])\n",
        "y = weekly_data['total_trips']\n",
        "\n",
        "# Handle categorical features using One-Hot Encoding\n",
        "categorical_cols = X.select_dtypes(include='object').columns\n",
        "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True) # drop_first to avoid multicollinearity\n",
        "\n",
        "# Handle any remaining NaNs in X (e.g., from averages if a feature was all NaN for a week)\n",
        "# Common strategies: fill with mean, median, or 0. For demonstration, fill with 0.\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Split data into training and testing sets (optional for just feature importance, but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestRegressor\n",
        "# Use a relatively simple model for feature importance, number of estimators can be tuned\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nTop 20 Features by Importance:\")\n",
        "print(feature_importances.nlargest(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvNr-LqpK0AW"
      },
      "source": [
        "Feature Insights (Top 20):<br>\n",
        "* total_rush_hour (0.1349): Most important; rush hour trip volume strongly drives total weekly trips.\n",
        "\n",
        "* total_is_far_from_any_transit (0.1113): Trips far from transit hubs correlate with higher trip volumes, suggesting mid-mile or point-to-point usage.\n",
        "\n",
        "* total_start_in_cbd (0.1062): Trips starting in the CBD signal strong demand linked to business and commute activities.\n",
        "\n",
        "* total_is_far_from_metro_end (0.0898): Trips ending far from metro stations are influential in weekly totals.\n",
        "\n",
        "* total_is_far_from_shuttle_end (0.0885): Trips ending far from shuttle stops also impact demand notably.\n",
        "\n",
        "* total_is_far_from_metro_start (0.0811): Trips starting far from metro stations are significant predictors.\n",
        "\n",
        "* total_end_in_cbd (0.0779): Trips ending in the CBD add to the demand, reinforcing CBD’s role as a hub.\n",
        "\n",
        "* total_is_round_trip (0.0721): Recreational or round trips contribute meaningfully to weekly totals.\n",
        "\n",
        "* prop_casual (0.0576): Higher share of casual riders predicts increased weekly trip volumes.\n",
        "\n",
        "* avg_trip_duration_minutes (0.0377): Longer average trip durations correlate with more trips overall.\n",
        "\n",
        "* total_is_weekend (0.0267): Weekend trips matter but less than rush hour or transit-related features.\n",
        "\n",
        "* prop_electric_bike (0.0213): Popularity of electric bikes moderately influences total trips.\n",
        "\n",
        "* prop_member (0.0201): Member proportion less predictive than casual riders.\n",
        "\n",
        "* total_is_far_from_shuttle_start (0.0197): Trips starting far from shuttle stops have modest importance.\n",
        "\n",
        "* avg_temp (0.0164): Temperature affects usage but less than operational features.\n",
        "\n",
        "* avg_trip_cost (0.0108): Trip cost plays a smaller role in predicting trip counts.\n",
        "\n",
        "* weekly_season_Winter (0.0082): Seasonality captured somewhat by this feature.\n",
        "\n",
        "* weekly_conditions_Partially cloudy (0.0076): Weather conditions show minor influence.\n",
        "\n",
        "* prop_classic_bike (0.0053): Classic bike share has minimal effect.\n",
        "\n",
        "* avg_humidity (0.0036): Humidity has very low importance.\n",
        "_______\n",
        "**Overall Summary:**<br>\n",
        "The strongest drivers of weekly trip demand are operational and geographic factors—rush hour activity, CBD involvement, and proximity to transit hubs dominate. Casual riders play a bigger role than members in driving demand surges, indicating leisure or event-driven usage patterns. Weather and seasonality impact usage but to a lesser extent compared to core trip characteristics and user behavior. This feature importance analysis provides clear direction on prioritizing fleet management, marketing, and service improvements to optimize demand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbac9esNMTk_"
      },
      "source": [
        "---\n",
        "**PCA**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmfQOdUaH35q",
        "outputId": "214b5f46-1297-4a62-b6ed-85ebd3d9ffe3"
      },
      "outputs": [],
      "source": [
        "# Select Numerical Features for PCA\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Make sure weekly_data is available from the previous step\n",
        "# For demonstration, let's assume weekly_data DataFrame is loaded.\n",
        "\n",
        "# Identify numerical features for PCA\n",
        "# These are the numerical, total_ and prop_ features from your weekly_data\n",
        "pca_features = [\n",
        "    'avg_trip_duration_minutes', 'avg_trip_cost', 'avg_temp', 'avg_humidity', 'avg_windspeedmean',\n",
        "    'total_is_round_trip', 'total_is_weekend', 'total_rush_hour',\n",
        "    'total_start_in_cbd', 'total_end_in_cbd',\n",
        "    'total_is_far_from_metro_start', 'total_is_far_from_metro_end',\n",
        "    'total_is_far_from_shuttle_start', 'total_is_far_from_shuttle_end',\n",
        "    'total_is_far_from_any_transit',\n",
        "    'prop_electric_bike', 'prop_classic_bike', 'prop_member', 'prop_casual'\n",
        "]\n",
        "\n",
        "# Filter out features that might not exist in case of partial data or previous steps\n",
        "# This ensures the code runs even if some features are missing.\n",
        "pca_features = [f for f in pca_features if f in weekly_data.columns]\n",
        "\n",
        "\n",
        "X_pca = weekly_data[pca_features]\n",
        "\n",
        "# Check for any remaining NaNs (though our previous step should have handled most)\n",
        "if X_pca.isnull().sum().sum() > 0:\n",
        "    print(\"Warning: NaNs found in PCA features. Filling with 0 or mean/median is recommended.\")\n",
        "    X_pca = X_pca.fillna(X_pca.mean()) # A common strategy, replace with median if skewed\n",
        "\n",
        "print(f\"Features selected for PCA: {len(pca_features)}\")\n",
        "print(X_pca.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY5fWOxNMdk5",
        "outputId": "f22c4bd6-6064-4a85-e716-849b10dbf6b6"
      },
      "outputs": [],
      "source": [
        "# Standardization/Scaling\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the features\n",
        "X_scaled = scaler.fit_transform(X_pca)\n",
        "\n",
        "# Convert back to DataFrame for better readability (optional, but good for understanding)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=pca_features, index=weekly_data.index)\n",
        "print(\"\\nScaled Features Head:\")\n",
        "print(X_scaled_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RTWmn0TsMjPt",
        "outputId": "77c02ba0-8c42-4746-ac4e-75235bf2ed91"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Step 1: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # X = your features DataFrame\n",
        "\n",
        "# Step 2: Apply PCA (initial - to inspect variance)\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "\n",
        "# Step 3: Plot Explained Variance\n",
        "fig1 = go.Figure()\n",
        "fig1.add_trace(go.Scatter(\n",
        "    x=np.arange(1, len(explained_variance) + 1),\n",
        "    y=explained_variance,\n",
        "    mode='lines+markers',\n",
        "    name='Explained Variance Ratio'\n",
        "))\n",
        "fig1.update_layout(\n",
        "    title='Explained Variance Ratio per Principal Component',\n",
        "    xaxis_title='Number of Components',\n",
        "    yaxis_title='Explained Variance Ratio',\n",
        ")\n",
        "fig1.show(config={'staticPlot': True})\n",
        "\n",
        "# Step 4: Plot Cumulative Explained Variance\n",
        "threshold_line = 0.95\n",
        "n_components_95 = (cumulative_variance >= threshold_line).argmax() + 1\n",
        "\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(\n",
        "    x=np.arange(1, len(cumulative_variance) + 1),\n",
        "    y=cumulative_variance,\n",
        "    mode='lines+markers',\n",
        "    name='Cumulative Explained Variance'\n",
        "))\n",
        "fig2.add_hline(y=threshold_line, line_dash=\"dot\", line_color=\"red\", annotation_text=\"95% Variance\")\n",
        "fig2.add_vline(x=n_components_95, line_dash=\"dot\", line_color=\"green\", annotation_text=f\"{n_components_95} Components\")\n",
        "fig2.update_layout(\n",
        "    title='Cumulative Explained Variance Ratio',\n",
        "    xaxis_title='Number of Components',\n",
        "    yaxis_title='Cumulative Explained Variance',\n",
        ")\n",
        "fig2.show(config={'staticPlot': True})\n",
        "\n",
        "# Step 5: Apply PCA with chosen components\n",
        "chosen_n_components = max(1, n_components_95)\n",
        "pca_final = PCA(n_components=chosen_n_components)\n",
        "X_pca_transformed = pca_final.fit_transform(X_scaled)\n",
        "\n",
        "# Step 6: Create PCA DataFrame\n",
        "pca_df = pd.DataFrame(X_pca_transformed, columns=[f'PC_{i+1}' for i in range(chosen_n_components)])\n",
        "pca_df['week_start_date'] = weekly_data['week_start_date'].reset_index(drop=True)\n",
        "pca_df['total_trips'] = weekly_data['total_trips'].reset_index(drop=True)\n",
        "\n",
        "# Display head\n",
        "print(f\"\\n✅ Transformed data with {chosen_n_components} Principal Components:\")\n",
        "print(pca_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGIYkGdpPHm3"
      },
      "source": [
        "1. Explained Variance Ratio Analysis:\n",
        "* The first principal component (PC1) explains a significantly large proportion of the variance (around 67%). This means that PC1 alone captures the majority of the information content present in your 19 original features.\n",
        "* the variance explained drops off rapidly after the first few components, indicating that a large portion of the data's variability is captured by a small number of dimensions.\n",
        "2. Cumulative Explained Variance:\n",
        "* Your analysis shows that 6 principal components are sufficient to explain approximately 95% of the total variance in your original 19 features. This is an excellent outcome!\n",
        "* This means you've successfully reduced the dimensionality from 19 features down to 6 components while retaining most of the meaningful information. This is a significant reduction (over 68% fewer dimensions) which can help with model training efficiency and potentially generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8JB2MpXuMmsA",
        "outputId": "55b5dcae-7e5a-4b94-d0a1-a4ef66a04181"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create DataFrame for loadings (same as before)\n",
        "loadings = pd.DataFrame(\n",
        "    pca_final.components_.T,\n",
        "    columns=[f'PC_{i+1}' for i in range(chosen_n_components)],\n",
        "    index=X.columns  # Use the feature names from X\n",
        ")\n",
        "\n",
        "print(\"\\nPrincipal Component Loadings:\")\n",
        "print(loadings)\n",
        "\n",
        "# Visualize loadings using a heatmap for the first two components\n",
        "if chosen_n_components >= 2:\n",
        "    loadings_melted = loadings.iloc[:, :2].reset_index().melt(id_vars='index', var_name='Principal Component', value_name='Loading')\n",
        "    fig = px.imshow(\n",
        "        loadings.iloc[:, :2].T,\n",
        "        labels=dict(x=\"Feature\", y=\"Principal Component\", color=\"Loading\"),\n",
        "        x=loadings.index,\n",
        "        y=[f'PC_{i+1}' for i in range(2)],\n",
        "        color_continuous_scale='viridis'\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title='Feature Loadings on Principal Components 1 and 2',\n",
        "        xaxis_title='Original Feature',\n",
        "        yaxis_title='Principal Component',\n",
        "        width=1000,\n",
        "        height=400\n",
        "    )\n",
        "    fig.show(config={'staticPlot': True})\n",
        "\n",
        "# Interpretation Helper\n",
        "print(\"\\nInterpretation of Principal Components:\")\n",
        "for i in range(chosen_n_components):\n",
        "    pc_name = f'PC_{i+1}'\n",
        "    print(f\"\\n--- {pc_name} ---\")\n",
        "    top_features = loadings[pc_name].abs().nlargest(5).index.tolist()\n",
        "    print(f\"Top contributing features (by absolute loading): {top_features}\")\n",
        "    print(loadings[pc_name][top_features])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC1SGmoiPmLa"
      },
      "source": [
        "This is the most critical part, as it tells us what these 6 new, uncorrelated dimensions actually represent in terms of your original features.\n",
        "\n",
        "* PC_1 (Dominant Component - ~67% of Variance):\n",
        "\n",
        "Top Contributors: total_is_round_trip, total_rush_hour, total_start_in_cbd, total_is_far_from_any_transit, total_end_in_cbd.\n",
        "Interpretation: This component appears to be a strong indicator of Overall High Urban Activity and Demand. It captures the simultaneous increase in rush hour trips, trips within/to/from the CBD, and trips that are not directly connected to existing transit infrastructure. Essentially, a higher value on PC1 would indicate a week with high general bike usage for various purposes within the core urban and surrounding areas, including commuting and potentially recreational/point-to-point trips that don't rely on other transit.\n",
        "* PC_2:\n",
        "\n",
        "Top Contributors: avg_humidity, weekly_conditions_Rain, Partially cloudy (positive loadings), and weekly_conditions_Partially cloudy, avg_windspeedmean, avg_trip_cost (negative loadings).\n",
        "Interpretation: This component seems to capture a \"Unfavorable Riding Conditions\" factor. Higher humidity and rainy conditions contribute positively, while partially cloudy conditions, higher wind speeds, and lower average trip costs contribute negatively. A high value in PC2 might indicate a week with more challenging weather conditions (humid, rainy, less windy) and potentially lower-cost rides.\n",
        "* PC_3:\n",
        "\n",
        "Top Contributors: weekly_conditions_Partially cloudy (positive), weekly_conditions_Rain, Partially cloudy, avg_trip_cost, avg_windspeedmean, avg_trip_duration_minutes (negative).\n",
        "Interpretation: This component also relates to Weather and Trip Characteristics. It seems to contrast weeks with partially cloudy conditions (positive) against weeks with rainy/partially cloudy weather, higher average costs, higher wind, and longer durations. It might separate \"mildly cloudy and efficient trip\" weeks from \"more adverse and longer/costlier trip\" weeks.\n",
        "* PC_4:\n",
        "\n",
        "Top Contributors: avg_windspeedmean (strong positive), avg_trip_cost, total_is_weekend, weekly_season_Winter, avg_humidity (negative).\n",
        "Interpretation: This component could represent \"Windy Winter Weekends with Lower Costs\". High wind speeds contribute positively, while higher trip costs, weekend activity, and winter season contribute negatively. This might capture a specific dynamic where windy conditions prevail during colder months or on weekends, influencing ride patterns and costs.\n",
        "* PC_5:\n",
        "\n",
        "Top Contributors: total_is_weekend, prop_electric_bike (positive), prop_classic_bike (negative), avg_windspeedmean, avg_humidity.\n",
        "Interpretation: This component highlights \"Weekend Electric Bike Preference & Weather Sensitivity\". Weeks with more weekend activity and a higher proportion of electric bike usage (and thus lower classic bike usage) tend to score higher on this component. It also incorporates some weather aspects.\n",
        "* PC_6:\n",
        "\n",
        "Top Contributors: avg_humidity, weekly_conditions_Partially cloudy, avg_trip_duration_minutes (positive), weekly_season_Winter, weekly_conditions_Rain, Partially cloudy (negative).\n",
        "Interpretation: This component appears to represent \"Humid, Mild Weather, and Longer Duration Trips vs. Winter/Rain\". Weeks with higher humidity, partially cloudy conditions, and longer average trip durations score higher, contrasting with colder winter conditions or very rainy periods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGt-C2NIPwm-"
      },
      "source": [
        "Overall Takeaways from PCA:\n",
        "* Effective Dimensionality Reduction: You've successfully reduced your 19 aggregated numerical features into 6 principal components, capturing 95% of the variance. This is a significant achievement for simplifying your dataset without losing much information.\n",
        "* Meaningful Components: The principal components are not just abstract numbers; they can be interpreted as meaningful underlying \"factors\" or \"dimensions\" that describe your weekly bike-sharing patterns. They often combine related aspects (e.g., various measures of urban activity, different weather conditions, or user/bike preferences).\n",
        "* Latent Factors: PCA has identified latent factors that drive variations in your weekly bike usage. For example, PC1 is a clear \"overall demand\" factor, while other PCs delineate different environmental and usage patterns.\n",
        "* Ready for Modeling: You can now use these 6 principal components (along with your one-hot encoded categorical features that were not included in PCA, such as weekly_season_Spring, etc.) as input for a predictive model. This can help prevent multicollinearity issues and potentially speed up model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itLstIRYN_cT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
